{"2025-03-27T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2501.12911v3","updated":"2025-03-27T17:44:27Z","published":"2025-01-22T14:37:44Z","title":"A Selective Homomorphic Encryption Approach for Faster\n  Privacy-Preserving Federated Learning","summary":"  Federated learning is a machine learning method that supports training models\non decentralized devices or servers, where each holds its local data, removing\nthe need for data exchange. This approach is especially useful in healthcare,\nas it enables training on sensitive data without needing to share them. The\nnature of federated learning necessitates robust security precautions due to\ndata leakage concerns during communication. To address this issue, we propose a\nnew approach that employs selective encryption, homomorphic encryption,\ndifferential privacy, and bit-wise scrambling to minimize data leakage while\nachieving good execution performance. Our technique , FAS (fast and secure\nfederated learning) is used to train deep learning models on medical imaging\ndata. We implemented our technique using the Flower framework and compared with\na state-of-the-art federated learning approach that also uses selective\nhomomorphic encryption. Our experiments were run in a cluster of eleven\nphysical machines to create a real-world federated learning scenario on\ndifferent datasets. We observed that our approach is up to 90\\% faster than\napplying fully homomorphic encryption on the model weights. In addition, we can\navoid the pretraining step that is required by our competitor and can save up\nto 46% in terms of total execution time. While our approach was faster, it\nobtained similar security results as the competitor.\n","authors":["Abdulkadir Korkmaz","Praveen Rao"],"pdf_url":"https://arxiv.org/pdf/2501.12911v3.pdf","comment":"23 pages, 32 figures"},{"id":"http://arxiv.org/abs/2503.21476v1","updated":"2025-03-27T13:06:26Z","published":"2025-03-27T13:06:26Z","title":"Robust DNN Partitioning and Resource Allocation Under Uncertain\n  Inference Time","summary":"  In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices.\n","authors":["Zhaojun Nan","Yunchu Han","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2503.21476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21453v1","updated":"2025-03-27T12:43:31Z","published":"2025-03-27T12:43:31Z","title":"OCEP: An Ontology-Based Complex Event Processing Framework for\n  Healthcare Decision Support in Big Data Analytics","summary":"  The exponential expansion of real-time data streams across multiple domains\nneeds the development of effective event detection, correlation, and\ndecision-making systems. However, classic Complex Event Processing (CEP)\nsystems struggle with semantic heterogeneity, data interoperability, and\nknowledge driven event reasoning in Big Data environments. To solve these\nchallenges, this research work presents an Ontology based Complex Event\nProcessing (OCEP) framework, which utilizes semantic reasoning and Big Data\nAnalytics to improve event driven decision support. The proposed OCEP\narchitecture utilizes ontologies to support reasoning to event streams. It\nensures compatibility with different data sources and lets you find the events\nbased on the context. The Resource Description Framework (RDF) organizes event\ndata, and SPARQL query enables rapid event reasoning and retrieval. The\napproach is implemented within the Hadoop environment, which consists of Hadoop\nDistributed File System (HDFS) for scalable storage and Apache Kafka for\nreal-time CEP based event execution. We perform a real-time healthcare analysis\nand case study to validate the model, utilizing IoT sensor data for illness\nmonitoring and emergency responses. This OCEP framework successfully integrates\nseveral event streams, leading to improved early disease detection and aiding\ndoctors in decision-making. The result shows that OCEP predicts event detection\nwith an accuracy of 85%. This research work utilizes an OCEP to solve the\nproblems with semantic interoperability and correlation of complex events in\nBig Data analytics. The proposed architecture presents an intelligent, scalable\nand knowledge driven event processing framework for healthcare based decision\nsupport.\n","authors":["Ritesh Chandra","Sonali Agarwal","Shashi Shekhar Kumar","Navjot Singh"],"pdf_url":"https://arxiv.org/pdf/2503.21453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15474v2","updated":"2025-03-27T12:41:08Z","published":"2024-05-24T11:53:13Z","title":"Unlearning during Learning: An Efficient Federated Machine Unlearning\n  Method","summary":"  In recent years, Federated Learning (FL) has garnered significant attention\nas a distributed machine learning paradigm. To facilitate the implementation of\nthe right to be forgotten, the concept of federated machine unlearning (FMU)\nhas also emerged. However, current FMU approaches often involve additional\ntime-consuming steps and may not offer comprehensive unlearning capabilities,\nwhich renders them less practical in real FL scenarios. In this paper, we\nintroduce FedAU, an innovative and efficient FMU framework aimed at overcoming\nthese limitations. Specifically, FedAU incorporates a lightweight auxiliary\nunlearning module into the learning process and employs a straightforward\nlinear operation to facilitate unlearning. This approach eliminates the\nrequirement for extra time-consuming steps, rendering it well-suited for FL.\nFurthermore, FedAU exhibits remarkable versatility. It not only enables\nmultiple clients to carry out unlearning tasks concurrently but also supports\nunlearning at various levels of granularity, including individual data samples,\nspecific classes, and even at the client level. We conducted extensive\nexperiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the\nperformance of FedAU. The results demonstrate that FedAU effectively achieves\nthe desired unlearning effect while maintaining model accuracy. Our code is\navailiable at https://github.com/Liar-Mask/FedAU.\n","authors":["Hanlin Gu","Gongxi Zhu","Jie Zhang","Xinyuan Zhao","Yuxing Han","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.15474v2.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2503.20313v2","updated":"2025-03-27T12:13:46Z","published":"2025-03-26T08:25:12Z","title":"TileLink: Generating Efficient Compute-Communication Overlapping Kernels\n  using Tile-Centric Primitives","summary":"  Large deep learning models have achieved state-of-the-art performance in a\nwide range of tasks. These models often necessitate distributed systems for\nefficient training and inference. The fundamental building blocks for\ndistributed model execution are intra-layer parallel operators. The most\neffective approach to enhancing the performance of intra-layer parallel\noperators involves overlapping computation with communication. The overlapping\ncan be achieved through either operator decomposition or kernel fusion. While\ndecomposing operators is straightforward to implement, it often results in\nsuboptimal performance. On the other hand, fusing communication kernels with\ncompute kernels demands significant expertise and is error-prone.\n  In this paper, we propose TileLink to enable efficient compilation and\ngeneration of overlapped compute-communication kernels. TileLink is composed of\nfrontend and backend. In the frontend, TileLink decouples the design space of\ncommunication and computation, linking these two parts via tile-centric\nprimitives. In the backend, TileLink translates these primitives into low-level\ncommunication instructions, integrating the communication and computation\ncomponents to achieve overlapped execution. In experiments, TileLink achieves\nfrom $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and\nachieves performance comparable to state-of-the-art overlapping libraries on\nGPUs.\n","authors":["Size Zheng","Jin Fang","Xuegui Zheng","Qi Hou","Wenlei Bao","Ningxin Zheng","Ziheng Jiang","Dongyang Wang","Jianxi Ye","Haibin Lin","Li-Wen Chang","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.20313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15404v2","updated":"2025-03-27T10:23:39Z","published":"2024-09-23T17:57:00Z","title":"Renaming in distributed certification","summary":"  Local certification is the area of distributed network computing asking the\nfollowing question: How to certify to the nodes of a network that a global\nproperty holds, if they are limited to a local verification?\n  In this area, it is often essential to have identifiers, that is, unique\nintegers assigned to the nodes. In this short paper, we show how to reduce the\nrange of the identifiers, in three different settings. More precisely, we show\nhow to rename identifiers in the classical local certification setting, when we\ncan (resp.\\ cannot) choose the new identifiers, and we show how a global\ncertificate can help to encode very compactly a new identifier assignment that\nis not injective in general, but still useful in applications.\n  We conclude with a number of applications of these results: For every $\\ell$,\nthere are local certification schemes for the properties of having clique\nnumber at most $\\ell$, having diameter at most $\\ell$, and having independence\nnumber at most~2, with certificates of size $O(n)$. We also show that there is\na global certification scheme for bipartiteness with certificates of size\n$O(n)$. All these results are optimal.\n","authors":["Nicolas Bousquet","Louis Esperet","Laurent Feuilloley","Sébastien Zeitoun"],"pdf_url":"https://arxiv.org/pdf/2409.15404v2.pdf","comment":"14 pages, 1 figure: v2: added a number of applications"},{"id":"http://arxiv.org/abs/2412.15814v2","updated":"2025-03-27T10:13:48Z","published":"2024-12-20T11:43:51Z","title":"Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin\n  Analysis","summary":"  Stablecoins are digital assets designed to maintain a stable value, typically\npegged to traditional currencies. Despite their growing prominence, many\nstablecoins have struggled to consistently meet stability expectations, and\ntheir underlying mechanisms often remain opaque and challenging to analyze.\nThis paper focuses on the DAI stablecoin, which combines\ncrypto-collateralization and algorithmic mechanisms. We propose a formal\nlogic-based framework for representing the policies and operations of DAI,\nimplemented in Prolog and released as open-source software. Our framework\nenables detailed analysis and simulation of DAI's stability mechanisms,\nproviding a foundation for understanding its robustness and identifying\npotential vulnerabilities.\n","authors":["Francesco De Sclavis","Giuseppe Galano","Aldo Glielmo","Matteo Nardelli"],"pdf_url":"https://arxiv.org/pdf/2412.15814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21297v1","updated":"2025-03-27T09:24:18Z","published":"2025-03-27T09:24:18Z","title":"MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level\n  Hardware","summary":"  To efficiently support large-scale NNs, multi-level hardware, leveraging\nadvanced integration and interconnection technologies, has emerged as a\npromising solution to counter the slowdown of Moore's law. However, the vast\ndesign space of such hardware, coupled with the complexity of their spatial\nhierarchies and organizations, introduces significant challenges for design\nspace exploration (DSE). Existing DSE tools, which rely on predefined hardware\ntemplates to explore parameters for specific architectures, fall short in\nexploring diverse organizations, spatial hierarchies, and architectural\npolymorphisms inherent in multi-level hardware. To address these limitations,\nwe present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure\nfor domain-specific DSE of multi-level hardware. MLDSE introduces three key\ninnovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces\na hardware intermediate representation (IR) that can recursively model diverse\nmulti-level hardware with composable elements at various granularities. 2)\nMapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping\nprimitives, facilitating the mapping strategy exploration on multi-level\nhardware, especially synchronization and cross-level communication; 3)\nSimulation: MLDSE supports universal simulator generation based on task-level\nevent-driven simulation mechanism. It features a hardware-consistent scheduling\nalgorithm that can handle general task-level resource contention. Through\nexperiments on LLM workloads, we demonstrate MLDSE's unique capability to\nperform three-tier DSE spanning architecture, hardware parameter, and mapping.\n","authors":["Huanyu Qu","Weihao Zhang","Junfeng Lin","Songchen Ma","Hongyi Li","Luping Shi","Chengzhong Xu"],"pdf_url":"https://arxiv.org/pdf/2503.21297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21279v1","updated":"2025-03-27T08:59:30Z","published":"2025-03-27T08:59:30Z","title":"Asynchronous BFT Consensus Made Wireless","summary":"  Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for\ntheir robustness in unpredictable environments without relying on timing\nassumptions, are becoming increasingly vital for wireless applications. While\nthese protocols have proven effective in wired networks, their adaptation to\nwireless environments presents significant challenges. Asynchronous BFT\nconsensus, characterized by its N parallel consensus components (e.g.,\nasynchronous Byzantine agreement, reliable broadcast), suffers from high\nmessage complexity, leading to network congestion and inefficiency, especially\nin resource-constrained wireless networks. Asynchronous Byzantine agreement\n(ABA) protocols, a foundational component of asynchronous BFT, require careful\nbalancing of message complexity and cryptographic overhead to achieve efficient\nimplementation in wireless settings. Additionally, the absence of dedicated\ntestbeds for asynchronous wireless BFT consensus protocols hinders development\nand performance evaluation. To address these challenges, we propose a consensus\nbatching protocol (ConsensusBatcher), which supports both vertical and\nhorizontal batching of multiple parallel consensus components. We leverage\nConsensusBatcher to adapt three asynchronous BFT consensus protocols\n(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained\nwireless networks. To evaluate the performance of ConsensusBatcher-enabled\nconsensus protocols in wireless environments, we develop and open-source a\ntestbed for deployment and performance assessment of these protocols. Using\nthis testbed, we demonstrate that ConsensusBatcher-based consensus reduces\nlatency by 48% to 59% and increases throughput by 48% to 62% compared to\nbaseline consensus protocols.\n","authors":["Shuo Liu","Minghui Xu","Tianyi Sun","Xiuzhen Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.21279v1.pdf","comment":"Accepted to IEEE ICDCS 2025, 11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.08756v3","updated":"2025-03-27T08:52:35Z","published":"2024-06-13T02:31:36Z","title":"Optimizing Large Model Training through Overlapped Activation\n  Recomputation","summary":"  Large model training often uses recomputation to alleviate memory pressure\nand pipelines to exploit the parallelism of data, tensors, and devices.\nHowever, existing recomputation approaches may incur high overhead when\ntraining real-world models, as they are executed on demand in the critical\ntraining path. In this paper, we present Lynx, a new recomputation framework to\nreduce overhead by overlapping recomputation with communication in training\npipelines. To reduce the large search space for recomputation strategies, we\npropose a heuristic-based recomputation scheduling algorithm, which is based on\nthe observation that there are identical structures in large DNN models so that\nwe can apply the same scheduling policy to all such structures. Additionally,\nwe propose a recomputation-aware model partitioning method to balance each\nstage's execution time for improved training throughput. Our comprehensive\nevaluation using GPT models with 1.3B-23B parameters shows that Lynx\noutperforms existing recomputation approaches by up to 1.37x.\n","authors":["Ping Chen","Wenjie Zhang","Shuibing He","Weijian Chen","Siling Yang","Kexin Huang","Yanlong Yin","Xuan Zhan","Yingjie Gu","Zhuwei Peng","Yi Zheng","Zhefeng Wang","Gang Chen Yingjie Gu","Zhuwei Peng","Kexin Huang","Xuan Zhan","Weijian Chen","Yi Zheng","Zhefeng Wang","Yanlong Yin","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08756v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2503.21206v1","updated":"2025-03-27T06:48:18Z","published":"2025-03-27T06:48:18Z","title":"PilotANN: Memory-Bounded GPU Acceleration for Vector Search","summary":"  Approximate Nearest Neighbor Search (ANNS) has become fundamental to modern\ndeep learning applications, having gained particular prominence through its\nintegration into recent generative models that work with increasingly complex\ndatasets and higher vector dimensions. Existing CPU-only solutions, even the\nmost efficient graph-based ones, struggle to meet these growing computational\ndemands, while GPU-only solutions face memory constraints. As a solution, we\npropose PilotANN, a hybrid CPU-GPU system for graph-based ANNS that utilizes\nboth CPU's abundant RAM and GPU's parallel processing capabilities. Our\napproach decomposes the graph traversal process of top-$k$ search into three\nstages: GPU-accelerated subgraph traversal using SVD-reduced vectors, CPU\nrefinement and precise search using complete vectors. Furthermore, we introduce\nfast entry selection to improve search starting points while maximizing GPU\nutilization. Experimental results demonstrate that PilotANN achieves $3.9 - 5.4\n\\times$ speedup in throughput on 100-million scale datasets, and is able to\nhandle datasets up to $12 \\times$ larger than the GPU memory. We offer a\ncomplete open-source implementation at https://github.com/ytgui/PilotANN.\n","authors":["Yuntao Gui","Peiqi Yin","Xiao Yan","Chaorui Zhang","Weixi Zhang","James Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.21206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20275v2","updated":"2025-03-27T04:16:38Z","published":"2025-03-26T06:49:36Z","title":"Survey of Disaggregated Memory: Cross-layer Technique Insights for\n  Next-Generation Datacenters","summary":"  The growing scale of data requires efficient memory subsystems with large\nmemory capacity and high memory performance. Disaggregated architecture has\nbecome a promising solution for today's cloud and edge computing for its\nscalability and elasticity. As a critical part of disaggregation, disaggregated\nmemory faces many design challenges in many dimensions, including hardware\nscalability, architecture structure, software system design, application\nprogrammability, resource allocation, power management, etc. These challenges\ninspire a number of novel solutions at different system levels to improve\nsystem efficiency. In this paper, we provide a comprehensive review of\ndisaggregated memory, including the methodology and technologies of\ndisaggregated memory system foundation, optimization, and management. We study\nthe technical essentials of disaggregated memory systems and analyze them from\nthe hardware, architecture, system, and application levels. Then, we compare\nthe design details of typical cross-layer designs on disaggregated memory.\nFinally, we discuss the challenges and opportunities of future disaggregated\nmemory works that serve better for next-generation elastic and efficient\ndatacenters.\n","authors":["Jing Wang","Chao Li","Taolei Wang","Jinyang Guo","Hanzhang Yang","Yiming Zhuansun","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2503.20275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21109v1","updated":"2025-03-27T03:03:09Z","published":"2025-03-27T03:03:09Z","title":"Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous\n  Processor Co-Execution","summary":"  Deep Neural Networks (DNNs) are increasingly deployed across diverse\nindustries, driving demand for mobile device support. However, existing mobile\ninference frameworks often rely on a single processor per model, limiting\nhardware utilization and causing suboptimal performance and energy efficiency.\nExpanding DNN accessibility on mobile platforms requires adaptive,\nresource-efficient solutions to meet rising computational needs without\ncompromising functionality. Parallel inference of multiple DNNs on\nheterogeneous processors remains challenging. Some works partition DNN\noperations into subgraphs for parallel execution across processors, but these\noften create excessive subgraphs based only on hardware compatibility,\nincreasing scheduling complexity and memory overhead.\n  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)\nstrategy for optimizing multi-DNN inference on mobile heterogeneous processors.\nADMS constructs an optimal subgraph partitioning strategy offline, balancing\nhardware operation support and scheduling granularity, and uses a\nprocessor-state-aware algorithm to dynamically adjust workloads based on\nreal-time conditions. This ensures efficient workload distribution and\nmaximizes processor utilization. Experiments show ADMS reduces multi-DNN\ninference latency by 4.04 times compared to vanilla frameworks.\n","authors":["Yunquan Gao","Zhiguo Zhang","Praveen Kumar Donta","Chinmaya Kumar Dehury","Xiujun Wang","Dusit Niyato","Qiyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21109v1.pdf","comment":"14 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.21096v1","updated":"2025-03-27T02:29:55Z","published":"2025-03-27T02:29:55Z","title":"Cloud Resource Allocation with Convex Optimization","summary":"  We present a convex optimization framework for overcoming the limitations of\nKubernetes Cluster Autoscaler by intelligently allocating diverse cloud\nresources while minimizing costs and fragmentation. Current Kubernetes scaling\nmechanisms are restricted to homogeneous scaling of existing node types,\nlimiting cost-performance optimization possibilities. Our matrix-based model\ncaptures resource demands, costs, and capacity constraints in a unified\nmathematical framework. A key contribution is our logarithmic approximation to\nthe indicator function, which enables dynamic node type selection while\nmaintaining problem convexity. Our approach balances cost optimization with\noperational complexity through interior-point methods. Experiments with\nreal-world Kubernetes workloads demonstrate reduced costs and improved resource\nutilization compared to conventional Cluster Autoscaler strategies that can\nonly scale up or down existing node pools.\n","authors":["Shayan Boghani","Emin Kirimlioglu","Amrita Moturi","Hao-Ting Tso"],"pdf_url":"https://arxiv.org/pdf/2503.21096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20768v2","updated":"2025-03-27T02:16:06Z","published":"2025-03-26T17:52:30Z","title":"An Empirical Study of the Impact of Federated Learning on Machine\n  Learning Model Accuracy","summary":"  Federated Learning (FL) enables distributed ML model training on private user\ndata at the global scale. Despite the potential of FL demonstrated in many\ndomains, an in-depth view of its impact on model accuracy remains unclear. In\nthis paper, we investigate, systematically, how this learning paradigm can\naffect the accuracy of state-of-the-art ML models for a variety of ML tasks. We\npresent an empirical study that involves various data types: text, image,\naudio, and video, and FL configuration knobs: data distribution, FL scale,\nclient sampling, and local and global computations. Our experiments are\nconducted in a unified FL framework to achieve high fidelity, with substantial\nhuman efforts and resource investments. Based on the results, we perform a\nquantitative analysis of the impact of FL, and highlight challenging scenarios\nwhere applying FL degrades the accuracy of the model drastically and identify\ncases where the impact is negligible. The detailed and extensive findings can\nbenefit practical deployments and future development of FL.\n","authors":["Haotian Yang","Zhuoran Wang","Benson Chou","Sophie Xu","Hao Wang","Jingxian Wang","Qizhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20768v2.pdf","comment":null}],"Some new methods":[{"id":"http://arxiv.org/abs/2503.21775v1","updated":"2025-03-27T17:59:46Z","published":"2025-03-27T17:59:46Z","title":"StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross\n  Fusion","summary":"  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io\n","authors":["Ziyu Guo","Young Yoon Lee","Joseph Liu","Yizhak Ben-Shabat","Victor Zordan","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2503.21775v1.pdf","comment":"Project Page: https://stylemotif.github.io"},{"id":"http://arxiv.org/abs/2406.15341v2","updated":"2025-03-27T17:59:22Z","published":"2024-06-21T17:55:24Z","title":"GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in\n  Alignment with Bioinformaticians","summary":"  Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides annotated code and results for solving a wide\nrange of gene identification problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTex.\n","authors":["Haoyang Liu","Shuyu Chen","Ye Zhang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15341v2.pdf","comment":"29 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.21766v1","updated":"2025-03-27T17:59:02Z","published":"2025-03-27T17:59:02Z","title":"Stable-SCore: A Stable Registration-based Framework for 3D Shape\n  Correspondence","summary":"  Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.\n","authors":["Haolin Liu","Xiaohang Zhan","Zizheng Yan","Zhongjin Luo","Yuxin Wen","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2503.21766v1.pdf","comment":"Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/"},{"id":"http://arxiv.org/abs/2503.21761v1","updated":"2025-03-27T17:57:32Z","published":"2025-03-27T17:57:32Z","title":"Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single\n  Video","summary":"  This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.\n","authors":["David Yifan Yao","Albert J. Zhai","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21761v1.pdf","comment":"CVPR 2025. Project page (with code):\n  https://davidyao99.github.io/uni4d"},{"id":"http://arxiv.org/abs/2503.21757v1","updated":"2025-03-27T17:57:07Z","published":"2025-03-27T17:57:07Z","title":"Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck","summary":"  In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.\n","authors":["Adrian Bulat","Yassine Ouali","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2503.21757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12831v3","updated":"2025-03-27T17:56:31Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing serves as a fundamental pillar of digital media, spanning\napplications in entertainment, education, and professional communication.\nHowever, previous methods often overlook the necessity of comprehensively\nunderstanding both global and local contexts, leading to inaccurate and\ninconsistent edits in the spatiotemporal dimension, especially for long videos.\nIn this paper, we introduce VIA, a unified spatiotemporal Video Adaptation\nframework for global and local video editing, pushing the limits of\nconsistently editing minute-long videos. First, to ensure local consistency\nwithin individual frames, we designed test-time editing adaptation to adapt a\npre-trained image editing model for improving consistency between potential\nediting directions and the text instruction, and adapts masked latent variables\nfor precise local control. Furthermore, to maintain global consistency over the\nvideo sequence, we introduce spatiotemporal adaptation that recursively gather\nconsistent attention variables in key frames and strategically applies them\nacross the whole sequence to realize the editing effects. Extensive experiments\ndemonstrate that, compared to baseline methods, our VIA approach produces edits\nthat are more faithful to the source videos, more coherent in the\nspatiotemporal context, and more precise in local control. More importantly, we\nshow that VIA can achieve consistent long video editing in minutes, unlocking\nthe potential for advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v3.pdf","comment":"18 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.21747v1","updated":"2025-03-27T17:53:50Z","published":"2025-03-27T17:53:50Z","title":"CTRL-O: Language-Controllable Object-Centric Visual Representation\n  Learning","summary":"  Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.\n","authors":["Aniket Didolkar","Andrii Zadaianchuk","Rabiul Awal","Maximilian Seitzer","Efstratios Gavves","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2503.21747v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21735v1","updated":"2025-03-27T17:48:32Z","published":"2025-03-27T17:48:32Z","title":"GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics","summary":"  Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.\n","authors":["Arsham Gholamzadeh Khoee","Shuai Wang","Yinan Yu","Robert Feldt","Dhasarathy Parthasarathy"],"pdf_url":"https://arxiv.org/pdf/2503.21735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21729v1","updated":"2025-03-27T17:44:18Z","published":"2025-03-27T17:44:18Z","title":"ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation","summary":"  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).\n","authors":["Zhicheng Lee","Shulin Cao","Jinxin Liu","Jiajie Zhang","Weichuan Liu","Xiaoyin Che","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2503.21729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05510v2","updated":"2025-03-27T17:40:09Z","published":"2025-01-09T19:00:01Z","title":"OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?","summary":"  Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.\n","authors":["Yifei Li","Junbo Niu","Ziyang Miao","Chunjiang Ge","Yuanhang Zhou","Qihao He","Xiaoyi Dong","Haodong Duan","Shuangrui Ding","Rui Qian","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.05510v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2409.18119v2","updated":"2025-03-27T17:39:55Z","published":"2024-09-26T17:56:59Z","title":"Multi-View and Multi-Scale Alignment for Contrastive Language-Image\n  Pre-training in Mammography","summary":"  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential\nin medical image analysis but requires substantial data and computational\nresources. Due to these restrictions, existing CLIP applications in medical\nimaging focus mainly on modalities like chest X-rays that have abundant\nimage-report data available, leaving many other important modalities\nunderexplored. Here, we propose one of the first adaptations of the full CLIP\nmodel to mammography, which presents significant challenges due to labeled data\nscarcity, high-resolution images with small regions of interest, and class-wise\nimbalance. We first develop a specialized supervision framework for mammography\nthat leverages its multi-view nature. Furthermore, we design a symmetric local\nalignment module to better focus on detailed features in high-resolution\nimages. Lastly, we incorporate a parameter-efficient fine-tuning approach for\nlarge language models pre-trained with medical knowledge to address data\nlimitations. Our multi-view and multi-scale alignment (MaMA) method outperforms\nstate-of-the-art baselines for three different tasks on two large real-world\nmammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared\nwith the largest baseline. The code is available at\nhttps://github.com/XYPB/MaMA\n","authors":["Yuexi Du","John Onofrey","Nicha C. Dvornek"],"pdf_url":"https://arxiv.org/pdf/2409.18119v2.pdf","comment":"This paper is accepted by IPMI 2025 for Oral Presentation"},{"id":"http://arxiv.org/abs/2503.21720v1","updated":"2025-03-27T17:34:25Z","published":"2025-03-27T17:34:25Z","title":"Collab: Controlled Decoding using Mixture of Agents for LLM Alignment","summary":"  Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.\n","authors":["Souradip Chakraborty","Sujay Bhatt","Udari Madhushani Sehwag","Soumya Suvra Ghosal","Jiahao Qiu","Mengdi Wang","Dinesh Manocha","Furong Huang","Alec Koppel","Sumitra Ganesh"],"pdf_url":"https://arxiv.org/pdf/2503.21720v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2503.21718v1","updated":"2025-03-27T17:30:50Z","published":"2025-03-27T17:30:50Z","title":"Outlier dimensions favor frequent tokens in language model","summary":"  We study last-layer outlier dimensions, i.e.dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic.\n","authors":["Iuri Macocco","Nora Graichen","Gemma Boleda","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2503.21718v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06608v3","updated":"2025-03-27T17:25:50Z","published":"2025-02-10T16:07:54Z","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models","summary":"  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.\n","authors":["Yangguang Li","Zi-Xin Zou","Zexiang Liu","Dehu Wang","Yuan Liang","Zhipeng Yu","Xingchao Liu","Yuan-Chen Guo","Ding Liang","Wanli Ouyang","Yan-Pei Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21708v1","updated":"2025-03-27T17:20:44Z","published":"2025-03-27T17:20:44Z","title":"Elementwise Layer Normalization","summary":"  A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer\nNormalization. Although the method is empirically well-motivated and appealing\nfrom a practical point of view, it lacks a theoretical foundation. In this\nwork, we derive DyT mathematically and show that a well-defined approximation\nis needed to do so. By dropping said approximation, an alternative element-wise\ntransformation is obtained, which we call Elementwise Layer Normalization\n(ELN). We demonstrate that ELN resembles Layer Normalization more accurately\nthan DyT does.\n","authors":["Felix Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2503.21708v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.20074v2","updated":"2025-03-27T17:16:44Z","published":"2025-03-25T21:20:11Z","title":"Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience","summary":"  The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.\n","authors":["Yahav Biran","Imry Kissos"],"pdf_url":"https://arxiv.org/pdf/2503.20074v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.21699v1","updated":"2025-03-27T17:04:33Z","published":"2025-03-27T17:04:33Z","title":"MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX","summary":"  Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.\n","authors":["Liuyue Xie","George Z. Wei","Avik Kuthiala","Ce Zheng","Ananya Bal","Mosam Dabhi","Liting Wen","Taru Rustagi","Ethan Lai","Sushil Khyalia","Rohan Choudhury","Morteza Ziyadi","Xu Zhang","Hao Yang","László A. Jeni"],"pdf_url":"https://arxiv.org/pdf/2503.21699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21695v1","updated":"2025-03-27T16:59:39Z","published":"2025-03-27T16:59:39Z","title":"AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model\n  for High-Fidelity Histology Nuclei Segmentation","summary":"  Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.\n","authors":["Jiahe Qian","Yaoyu Fang","Jinkui Hao","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21695v1.pdf","comment":"13 pages, 4 tables, 2 figures"},{"id":"http://arxiv.org/abs/2503.21694v1","updated":"2025-03-27T16:59:15Z","published":"2025-03-27T16:59:15Z","title":"Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data","summary":"  It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.\n","authors":["Zhiyuan Ma","Xinyue Liang","Rongyuan Wu","Xiangyu Zhu","Zhen Lei","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21694v1.pdf","comment":"Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo"},{"id":"http://arxiv.org/abs/2503.21683v1","updated":"2025-03-27T16:52:25Z","published":"2025-03-27T16:52:25Z","title":"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning","summary":"  In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.\n","authors":["Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21674v1","updated":"2025-03-27T16:41:57Z","published":"2025-03-27T16:41:57Z","title":"Intelligent IoT Attack Detection Design via ODLLM with Feature\n  Ranking-based Knowledge Base","summary":"  The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity.\n","authors":["Satvik Verma","Qun Wang","E. Wes Bethel"],"pdf_url":"https://arxiv.org/pdf/2503.21674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21670v1","updated":"2025-03-27T16:36:39Z","published":"2025-03-27T16:36:39Z","title":"COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing","summary":"  The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.\n","authors":["Rajvee Sheth","Himanshu Beniwal","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2503.21670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21668v1","updated":"2025-03-27T16:35:02Z","published":"2025-03-27T16:35:02Z","title":"Cognitive Science-Inspired Evaluation of Core Capabilities for Object\n  Understanding in AI","summary":"  One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.\n","authors":["Danaja Rutar","Alva Markelius","Konstantinos Voudouris","José Hernández-Orallo","Lucy Cheke"],"pdf_url":"https://arxiv.org/pdf/2503.21668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18410v2","updated":"2025-03-27T16:34:13Z","published":"2025-02-25T18:04:45Z","title":"TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time\n  Series Forecasting","summary":"  Time series forecasting has long been a focus of research across diverse\nfields, including economics, energy, healthcare, and traffic management. Recent\nworks have introduced innovative architectures for time series models, such as\nthe Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs)\nto enhance prediction accuracy by effectively capturing both spatial and\ntemporal dependencies within the data. In this paper, we investigate the\ncapabilities of the Kolmogorov-Arnold Networks (KANs) for time-series\nforecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental\nresults demonstrate that TSKANMixer tends to improve prediction accuracy over\nthe original TSMixer across multiple datasets, ranking among the top-performing\nmodels compared to other time series approaches. Our results show that the KANs\nare promising alternatives to improve the performance of time series\nforecasting by replacing or extending traditional MLPs.\n","authors":["Young-Chae Hong","Bei Xiao","Yangho Chen"],"pdf_url":"https://arxiv.org/pdf/2502.18410v2.pdf","comment":"8 pages, 4 figures, 7 tables and accepted at the AI4TS: AI for Time\n  Series Analysis workshop, AAAI 2025"},{"id":"http://arxiv.org/abs/2503.21657v1","updated":"2025-03-27T16:21:53Z","published":"2025-03-27T16:21:53Z","title":"Model Assembly Learning with Heterogeneous Layer Weight Merging","summary":"  Model merging acquires general capabilities without extra data or training by\ncombining multiple models' parameters. Previous approaches achieve linear mode\nconnectivity by aligning parameters into the same loss basin using permutation\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\nparadigm for model merging that iteratively integrates parameters from diverse\nmodels in an open-ended model zoo to enhance the base model's capabilities.\nUnlike previous works that require identical architectures, MAL allows the\nmerging of heterogeneous architectures and selective parameters across layers.\nSpecifically, the base model can incorporate parameters from different layers\nof multiple pre-trained models. We systematically investigate the conditions\nand fundamental settings of heterogeneous parameter merging, addressing all\npossible mismatches in layer widths between the base and target models.\nFurthermore, we establish key laws and provide practical guidelines for\neffectively implementing MAL.\n","authors":["Yi-Kai Zhang","Jin Wang","Xu-Xiang Zhong","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2503.21657v1.pdf","comment":"ICLR 2025 Workshop on Neural Network Weights as a New Data Modality"},{"id":"http://arxiv.org/abs/2409.15272v4","updated":"2025-03-27T16:21:06Z","published":"2024-09-23T17:59:05Z","title":"OmniBench: Towards The Future of Universal Omni-Language Models","summary":"  Recent advancements in multimodal large language models (MLLMs) have focused\non integrating multiple modalities, yet their ability to simultaneously process\nand reason across different inputs remains underexplored. We introduce\nOmniBench, a novel benchmark designed to evaluate models' ability to recognize,\ninterpret, and reason across visual, acoustic, and textual inputs\nsimultaneously. We define language models capable of such tri-modal processing\nas omni-language models (OLMs). OmniBench features high-quality human\nannotations that require integrated understanding across all modalities. Our\nevaluation reveals that: i) open-source OLMs show significant limitations in\ninstruction-following and reasoning in tri-modal contexts; and ii) most\nbaseline models perform poorly (around 50% accuracy) even with textual\nalternatives to image/audio inputs. To address these limitations, we develop\nOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We\nadvocate for developing more robust tri-modal integration techniques and\ntraining strategies to enhance OLM performance. Codes and data could be found\nat our repo (https://github.com/multimodal-art-projection/OmniBench).\n","authors":["Yizhi Li","Ge Zhang","Yinghao Ma","Ruibin Yuan","Kang Zhu","Hangyu Guo","Yiming Liang","Jiaheng Liu","Zekun Wang","Jian Yang","Siwei Wu","Xingwei Qu","Jinjie Shi","Xinyue Zhang","Zhenzhu Yang","Xiangzhou Wang","Zhaoxiang Zhang","Zachary Liu","Emmanouil Benetos","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12257v3","updated":"2025-03-27T16:21:02Z","published":"2024-06-18T04:10:38Z","title":"CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large\n  Language Models","summary":"  The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead.\n","authors":["Yuetai Li","Zhangchen Xu","Fengqing Jiang","Luyao Niu","Dinuka Sahabandu","Bhaskar Ramasubramanian","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2406.12257v3.pdf","comment":"This paper is presented at EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.06581v6","updated":"2025-03-27T16:16:09Z","published":"2024-07-09T06:20:17Z","title":"Vision language models are blind: Failing to translate detailed visual\n  features into words","summary":"  While large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro, score high on many vision-understanding benchmarks, they are\nstill struggling with low-level vision tasks that are easy to humans.\nSpecifically, on BlindTest, our suite of 7 very simple tasks, including\nidentifying (a) whether two circles overlap; (b) how many times two lines\nintersect; (c) which letter is being circled in a word; and (d) the number of\ncircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%\naccurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,\nfar from the human expected accuracy of 100%. Across different image\nresolutions and line widths, VLMs including slow-thinking models consistently\nstruggle with those tasks that require precise spatial information when\ngeometric primitives overlap or are close. Yet, VLMs perform at near-100%\naccuracy when much more space is added to separate shapes and letters. Linear\nprobing experiments show that vision encoders contain sufficient visual\ninformation to solve BlindTest and that language models fail to decode this\ninformation into correct answers. Code and data are at:\nhttps://vlmsareblind.github.io\n","authors":["Pooyan Rahmanzadehgervi","Logan Bolton","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.06581v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21646v1","updated":"2025-03-27T16:10:02Z","published":"2025-03-27T16:10:02Z","title":"Unlocking the Potential of Past Research: Using Generative AI to\n  Reconstruct Healthcare Simulation Models","summary":"  Discrete-event simulation (DES) is widely used in healthcare Operations\nResearch, but the models themselves are rarely shared. This limits their\npotential for reuse and long-term impact in the modelling and healthcare\ncommunities. This study explores the feasibility of using generative artificial\nintelligence (AI) to recreate published models using Free and Open Source\nSoftware (FOSS), based on the descriptions provided in an academic journal.\nUsing a structured methodology, we successfully generated, tested and\ninternally reproduced two DES models, including user interfaces. The reported\nresults were replicated for one model, but not the other, likely due to missing\ninformation on distributions. These models are substantially more complex than\nAI-generated DES models published to date. Given the challenges we faced in\nprompt engineering, code generation, and model testing, we conclude that our\niterative approach to model development, systematic comparison and testing, and\nthe expertise of our team were necessary to the success of our recreated\nsimulation models.\n","authors":["Thomas Monks","Alison Harper","Amy Heather"],"pdf_url":"https://arxiv.org/pdf/2503.21646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08180v2","updated":"2025-03-27T16:07:18Z","published":"2025-02-12T07:37:39Z","title":"Enhancing LLM Character-Level Manipulation via Divide and Conquer","summary":"  Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.\n","authors":["Zhen Xiong","Yujun Cai","Bryan Hooi","Nanyun Peng","Zhecheng Li","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21640v1","updated":"2025-03-27T16:06:59Z","published":"2025-03-27T16:06:59Z","title":"Towards Fully Automated Decision-Making Systems for Greenhouse Control:\n  Challenges and Opportunities","summary":"  Machine learning has been successful in building control policies to drive a\ncomplex system to desired states in various applications (e.g. games, robotics,\netc.). To be specific, a number of parameters of policy can be automatically\noptimized from the observations of environment to be able to generate a\nsequence of decisions leading to the best performance. In this survey paper, we\nparticularly explore such policy-learning techniques for another unique,\npractical use-case scenario--farming, in which critical decisions (e.g., water\nsupply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,\ndamage to plants) while maximizing the revenue (e.g., healthy crops) in the\nend. We first provide a broad overview of latest studies on it to identify not\nonly domain-specific challenges but opportunities with potential solutions,\nsome of which are suggested as promising directions for future research. Also,\nwe then introduce our successful approach to being ranked second among 46 teams\nat the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to\ndiscuss the lessons learned about important considerations for design to create\nautonomous farm-management systems.\n","authors":["Yongshuai Liu","Taeyeong Choi","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11593v2","updated":"2025-03-27T15:57:57Z","published":"2024-09-17T22:58:20Z","title":"Self-Contrastive Forward-Forward Algorithm","summary":"  Agents that operate autonomously benefit from lifelong learning capabilities.\nHowever, compatible training algorithms must comply with the decentralized\nnature of these systems, which imposes constraints on both the parameter counts\nand the computational resources. The Forward-Forward (FF) algorithm is one of\nthese. FF relies only on feedforward operations, the same used for inference,\nfor optimizing layer-wise objectives. This purely forward approach eliminates\nthe need for transpose operations required in traditional backpropagation.\nDespite its potential, FF has failed to reach state-of-the-art performance on\nmost standard benchmark tasks, in part due to unreliable negative data\ngeneration methods for unsupervised learning.\n  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)\nalgorithm, a competitive training method aimed at closing this performance gap.\nInspired by standard self-supervised contrastive learning for vision tasks,\nSCFF generates positive and negative inputs applicable across various datasets.\nThe method demonstrates superior performance compared to existing unsupervised\nlocal learning algorithms on several benchmark datasets, including MNIST,\nCIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training\nrecurrent neural networks, expanding its utility to sequential data tasks.\nThese findings pave the way for high-accuracy, real-time learning on\nresource-constrained edge devices.\n","authors":["Xing Chen","Dongshu Liu","Jeremie Laydevant","Julie Grollier"],"pdf_url":"https://arxiv.org/pdf/2409.11593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21634v1","updated":"2025-03-27T15:56:55Z","published":"2025-03-27T15:56:55Z","title":"When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in\n  Morocco","summary":"  The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.\n","authors":["Yassir Lairgi"],"pdf_url":"https://arxiv.org/pdf/2503.21634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16822v2","updated":"2025-03-27T15:42:18Z","published":"2024-12-22T02:04:17Z","title":"Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for\n  Efficient Diffusion Transformers","summary":"  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image\ngeneration quality but suffer from high latency and memory inefficiency, making\nthem difficult to deploy on resource-constrained devices. One major efficiency\nbottleneck is that existing DiTs apply equal computation across all regions of\nan image. However, not all image tokens are equally important, and certain\nlocalized areas require more computation, such as objects. To address this, we\npropose DiffCR, a dynamic DiT inference framework with differentiable\ncompression ratios, which automatically learns to dynamically route computation\nacross layers and timesteps for each image token, resulting in efficient DiTs.\nSpecifically, DiffCR integrates three features: (1) A token-level routing\nscheme where each DiT layer includes a router that is fine-tuned jointly with\nmodel weights to predict token importance scores. In this way, unimportant\ntokens bypass the entire layer's computation; (2) A layer-wise differentiable\nratio mechanism where different DiT layers automatically learn varying\ncompression ratios from a zero initialization, resulting in large compression\nratios in redundant layers while others remain less compressed or even\nuncompressed; (3) A timestep-wise differentiable ratio mechanism where each\ndenoising timestep learns its own compression ratio. The resulting pattern\nshows higher ratios for noisier timesteps and lower ratios as the image becomes\nclearer. Extensive experiments on text-to-image and inpainting tasks show that\nDiffCR effectively captures dynamism across token, layer, and timestep axes,\nachieving superior trade-offs between generation quality and efficiency\ncompared to prior works. The project website is available at\nhttps://www.haoranyou.com/diffcr.\n","authors":["Haoran You","Connelly Barnes","Yuqian Zhou","Yan Kang","Zhenbang Du","Wei Zhou","Lingzhi Zhang","Yotam Nitzan","Xiaoyang Liu","Zhe Lin","Eli Shechtman","Sohrab Amirghodsi","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2412.16822v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21620v1","updated":"2025-03-27T15:39:30Z","published":"2025-03-27T15:39:30Z","title":"UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning","summary":"  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.\n","authors":["Zhengxi Lu","Yuxiang Chai","Yaxuan Guo","Xi Yin","Liang Liu","Hao Wang","Guanjing Xiong","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.21620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15220v4","updated":"2025-03-27T15:37:03Z","published":"2023-07-27T22:38:12Z","title":"Learning Multi-modal Representations by Watching Hundreds of Surgical\n  Video Lectures","summary":"  Recent advancements in surgical computer vision applications have been driven\nby vision-only models, which do not explicitly integrate the rich semantics of\nlanguage into their design. These methods rely on manually annotated surgical\nvideos to predict a fixed set of object categories, limiting their\ngeneralizability to unseen surgical procedures and downstream tasks. In this\nwork, we put forward the idea that the surgical video lectures available\nthrough open surgical e-learning platforms can provide effective vision and\nlanguage supervisory signals for multi-modal representation learning without\nrelying on manual annotations. We address the surgery-specific linguistic\nchallenges present in surgical video lectures by employing multiple\ncomplementary automatic speech recognition systems to generate text\ntranscriptions. We then present a novel method, SurgVLP - Surgical Vision\nLanguage Pre-training, for multi-modal representation learning. Extensive\nexperiments across diverse surgical procedures and tasks demonstrate that the\nmulti-modal representations learned by SurgVLP exhibit strong transferability\nand adaptability in surgical video analysis. Furthermore, our zero-shot\nevaluations highlight SurgVLP's potential as a general-purpose foundation model\nfor surgical workflow analysis, reducing the reliance on extensive manual\nannotations for downstream tasks, and facilitating adaptation methods such as\nfew-shot learning to build a scalable and data-efficient solution for various\ndownstream surgical applications. The [training\ncode](https://github.com/CAMMA-public/SurgVLP) and\n[weights](https://github.com/CAMMA-public/PeskaVLP) are public.\n","authors":["Kun Yuan","Vinkle Srivastav","Tong Yu","Joel L. Lavanchy","Jacques Marescaux","Pietro Mascagni","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2307.15220v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21615v1","updated":"2025-03-27T15:36:49Z","published":"2025-03-27T15:36:49Z","title":"A Measure Based Generalizable Approach to Understandability","summary":"  Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.\n","authors":["Vikas Kushwaha","Sruti Srinivasa Ragavan","Subhajit Roy"],"pdf_url":"https://arxiv.org/pdf/2503.21615v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2503.21602v1","updated":"2025-03-27T15:22:02Z","published":"2025-03-27T15:22:02Z","title":"GenEdit: Compounding Operators and Continuous Improvement to Tackle\n  Text-to-SQL in the Enterprise","summary":"  Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations.\n","authors":["Karime Maamari","Connor Landy","Amine Mhedhbi"],"pdf_url":"https://arxiv.org/pdf/2503.21602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21598v1","updated":"2025-03-27T15:19:55Z","published":"2025-03-27T15:19:55Z","title":"Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing","summary":"  Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.\n","authors":["Johan Wahréus","Ahmed Hussain","Panos Papadimitratos"],"pdf_url":"https://arxiv.org/pdf/2503.21598v1.pdf","comment":"22 pages; 26 figures"},{"id":"http://arxiv.org/abs/2503.21592v1","updated":"2025-03-27T15:08:58Z","published":"2025-03-27T15:08:58Z","title":"Critical Iterative Denoising: A Discrete Generative Model Applied to\n  Graphs","summary":"  Discrete Diffusion and Flow Matching models have significantly advanced\ngenerative modeling for discrete structures, including graphs. However, the\ntime dependencies in the noising process of these models lead to error\naccumulation and propagation during the backward process. This issue,\nparticularly pronounced in mask diffusion, is a known limitation in sequence\nmodeling and, as we demonstrate, also impacts discrete diffusion models for\ngraphs.\n  To address this problem, we propose a novel framework called Iterative\nDenoising, which simplifies discrete diffusion and circumvents the issue by\nassuming conditional independence across time. Additionally, we enhance our\nmodel by incorporating a Critic, which during generation selectively retains or\ncorrupts elements in an instance based on their likelihood under the data\ndistribution. Our empirical evaluations demonstrate that the proposed method\nsignificantly outperforms existing discrete diffusion baselines in graph\ngeneration tasks.\n","authors":["Yoann Boget","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2503.21592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21581v1","updated":"2025-03-27T14:59:59Z","published":"2025-03-27T14:59:59Z","title":"AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion","summary":"  Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.\n","authors":["Liuyue Xie","Jiancong Guo","Ozan Cakmakci","Andre Araujo","Laszlo A. Jeni","Zhiheng Jia"],"pdf_url":"https://arxiv.org/pdf/2503.21581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21571v1","updated":"2025-03-27T14:52:06Z","published":"2025-03-27T14:52:06Z","title":"Magnitude-Phase Dual-Path Speech Enhancement Network based on\n  Self-Supervised Embedding and Perceptual Contrast Stretch Boosting","summary":"  Speech self-supervised learning (SSL) has made great progress in various\nspeech processing tasks, but there is still room for improvement in speech\nenhancement (SE). This paper presents BSP-MPNet, a dual-path framework that\ncombines self-supervised features with magnitude-phase information for SE. The\napproach starts by applying the perceptual contrast stretching (PCS) algorithm\nto enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC)\nencoder then extracts coarse features from the enhanced spectrum. Next, a\nfeature-separating self-supervised learning (FS-SSL) model generates\nself-supervised embeddings for the magnitude and phase components separately.\nThese embeddings are fused to create cross-domain feature representations.\nFinally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine\nthe features, apply them to the mask, and reconstruct the speech signal. We\nevaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental\nresults show that BSP-MPNet outperforms existing methods under various noise\nconditions, providing new directions for self-supervised speech enhancement\nresearch. The implementation of the BSP-MPNet code is available\nonline\\footnote[2]{https://github.com/AlimMat/BSP-MPNet. \\label{s1}}\n","authors":["Alimjan Mattursun","Liejun Wang","Yinfeng Yu","Chunyang Ma"],"pdf_url":"https://arxiv.org/pdf/2503.21571v1.pdf","comment":"Main paper (6 pages). Accepted for publication by ICME 2025"},{"id":"http://arxiv.org/abs/2503.21558v1","updated":"2025-03-27T14:43:42Z","published":"2025-03-27T14:43:42Z","title":"A Local Perspective-based Model for Overlapping Community Detection","summary":"  Community detection, which identifies densely connected node clusters with\nsparse between-group links, is vital for analyzing network structure and\nfunction in real-world systems. Most existing community detection methods based\non GCNs primarily focus on node-level information while overlooking\ncommunity-level features, leading to performance limitations on large-scale\nnetworks. To address this issue, we propose LQ-GCN, an overlapping community\ndetection model from a local community perspective. LQ-GCN employs a\nBernoulli-Poisson model to construct a community affiliation matrix and form an\nend-to-end detection framework. By adopting local modularity as the objective\nfunction, the model incorporates local community information to enhance the\nquality and accuracy of clustering results. Additionally, the conventional GCNs\narchitecture is optimized to improve the model capability in identifying\noverlapping communities in large-scale networks. Experimental results\ndemonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual\nInformation (NMI) and a 26.3% improvement in Recall compared to baseline models\nacross multiple real-world benchmark datasets.\n","authors":["Gaofeng Zhou","Rui-Feng Wang","Kangning Cui"],"pdf_url":"https://arxiv.org/pdf/2503.21558v1.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.21557v1","updated":"2025-03-27T14:43:28Z","published":"2025-03-27T14:43:28Z","title":"debug-gym: A Text-Based Environment for Interactive Debugging","summary":"  Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.\n","authors":["Xingdi Yuan","Morgane M Moss","Charbel El Feghali","Chinmay Singh","Darya Moldavskaya","Drew MacPhee","Lucas Caccia","Matheus Pereira","Minseon Kim","Alessandro Sordoni","Marc-Alexandre Côté"],"pdf_url":"https://arxiv.org/pdf/2503.21557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04765v2","updated":"2025-03-27T14:42:53Z","published":"2025-01-08T18:38:25Z","title":"TREAD: Token Routing for Efficient Architecture-agnostic Diffusion\n  Training","summary":"  Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes.\n","authors":["Felix Krause","Timy Phan","Ming Gui","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2501.04765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21544v1","updated":"2025-03-27T14:34:28Z","published":"2025-03-27T14:34:28Z","title":"SWI: Speaking with Intent in Large Language Models","summary":"  Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.\n","authors":["Yuwei Yin","EunJeong Hwang","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2503.21544v1.pdf","comment":"24 pages. Code: https://github.com/YuweiYin/SWI"},{"id":"http://arxiv.org/abs/2503.21541v1","updated":"2025-03-27T14:32:17Z","published":"2025-03-27T14:32:17Z","title":"LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing","summary":"  Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\n\\method consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/\n","authors":["Achint Soni","Meet Soni","Sirisha Rambhatla"],"pdf_url":"https://arxiv.org/pdf/2503.21541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13731v2","updated":"2025-03-27T14:20:21Z","published":"2025-02-19T13:56:20Z","title":"Robust Counterfactual Inference in Markov Decision Processes","summary":"  This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.\n","authors":["Jessica Lally","Milad Kazemi","Nicola Paoletti"],"pdf_url":"https://arxiv.org/pdf/2502.13731v2.pdf","comment":"Fixed typo in Equation (5)"},{"id":"http://arxiv.org/abs/2503.21530v1","updated":"2025-03-27T14:18:50Z","published":"2025-03-27T14:18:50Z","title":"Low-Resource Transliteration for Roman-Urdu and Urdu Using\n  Transformer-Based Models","summary":"  As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks.\n","authors":["Umer Butt","Stalin Veranasi","Günter Neumann"],"pdf_url":"https://arxiv.org/pdf/2503.21530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21522v1","updated":"2025-03-27T14:10:33Z","published":"2025-03-27T14:10:33Z","title":"MONO2REST: Identifying and Exposing Microservices: a Reusable\n  RESTification Approach","summary":"  The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.\n","authors":["Matthéo Lecrivain","Hanifa Barry","Dalila Tamzalit","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2503.21522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21514v1","updated":"2025-03-27T14:05:16Z","published":"2025-03-27T14:05:16Z","title":"Quantitative Evaluation of Quantum/Classical Neural Network Using a Game\n  Solver Metric","summary":"  To evaluate the performance of quantum computing systems relative to\nclassical counterparts and explore the potential for quantum advantage, we\npropose a game-solving benchmark based on Elo ratings in the game of\ntic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum\nconvolutional neural networks (QCNNs), and hybrid classical-quantum models by\nassessing their performance against a random-move agent in automated matches.\nAdditionally, we implement a QCNN integrated with quantum communication and\nevaluate its performance to quantify the overhead introduced by noisy quantum\nchannels. Our results show that the classical-quantum hybrid model achieves Elo\nratings comparable to those of classical CNNs, while the standalone QCNN\nunderperforms under current hardware constraints. The communication overhead\nwas found to be modest. These findings demonstrate the viability of using\ngame-based benchmarks for evaluating quantum computing systems and suggest that\nquantum communication can be incorporated with limited impact on performance,\nproviding a foundation for future hybrid quantum applications.\n","authors":["Suzukaze Kamei","Hideaki Kawaguchi","Shin Nishio","Tatakahiko Satoh"],"pdf_url":"https://arxiv.org/pdf/2503.21514v1.pdf","comment":"11 pages, 16 figures"},{"id":"http://arxiv.org/abs/2402.08514v2","updated":"2025-03-27T13:59:05Z","published":"2024-02-13T15:10:30Z","title":"Counterfactual Influence in Markov Decision Processes","summary":"  Our work addresses a fundamental problem in the context of counterfactual\ninference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this\nkind of inference allows us to derive counterfactual paths $\\tau'$ describing\nwhat-if versions of $\\tau$ obtained under different action sequences than those\nobserved in $\\tau$. However, as the counterfactual states and actions deviate\nfrom the observed ones over time, the observation $\\tau$ may no longer\ninfluence the counterfactual world, meaning that the analysis is no longer\ntailored to the individual observation, resulting in interventional outcomes\nrather than counterfactual ones. Even though this issue specifically affects\nthe popular Gumbel-max structural causal model used for MDP counterfactuals, it\nhas remained overlooked until now. In this work, we introduce a formal\ncharacterisation of influence based on comparing counterfactual and\ninterventional distributions. We devise an algorithm to construct\ncounterfactual models that automatically satisfy influence constraints.\nLeveraging such models, we derive counterfactual policies that are not just\noptimal for a given reward structure but also remain tailored to the observed\npath. Even though there is an unavoidable trade-off between policy optimality\nand strength of influence constraints, our experiments demonstrate that it is\npossible to derive (near-)optimal policies while remaining under the influence\nof the observation.\n","authors":["Milad Kazemi","Jessica Lally","Ekaterina Tishchenko","Hana Chockler","Nicola Paoletti"],"pdf_url":"https://arxiv.org/pdf/2402.08514v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.14847v2","updated":"2025-03-27T13:49:30Z","published":"2024-11-22T10:47:47Z","title":"Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D\n  Reconstruction","summary":"  The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction. Existing approaches mainly rely\non full-length multi-view videos, while there has been limited exploration of\nonline reconstruction methods that enable on-the-fly training and per-timestep\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features as well as neglecting the\ntemporal continuity in the scene. To address these limitations, we propose a\nnovel three-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage to distinguish\ndynamic and static primitives and optimize their movements, and an error-guided\ndensification stage to accommodate emerging objects. Our method achieves\nstate-of-the-art performance in online 4D reconstruction, demonstrating the\nfastest on-the-fly training, superior representation quality, and real-time\nrendering capability. Project page: https://www.liuzhening.top/DASS\n","authors":["Zhening Liu","Yingdong Hu","Xinjie Zhang","Rui Song","Jiawei Shao","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14847v2.pdf","comment":"Project page: https://www.liuzhening.top/DASS"},{"id":"http://arxiv.org/abs/2402.11317v2","updated":"2025-03-27T13:46:17Z","published":"2024-02-17T16:03:35Z","title":"Debiased Offline Representation Learning for Fast Online Adaptation in\n  Non-stationary Dynamics","summary":"  Developing policies that can adjust to non-stationary environments is\nessential for real-world reinforcement learning applications. However, learning\nsuch adaptable policies in offline settings, with only a limited set of\npre-collected trajectories, presents significant challenges. A key difficulty\narises because the limited offline data makes it hard for the context encoder\nto differentiate between changes in the environment dynamics and shifts in the\nbehavior policy, often leading to context misassociations. To address this\nissue, we introduce a novel approach called Debiased Offline Representation for\nfast online Adaptation (DORA). DORA incorporates an information bottleneck\nprinciple that maximizes mutual information between the dynamics encoding and\nthe environmental data, while minimizing mutual information between the\ndynamics encoding and the actions of the behavior policy. We present a\npractical implementation of DORA, leveraging tractable bounds of the\ninformation bottleneck principle. Our experimental evaluation across six\nbenchmark MuJoCo tasks with variable parameters demonstrates that DORA not only\nachieves a more precise dynamics encoding but also significantly outperforms\nexisting baselines in terms of performance.\n","authors":["Xinyu Zhang","Wenjie Qiu","Yi-Chen Li","Lei Yuan","Chengxing Jia","Zongzhang Zhang","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2402.11317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21504v1","updated":"2025-03-27T13:45:35Z","published":"2025-03-27T13:45:35Z","title":"Keyword-Oriented Multimodal Modeling for Euphemism Identification","summary":"  Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.\n","authors":["Yuxue Hu","Junsong Li","Meixuan Chen","Dongyu Su","Tongguan Wang","Ying Sha"],"pdf_url":"https://arxiv.org/pdf/2503.21504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06635v2","updated":"2025-03-27T13:44:59Z","published":"2025-03-09T14:24:09Z","title":"Deep Cut-informed Graph Embedding and Clustering","summary":"  Graph clustering aims to divide the graph into different clusters. The\nrecently emerging deep graph clustering approaches are largely built on graph\nneural networks (GNN). However, GNN is designed for general graph encoding and\nthere is a common issue of representation collapse in existing GNN-based deep\ngraph clustering algorithms. We attribute two main reasons for such issues: (i)\nthe inductive bias of GNN models: GNNs tend to generate similar representations\nfor proximal nodes. Since graphs often contain a non-negligible amount of\ninter-cluster links, the bias results in error message passing and leads to\nbiased clustering; (ii) the clustering guided loss function: most traditional\napproaches strive to make all samples closer to pre-learned cluster centers,\nwhich causes a degenerate solution assigning all data points to a single label\nthus make all samples and less discriminative. To address these challenges, we\ninvestigate graph clustering from a graph cut perspective and propose an\ninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering\nframework, namely DCGC. This framework includes two modules: (i) cut-informed\ngraph encoding; (ii) self-supervised graph clustering via optimal transport.\nFor the encoding module, we derive a cut-informed graph embedding objective to\nfuse graph structure and attributes by minimizing their joint normalized cut.\nFor the clustering module, we utilize the optimal transport theory to obtain\nthe clustering assignments, which can balance the guidance of \"proximity to the\npre-learned cluster center\". With the above two tailored designs, DCGC is more\nsuitable for the graph clustering task, which can effectively alleviate the\nproblem of representation collapse and achieve better performance. We conduct\nextensive experiments to demonstrate that our method is simple but effective\ncompared with benchmarks.\n","authors":["Zhiyuan Ning","Zaitian Wang","Ran Zhang","Ping Xu","Kunpeng Liu","Pengyang Wang","Wei Ju","Pengfei Wang","Yuanchun Zhou","Erik Cambria","Chong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.06635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01072v3","updated":"2025-03-27T13:42:00Z","published":"2024-08-02T07:47:51Z","title":"A Survey on Self-play Methods in Reinforcement Learning","summary":"  Self-play, characterized by agents' interactions with copies or past versions\nof themselves, has recently gained prominence in reinforcement learning (RL).\nThis paper first clarifies the preliminaries of self-play, including the\nmulti-agent reinforcement learning framework and basic game theory concepts.\nThen, it provides a unified framework and classifies existing self-play\nalgorithms within this framework. Moreover, the paper bridges the gap between\nthe algorithms and their practical implications by illustrating the role of\nself-play in different scenarios. Finally, the survey highlights open\nchallenges and future research directions in self-play. This paper is an\nessential guide map for understanding the multifaceted landscape of self-play\nin RL.\n","authors":["Ruize Zhang","Zelai Xu","Chengdong Ma","Chao Yu","Wei-Wei Tu","Wenhao Tang","Shiyu Huang","Deheng Ye","Wenbo Ding","Yaodong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01072v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21495v1","updated":"2025-03-27T13:32:42Z","published":"2025-03-27T13:32:42Z","title":"Adaptive Resampling with Bootstrap for Noisy Multi-Objective\n  Optimization Problems","summary":"  The challenge of noisy multi-objective optimization lies in the constant\ntrade-off between exploring new decision points and improving the precision of\nknown points through resampling. This decision should take into account both\nthe variability of the objective functions and the current estimate of a point\nin relation to the Pareto front. Since the amount and distribution of noise are\ngenerally unknown, it is desirable for a decision function to be highly\nadaptive to the properties of the optimization problem. This paper presents a\nresampling decision function that incorporates the stochastic nature of the\noptimization problem by using bootstrapping and the probability of dominance.\nThe distribution-free estimation of the probability of dominance is achieved\nusing bootstrap estimates of the means. To make the procedure applicable even\nwith very few observations, we transfer the distribution observed at other\ndecision points. The efficiency of this resampling approach is demonstrated by\napplying it in the NSGA-II algorithm with a sequential resampling procedure\nunder multiple noise variations.\n","authors":["Timo Budszuhn","Mark Joachim Krallmann","Daniel Horn"],"pdf_url":"https://arxiv.org/pdf/2503.21495v1.pdf","comment":"14 pages. 5 figures"},{"id":"http://arxiv.org/abs/2503.18684v2","updated":"2025-03-27T13:19:46Z","published":"2025-03-24T13:55:47Z","title":"Efficient Continual Adaptation of Pretrained Robotic Policy with Online\n  Meta-Learned Adapters","summary":"  Continual adaptation is essential for general autonomous agents. For example,\na household robot pretrained with a repertoire of skills must still adapt to\nunseen tasks specific to each household. Motivated by this, building upon\nparameter-efficient fine-tuning in language models, prior works have explored\nlightweight adapters to adapt pretrained policies, which can preserve learned\nfeatures from the pretraining phase and demonstrate good adaptation\nperformances. However, these approaches treat task learning separately,\nlimiting knowledge transfer between tasks. In this paper, we propose Online\nMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can\nfacilitate knowledge transfer from previously learned tasks to current learning\ntasks through a novel meta-learning objective. Extensive experiments in both\nsimulated and real-world environments demonstrate that OMLA can lead to better\nadaptation performances compared to the baseline methods. The project link:\nhttps://ricky-zhu.github.io/OMLA/.\n","authors":["Ruiqi Zhu","Endong Sun","Guanhe Huang","Oya Celiktutan"],"pdf_url":"https://arxiv.org/pdf/2503.18684v2.pdf","comment":"Project link: https://ricky-zhu.github.io/OMLA/"},{"id":"http://arxiv.org/abs/2503.21474v1","updated":"2025-03-27T13:05:40Z","published":"2025-03-27T13:05:40Z","title":"The Procedural Content Generation Benchmark: An Open-source Testbed for\n  Generative Challenges in Games","summary":"  This paper introduces the Procedural Content Generation Benchmark for\nevaluating generative algorithms on different game content creation tasks. The\nbenchmark comes with 12 game-related problems with multiple variants on each\nproblem. Problems vary from creating levels of different kinds to creating rule\nsets for simple arcade games. Each problem has its own content representation,\ncontrol parameters, and evaluation metrics for quality, diversity, and\ncontrollability. This benchmark is intended as a first step towards a\nstandardized way of comparing generative algorithms. We use the benchmark to\nscore three baseline algorithms: a random generator, an evolution strategy, and\na genetic algorithm. Results show that some problems are easier to solve than\nothers, as well as the impact the chosen objective has on quality, diversity,\nand controllability of the generated artifacts.\n","authors":["Ahmed Khalifa","Roberto Gallotta","Matthew Barthet","Antonios Liapis","Julian Togelius","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2503.21474v1.pdf","comment":"12 pages, 4 figures, 2 tables, published at FDG2025"},{"id":"http://arxiv.org/abs/2503.21465v1","updated":"2025-03-27T12:55:07Z","published":"2025-03-27T12:55:07Z","title":"Retinal Fundus Multi-Disease Image Classification using Hybrid\n  CNN-Transformer-Ensemble Architectures","summary":"  Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.\n","authors":["Deependra Singh","Saksham Agarwal","Subhankar Mishra"],"pdf_url":"https://arxiv.org/pdf/2503.21465v1.pdf","comment":"17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)"},{"id":"http://arxiv.org/abs/2503.21464v1","updated":"2025-03-27T12:54:00Z","published":"2025-03-27T12:54:00Z","title":"Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection","summary":"  In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.\n","authors":["Ryan Marinelli","Josef Pichlmeier","Tamas Bisztray"],"pdf_url":"https://arxiv.org/pdf/2503.21464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21463v1","updated":"2025-03-27T12:52:47Z","published":"2025-03-27T12:52:47Z","title":"Unveiling Latent Information in Transaction Hashes: Hypergraph Learning\n  for Ethereum Ponzi Scheme Detection","summary":"  With the widespread adoption of Ethereum, financial frauds such as Ponzi\nschemes have become increasingly rampant in the blockchain ecosystem, posing\nsignificant threats to the security of account assets. Existing Ethereum fraud\ndetection methods typically model account transactions as graphs, but this\napproach primarily focuses on binary transactional relationships between\naccounts, failing to adequately capture the complex multi-party interaction\npatterns inherent in Ethereum. To address this, we propose a hypergraph\nmodeling method for the Ponzi scheme detection method in Ethereum, called\nHyperDet. Specifically, we treat transaction hashes as hyperedges that connect\nall the relevant accounts involved in a transaction. Additionally, we design a\ntwo-step hypergraph sampling strategy to significantly reduce computational\ncomplexity. Furthermore, we introduce a dual-channel detection module,\nincluding the hypergraph detection channel and the hyper-homo graph detection\nchannel, to be compatible with existing detection methods. Experimental results\nshow that, compared to traditional homogeneous graph-based methods, the\nhyper-homo graph detection channel achieves significant performance\nimprovements, demonstrating the superiority of hypergraph in Ponzi scheme\ndetection. This research offers innovations for modeling complex relationships\nin blockchain data.\n","authors":["Junhao Wu","Yixin Yang","Chengxiang Jin","Silu Mu","Xiaolei Qian","Jiajun Zhou","Shanqing Yu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2503.21463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17411v2","updated":"2025-03-27T12:34:23Z","published":"2024-12-23T09:22:00Z","title":"Pretraining with random noise for uncertainty calibration","summary":"  Uncertainty calibration is crucial for various machine learning applications,\nyet it remains challenging. Many models exhibit hallucinations - confident yet\ninaccurate responses - due to miscalibrated confidence. Here, we show that the\ncommon practice of random initialization in deep learning, often considered a\nstandard technique, is an underlying cause of this miscalibration, leading to\nexcessively high confidence in untrained networks. Our method, inspired by\ndevelopmental neuroscience, addresses this issue by simply pretraining networks\nwith random noise and labels, reducing overconfidence and bringing initial\nconfidence levels closer to chance. This ensures optimal calibration, aligning\nconfidence with accuracy during subsequent data training, without the need for\nadditional pre- or post-processing. Pre-calibrated networks excel at\nidentifying \"unknown data,\" showing low confidence for out-of-distribution\ninputs, thereby resolving confidence miscalibration.\n","authors":["Jeonghwan Cheon","Se-Bum Paik"],"pdf_url":"https://arxiv.org/pdf/2412.17411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21435v1","updated":"2025-03-27T12:20:37Z","published":"2025-03-27T12:20:37Z","title":"Graph-to-Vision: Multi-graph Understanding and Reasoning using\n  Vision-Language Models","summary":"  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured\nlearning, have long faced dual challenges of exponentially escalating\ncomputational complexity and inadequate cross-scenario generalization\ncapability. With the rapid advancement of multimodal learning, Vision-Language\nModels (VLMs) have demonstrated exceptional cross-modal relational reasoning\ncapabilities and generalization capacities, thereby opening up novel pathways\nfor overcoming the inherent limitations of conventional graph learning\nparadigms. However, current research predominantly concentrates on\ninvestigating the single-graph reasoning capabilities of VLMs, which\nfundamentally fails to address the critical requirement for coordinated\nreasoning across multiple heterogeneous graph data in real-world application\nscenarios. To address these limitations, we propose the first multi-graph joint\nreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:\nknowledge graphs, flowcharts, mind maps, and route maps,with each graph group\naccompanied by three progressively challenging instruction-response pairs.\nLeveraging this benchmark, we conducted comprehensive capability assessments of\nstate-of-the-art VLMs and performed fine-tuning on open-source models. This\nstudy not only addresses the underexplored evaluation gap in multi-graph\nreasoning for VLMs but also empirically validates their generalization\nsuperiority in graph-structured learning.\n","authors":["Ruizhou Li","Haiyun Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.21435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21419v1","updated":"2025-03-27T12:09:04Z","published":"2025-03-27T12:09:04Z","title":"Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In \\& Out Learning","summary":"  Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.\n","authors":["Yupei Li","Manuel Milling","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2503.21419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07776v2","updated":"2025-03-27T11:57:50Z","published":"2024-12-10T18:59:58Z","title":"Video Motion Transfer with Diffusion Transformers","summary":"  We propose DiTFlow, a method for transferring the motion of a reference video\nto a newly synthesized one, designed specifically for Diffusion Transformers\n(DiT). We first process the reference video with a pre-trained DiT to analyze\ncross-frame attention maps and extract a patch-wise motion signal called the\nAttention Motion Flow (AMF). We guide the latent denoising process in an\noptimization-based, training-free, manner by optimizing latents with our AMF\nloss to generate videos reproducing the motion of the reference one. We also\napply our optimization strategy to transformer positional embeddings, granting\nus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow\nagainst recently published methods, outperforming all across multiple metrics\nand human evaluation.\n","authors":["Alexander Pondaven","Aliaksandr Siarohin","Sergey Tulyakov","Philip Torr","Fabio Pizzati"],"pdf_url":"https://arxiv.org/pdf/2412.07776v2.pdf","comment":"CVPR 2025 - Project page: https://ditflow.github.io/"},{"id":"http://arxiv.org/abs/2503.21412v1","updated":"2025-03-27T11:56:36Z","published":"2025-03-27T11:56:36Z","title":"Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning\n  and Collaborative Reasoning at the Network Edge","summary":"  Large artificial intelligence (AI) models exhibit remarkable capabilities in\nvarious application scenarios, but deploying them at the network edge poses\nsignificant challenges due to issues such as data privacy, computational\nresources, and latency. In this paper, we explore federated fine-tuning and\ncollaborative reasoning techniques to facilitate the implementation of large AI\nmodels in resource-constrained wireless networks. Firstly, promising\napplications of large AI models within specific domains are discussed.\nSubsequently, federated fine-tuning methods are proposed to adapt large AI\nmodels to specific tasks or environments at the network edge, effectively\naddressing the challenges associated with communication overhead and enhancing\ncommunication efficiency. These methodologies follow clustered, hierarchical,\nand asynchronous paradigms to effectively tackle privacy issues and eliminate\ndata silos. Furthermore, to enhance operational efficiency and reduce latency,\nefficient frameworks for model collaborative reasoning are developed, which\ninclude decentralized horizontal collaboration, cloud-edge-end vertical\ncollaboration, and multi-access collaboration. Next, simulation results\ndemonstrate the effectiveness of our proposed methods in reducing the\nfine-tuning loss of large AI models across various downstream tasks. Finally,\nseveral open challenges and research opportunities are outlined.\n","authors":["Wanli Ni","Haofeng Sun","Huiqing Ao","Hui Tian"],"pdf_url":"https://arxiv.org/pdf/2503.21412v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.21411v1","updated":"2025-03-27T11:56:27Z","published":"2025-03-27T11:56:27Z","title":"Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap","summary":"  Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.\n","authors":["Tong Nie","Jian Sun","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2503.21411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21406v1","updated":"2025-03-27T11:50:29Z","published":"2025-03-27T11:50:29Z","title":"Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for\n  Skill Learning","summary":"  Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.\n","authors":["Leon Keller","Daniel Tanneberg","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2503.21406v1.pdf","comment":"IEEE International Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.03708v3","updated":"2025-03-27T11:46:22Z","published":"2025-03-05T17:59:19Z","title":"Rethinking Video Tokenization: A Conditioned Diffusion-based Approach","summary":"  Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.\n","authors":["Nianzu Yang","Pandeng Li","Liming Zhao","Yang Li","Chen-Wei Xie","Yehui Tang","Xudong Lu","Zhihang Liu","Yun Zheng","Yu Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.03708v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21393v1","updated":"2025-03-27T11:35:40Z","published":"2025-03-27T11:35:40Z","title":"An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses","summary":"  Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.\n","authors":["Rohitash Chandra","Aryan Chaudhary","Yeshwanth Rayavarapu"],"pdf_url":"https://arxiv.org/pdf/2503.21393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17132v2","updated":"2025-03-27T11:35:37Z","published":"2025-03-21T13:31:16Z","title":"Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition","summary":"  This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.\n","authors":["Siyuan Yang","Shilin Lu","Shizheng Wang","Meng Hwa Er","Zengwei Zheng","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2503.17132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21392v1","updated":"2025-03-27T11:35:25Z","published":"2025-03-27T11:35:25Z","title":"HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion\n  Battery RUL Prediction","summary":"  Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery\n(LIB) health management systems is crucial for ensuring reliability and safety.\nCurrent methods typically assume that training and testing data share the same\ndistribution, overlooking the benefits of incorporating diverse data sources to\nenhance model performance. To address this limitation, we introduce a\ndata-independent RUL prediction framework along with its domain adaptation (DA)\napproach, which leverages heterogeneous data sources for improved target\npredictions. Our approach integrates comprehensive data preprocessing,\nincluding feature extraction, denoising, and normalization, with a\ndata-independent prediction model that combines Long Short-Term Memory (LSTM),\nMultihead Attention, and a Neural Ordinary Differential Equation (NODE) block,\ntermed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained\nusing a novel technique inspired by the Domain-Adversarial Neural Network\n(DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy\n(MMD) to learn domain-invariant features from labeled cycling data in the\nsource and target domains. Experimental results demonstrate that our approach\noutperforms state-of-the-art techniques, providing reliable RUL predictions for\nreal-world applications.\n","authors":["Khoa Tran","Bao Huynh","Tri Le","Lam Pham","Vy-Rin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.21392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01791v3","updated":"2025-03-27T11:30:07Z","published":"2023-10-03T04:40:38Z","title":"Online POMDP Planning with Anytime Deterministic Guarantees","summary":"  Decision-making under uncertainty is a critical aspect of many practical\nautonomous systems due to incomplete information. Partially Observable Markov\nDecision Processes (POMDPs) offer a mathematically principled framework for\nformulating decision-making problems under such conditions. However, finding an\noptimal solution for a POMDP is generally intractable. In recent years, there\nhas been a significant progress of scaling approximate solvers from small to\nmoderately sized problems, using online tree search solvers. Often, such\napproximate solvers are limited to probabilistic or asymptotic guarantees\ntowards the optimal solution. In this paper, we derive a deterministic\nrelationship for discrete POMDPs between an approximated and the optimal\nsolution. We show that at any time, we can derive bounds that relate between\nthe existing solution and the optimal one. We show that our derivations provide\nan avenue for a new set of algorithms and can be attached to existing\nalgorithms that have a certain structure to provide them with deterministic\nguarantees with marginal computational overhead. In return, not only do we\ncertify the solution quality, but we demonstrate that making a decision based\non the deterministic guarantee may result in superior performance compared to\nthe original algorithm without the deterministic certification.\n","authors":["Moran Barenboim","Vadim Indelman"],"pdf_url":"https://arxiv.org/pdf/2310.01791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07091v3","updated":"2025-03-27T11:23:24Z","published":"2025-03-10T09:14:47Z","title":"FaceID-6M: A Large-Scale, Open-Source FaceID Customization Dataset","summary":"  Due to the data-driven nature of current face identity (FaceID) customization\nmethods, all state-of-the-art models rely on large-scale datasets containing\nmillions of high-quality text-image pairs for training. However, none of these\ndatasets are publicly available, which restricts transparency and hinders\nfurther advancements in the field.\n  To address this issue, in this paper, we collect and release FaceID-6M, the\nfirst large-scale, open-source FaceID dataset containing 6 million high-quality\ntext-image pairs. Filtered from LAION-5B \\cite{schuhmann2022laion}, FaceID-6M\nundergoes a rigorous image and text filtering steps to ensure dataset quality,\nincluding resolution filtering to maintain high-quality images and faces, face\nfiltering to remove images that lack human faces, and keyword-based strategy to\nretain descriptions containing human-related terms (e.g., nationality,\nprofessions and names). Through these cleaning processes, FaceID-6M provides a\nhigh-quality dataset optimized for training powerful FaceID customization\nmodels, facilitating advancements in the field by offering an open resource for\nresearch and development.\n  We conduct extensive experiments to show the effectiveness of our FaceID-6M,\ndemonstrating that models trained on our FaceID-6M dataset achieve performance\nthat is comparable to, and slightly better than currently available industrial\nmodels. Additionally, to support and advance research in the FaceID\ncustomization community, we make our code, datasets, and models fully publicly\navailable. Our codes, models, and datasets are available at:\nhttps://github.com/ShuheSH/FaceID-6M.\n","authors":["Shuhe Wang","Xiaoya Li","Jiwei Li","Guoyin Wang","Xiaofei Sun","Bob Zhu","Han Qiu","Mo Yu","Shengjie Shen","Tianwei Zhang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2503.07091v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2501.15407"},{"id":"http://arxiv.org/abs/2503.21356v1","updated":"2025-03-27T10:48:40Z","published":"2025-03-27T10:48:40Z","title":"Investigating the Duality of Interpretability and Explainability in\n  Machine Learning","summary":"  The rapid evolution of machine learning (ML) has led to the widespread\nadoption of complex \"black box\" models, such as deep neural networks and\nensemble methods. These models exhibit exceptional predictive performance,\nmaking them invaluable for critical decision-making across diverse domains\nwithin society. However, their inherently opaque nature raises concerns about\ntransparency and interpretability, making them untrustworthy decision support\nsystems. To alleviate such a barrier to high-stakes adoption, research\ncommunity focus has been on developing methods to explain black box models as a\nmeans to address the challenges they pose. Efforts are focused on explaining\nthese models instead of developing ones that are inherently interpretable.\nDesigning inherently interpretable models from the outset, however, can pave\nthe path towards responsible and beneficial applications in the field of ML. In\nthis position paper, we clarify the chasm between explaining black boxes and\nadopting inherently interpretable models. We emphasize the imperative need for\nmodel interpretability and, following the purpose of attaining better (i.e.,\nmore effective or efficient w.r.t. predictive performance) and trustworthy\npredictors, provide an experimental evaluation of latest hybrid learning\nmethods that integrates symbolic knowledge into neural network predictors. We\ndemonstrate how interpretable hybrid models could potentially supplant black\nbox ones in different domains.\n","authors":["Moncef Garouani","Josiane Mothe","Ayah Barhrhouj","Julien Aligon"],"pdf_url":"https://arxiv.org/pdf/2503.21356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21352v1","updated":"2025-03-27T10:42:19Z","published":"2025-03-27T10:42:19Z","title":"Using large language models to produce literature reviews: Usages and\n  systematic biases of microphysics parametrizations in 2699 publications","summary":"  Large language models afford opportunities for using computers for intensive\ntasks, realizing research opportunities that have not been considered before.\nOne such opportunity could be a systematic interrogation of the scientific\nliterature. Here, we show how a large language model can be used to construct a\nliterature review of 2699 publications associated with microphysics\nparametrizations in the Weather and Research Forecasting (WRF) model, with the\ngoal of learning how they were used and their systematic biases, when\nsimulating precipitation. The database was constructed of publications\nidentified from Web of Science and Scopus searches. The large language model\nGPT-4 Turbo was used to extract information about model configurations and\nperformance from the text of 2699 publications. Our results reveal the\nlandscape of how nine of the most popular microphysics parameterizations have\nbeen used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus\nEnsemble, Morrison, Thompson, and WRF Double-Moment. More studies used\none-moment parameterizations before 2020 and two-moment parameterizations after\n2020. Seven out of nine parameterizations tended to overestimate precipitation.\nHowever, systematic biases of parameterizations differed in various regions.\nExcept simulations using the Lin, Ferrier, and Goddard parameterizations that\ntended to underestimate precipitation over almost all locations, the remaining\nsix parameterizations tended to overestimate, particularly over China,\nsoutheast Asia, western United States, and central Africa. This method could be\nused by other researchers to help understand how the increasingly massive body\nof scientific literature can be harnessed through the power of artificial\nintelligence to solve their research problems.\n","authors":["Tianhang Zhang","Shengnan Fu","David M. Schultz","Zhonghua Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.21352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01877v2","updated":"2025-03-27T10:38:45Z","published":"2025-02-26T15:20:01Z","title":"Starjob: Dataset for LLM-Driven Job Shop Scheduling","summary":"  Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, but their potential for solving combinatorial optimization\nproblems remains largely unexplored. In this paper, we investigate the\napplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic\nchallenge in combinatorial optimization that requires efficient job allocation\nto machines to minimize makespan. To this end, we introduce Starjob, the first\nsupervised dataset for JSSP, comprising 130k instances specifically designed\nfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit\nquantized model with the LoRA method to develop an end-to-end scheduling\napproach. Our evaluation on standard benchmarks demonstrates that the proposed\nLLM-based method not only surpasses traditional Priority Dispatching Rules\n(PDRs) but also achieves notable improvements over state-of-the-art neural\napproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on\nTaillard benchmarks. These results highlight the untapped potential of LLMs in\ntackling combinatorial optimization problems, paving the way for future\nadvancements in this area.\n","authors":["Henrik Abgaryan","Tristan Cazenave","Ararat Harutyunyan"],"pdf_url":"https://arxiv.org/pdf/2503.01877v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2408.06993"},{"id":"http://arxiv.org/abs/2405.00205v2","updated":"2025-03-27T10:38:15Z","published":"2024-04-30T21:16:38Z","title":"A Logic for Reasoning About Aggregate-Combine Graph Neural Networks","summary":"  We propose a modal logic in which counting modalities appear in linear\ninequalities. We show that each formula can be transformed into an equivalent\ngraph neural network (GNN). We also show that a broad class of GNNs can be\ntransformed efficiently into a formula, thus significantly improving upon the\nliterature about the logical expressiveness of GNNs. We also show that the\nsatisfiability problem is PSPACE-complete. These results bring together the\npromise of using standard logical methods for reasoning about GNNs and their\nproperties, particularly in applications such as GNN querying, equivalence\nchecking, etc. We prove that such natural problems can be solved in polynomial\nspace.\n","authors":["Pierre Nunn","Marco Sälzer","François Schwarzentruber","Nicolas Troquard"],"pdf_url":"https://arxiv.org/pdf/2405.00205v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.05150"},{"id":"http://arxiv.org/abs/2503.21347v1","updated":"2025-03-27T10:27:17Z","published":"2025-03-27T10:27:17Z","title":"Residual Learning Inspired Crossover Operator and Strategy Enhancements\n  for Evolutionary Multitasking","summary":"  In evolutionary multitasking, strategies such as crossover operators and\nskill factor assignment are critical for effective knowledge transfer. Existing\nimprovements to crossover operators primarily focus on low-dimensional variable\ncombinations, such as arithmetic crossover or partially mapped crossover, which\nare insufficient for modeling complex high-dimensional interactions.Moreover,\nstatic or semi-dynamic crossover strategies fail to adapt to the dynamic\ndependencies among tasks. In addition, current Multifactorial Evolutionary\nAlgorithm frameworks often rely on fixed skill factor assignment strategies,\nlacking flexibility. To address these limitations, this paper proposes the\nMultifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based\non residual learning. The method employs a Very Deep Super-Resolution (VDSR)\nmodel to generate high-dimensional residual representations of individuals,\nenhancing the modeling of complex relationships within dimensions. A\nResNet-based mechanism dynamically assigns skill factors to improve task\nadaptability, while a random mapping mechanism efficiently performs crossover\noperations and mitigates the risk of negative transfer. Theoretical analysis\nand experimental results show that MFEA-RL outperforms state-of-the-art\nmultitasking algorithms. It excels in both convergence and adaptability on\nstandard evolutionary multitasking benchmarks, including CEC2017-MTSO and\nWCCI2020-MTSO. Additionally, its effectiveness is validated through a\nreal-world application scenario.\n","authors":["Ruilin Wang","Xiang Feng","Huiqun Yu","Edmund M-K Lai"],"pdf_url":"https://arxiv.org/pdf/2503.21347v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.17819v4","updated":"2025-03-27T10:19:28Z","published":"2024-06-25T08:29:32Z","title":"Automatically Adaptive Conformal Risk Control","summary":"  Science and technology have a growing need for effective mechanisms that\nensure reliable, controlled performance from black-box machine learning\nalgorithms. These performance guarantees should ideally hold conditionally on\nthe input-that is the performance guarantees should hold, at least\napproximately, no matter what the input. However, beyond stylized discrete\ngroupings such as ethnicity and gender, the right notion of conditioning can be\ndifficult to define. For example, in problems such as image segmentation, we\nwant the uncertainty to reflect the intrinsic difficulty of the test sample,\nbut this may be difficult to capture via a conditioning event. Building on the\nrecent work of Gibbs et al. [2023], we propose a methodology for achieving\napproximate conditional control of statistical risks-the expected value of loss\nfunctions-by adapting to the difficulty of test samples. Our framework goes\nbeyond traditional conditional risk control based on user-provided conditioning\nevents to the algorithmic, data-driven determination of appropriate function\nclasses for conditioning. We apply this framework to various regression and\nsegmentation tasks, enabling finer-grained control over model performance and\ndemonstrating that by continuously monitoring and adjusting these parameters,\nwe can achieve superior precision compared to conventional risk-control\nmethods.\n","authors":["Vincent Blot","Anastasios N Angelopoulos","Michael I Jordan","Nicolas J-B Brunel"],"pdf_url":"https://arxiv.org/pdf/2406.17819v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21337v1","updated":"2025-03-27T10:14:00Z","published":"2025-03-27T10:14:00Z","title":"A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking\n  Neural Network","summary":"  This paper introduces a 71.2-$\\mu$W speech recognition accelerator designed\nfor edge devices' real-time applications, emphasizing an ultra low power\ndesign. Achieved through algorithm and hardware co-optimizations, we propose a\ncompact recurrent spiking neural network with two recurrent layers, one fully\nconnected layer, and a low time step (1 or 2). The 2.79-MB model undergoes\npruning and 4-bit fixed-point quantization, shrinking it by 96.42\\% to 0.1 MB.\nOn the hardware front, we take advantage of \\textit{mixed-level pruning},\n\\textit{zero-skipping} and \\textit{merged spike} techniques, reducing\ncomplexity by 90.49\\% to 13.86 MMAC/S. The \\textit{parallel time-step\nexecution} addresses inter-time-step data dependencies and enables weight\nbuffer power savings through weight sharing. Capitalizing on the sparse spike\nactivity, an input broadcasting scheme eliminates zero computations, further\nsaving power. Implemented on the TSMC 28-nm process, the design operates in\nreal time at 100 kHz, consuming 71.2 $\\mu$W, surpassing state-of-the-art\ndesigns. At 500 MHz, it has 28.41 TOPS/W and 1903.11 GOPS/mm$^2$ in energy and\narea efficiency, respectively.\n","authors":["Chih-Chyau Yang","Tian-Sheuan Chang"],"pdf_url":"https://arxiv.org/pdf/2503.21337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21335v1","updated":"2025-03-27T10:13:41Z","published":"2025-03-27T10:13:41Z","title":"A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices","summary":"  Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency.\n","authors":["Ci-Hao Wu","Tian-Sheuan Chang"],"pdf_url":"https://arxiv.org/pdf/2503.21335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21332v1","updated":"2025-03-27T10:11:41Z","published":"2025-03-27T10:11:41Z","title":"ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback","summary":"  Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.\n","authors":["Taewon Yun","Jihwan Oh","Hyangsuk Min","Yuho Lee","Jihwan Bang","Jason Cai","Hwanjun Song"],"pdf_url":"https://arxiv.org/pdf/2503.21332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19654v2","updated":"2025-03-27T10:11:22Z","published":"2025-03-25T13:43:47Z","title":"RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of\n  Vision Language Models","summary":"  We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.\n","authors":["Mehdi Moshtaghi","Siavash H. Khajavi","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2503.19654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21322v1","updated":"2025-03-27T10:01:16Z","published":"2025-03-27T10:01:16Z","title":"HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured\n  Knowledge Representation","summary":"  While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAG\nstructures knowledge as graphs to leverage the relations among entities.\nHowever, previous GraphRAG methods are limited by binary relations: one edge in\nthe graph only connects two entities, which cannot well model the n-ary\nrelations among more than two entities that widely exist in reality. To address\nthis limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG method\nthat represents n-ary relational facts via hyperedges, modeling the complicated\nn-ary relations in the real world. To retrieve and generate over hypergraphs,\nwe introduce a complete pipeline with a hypergraph construction method, a\nhypergraph retrieval strategy, and a hypergraph-guided generation mechanism.\nExperiments across medicine, agriculture, computer science, and law demonstrate\nthat HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy and\ngeneration quality.\n","authors":["Haoran Luo","Haihong E","Guanting Chen","Yandan Zheng","Xiaobao Wu","Yikai Guo","Qika Lin","Yu Feng","Zemin Kuang","Meina Song","Yifan Zhu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2503.21322v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.21309v1","updated":"2025-03-27T09:34:21Z","published":"2025-03-27T09:34:21Z","title":"FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for\n  Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git.\n","authors":["Zixu Li","Zhiheng Fu","Yupeng Hu","Zhiwei Chen","Haokun Wen","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.21309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21307v1","updated":"2025-03-27T09:31:35Z","published":"2025-03-27T09:31:35Z","title":"InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression","summary":"  Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.\n","authors":["Dongchen Lu","Yuyao Sun","Zilu Zhang","Leping Huang","Jianliang Zeng","Mao Shu","Huo Cao"],"pdf_url":"https://arxiv.org/pdf/2503.21307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21305v1","updated":"2025-03-27T09:31:10Z","published":"2025-03-27T09:31:10Z","title":"DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep\n  Models with Limited Data","summary":"  Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.\n","authors":["Dorde Popovic","Amin Sadeghi","Ting Yu","Sanjay Chawla","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2503.21305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15469v3","updated":"2025-03-27T09:24:27Z","published":"2025-03-19T17:45:13Z","title":"Dynamic Bi-Elman Attention Networks: A Dual-Directional Context-Aware\n  Test-Time Learning for Text Classification","summary":"  Text classification, a fundamental task in natural language processing, aims\nto categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies.\nHowever, the advent of deep learning, particularly recurrent neural networks\nand Transformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite these\nimprovements, existing models still exhibit limitations in balancing\ninterpretability, computational efficiency, and long-range contextual\nunderstanding. To address these challenges, this paper proposes the Dynamic\nBidirectional Elman with Attention Network (DBEAN). DBEAN integrates\nbidirectional temporal modeling with self-attention mechanisms. It dynamically\nassigns weights to critical segments of input, improving contextual\nrepresentation while maintaining computational efficiency.\n","authors":["ZhengLin Lai","MengYao Liao","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2503.15469v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2503.21284v1","updated":"2025-03-27T09:08:39Z","published":"2025-03-27T09:08:39Z","title":"Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate\n  Learned Image Compression","summary":"  Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates.The source code is available at\n\\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.\n","authors":["Hanyue Tu","Siqi Wu","Li Li","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2503.21284v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia 2025"},{"id":"http://arxiv.org/abs/2411.03055v3","updated":"2025-03-27T08:57:30Z","published":"2024-11-05T12:42:42Z","title":"ATM: Improving Model Merging by Alternating Tuning and Merging","summary":"  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, if the optimization is performed via gradient descent,\ntask vectors are after one step mathematically equivalent to the gradients\nobtained via gradient descent in a multi-task setting, and still approximate\nthese gradients in subsequent epochs. Furthermore, we show that the\neffectiveness of task vectors is largely driven by the first epoch's gradient.\nGiven this parallel between task vectors and gradients, we propose viewing\nmodel merging as a single step in an iterative process that alternates between\ntuning and merging (ATM). We then propose two ways to utilize ATM. The first is\nto replace multi-task learning with ATM in scenarios where data sharing is\nprohibited, such as federated learning. The second is to improve the outcome of\nany model merging algorithm by applying a few post-hoc iterations of ATM on a\nsmall validation dataset, which is commonly available for hyperparameter\ntuning. Finally, we provide both empirical and theoretical support for the\neffectiveness of ATM, demonstrating that it minimizes an upper bound on the\nloss obtained by jointly finetuning all tasks.\n","authors":["Luca Zhou","Daniele Solombrino","Donato Crisostomi","Maria Sofia Bucarelli","Fabrizio Silvestri","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2411.03055v3.pdf","comment":"Main paper: 9 Pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.21272v1","updated":"2025-03-27T08:52:41Z","published":"2025-03-27T08:52:41Z","title":"Reinforced Model Merging","summary":"  The success of large language models has garnered widespread attention for\nmodel merging techniques, especially training-free methods which combine model\ncapabilities within the parameter space. However, two challenges remain: (1)\nuniform treatment of all parameters leads to performance degradation; (2)\nsearch-based algorithms are often inefficient. In this paper, we present an\ninnovative framework termed Reinforced Model Merging (RMM), which encompasses\nan environment and agent tailored for merging tasks. These components interact\nto execute layer-wise merging actions, aiming to search the optimal merging\narchitecture. Notably, RMM operates without any gradient computations on the\noriginal models, rendering it feasible for edge devices. Furthermore, by\nutilizing data subsets during the evaluation process, we addressed the\nbottleneck in the reward feedback phase, thereby accelerating RMM by up to 100\ntimes. Extensive experiments demonstrate that RMM achieves state-of-the-art\nperformance across various vision and NLP datasets and effectively overcomes\nthe limitations of the existing baseline methods. Our code is available at\nhttps://github.com/WuDiHJQ/Reinforced-Model-Merging.\n","authors":["Jiaqi Han","Jingwen Ye","Shunyu Liu","Haofei Zhang","Jie Song","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2503.21272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21258v1","updated":"2025-03-27T08:31:46Z","published":"2025-03-27T08:31:46Z","title":"Learn by Reasoning: Analogical Weight Generation for Few-Shot\n  Class-Incremental Learning","summary":"  Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.\n","authors":["Jizhou Han","Chenhao Ding","Yuhang He","Songlin Dong","Qiang Wang","Xinyuan Gao","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2503.21258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21257v1","updated":"2025-03-27T08:28:22Z","published":"2025-03-27T08:28:22Z","title":"OminiAdapt: Learning Cross-Task Invariance for Robust and\n  Environment-Aware Robotic Manipulation","summary":"  With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub.\n","authors":["Yongxu Wang","Weiyun Yi","Xinhao Kong","Wanting Li"],"pdf_url":"https://arxiv.org/pdf/2503.21257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21254v1","updated":"2025-03-27T08:21:54Z","published":"2025-03-27T08:21:54Z","title":"Vision-to-Music Generation: A Survey","summary":"  Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.\n","authors":["Zhaokai Wang","Chenxi Bao","Le Zhuo","Jingrui Han","Yang Yue","Yihong Tang","Victor Shea-Jay Huang","Yue Liao"],"pdf_url":"https://arxiv.org/pdf/2503.21254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21251v1","updated":"2025-03-27T08:17:18Z","published":"2025-03-27T08:17:18Z","title":"Dual-Splitting Conformal Prediction for Multi-Step Time Series\n  Forecasting","summary":"  Time series forecasting is crucial for applications like resource scheduling\nand risk management, where multi-step predictions provide a comprehensive view\nof future trends. Uncertainty Quantification (UQ) is a mainstream approach for\naddressing forecasting uncertainties, with Conformal Prediction (CP) gaining\nattention due to its model-agnostic nature and statistical guarantees. However,\nmost variants of CP are designed for single-step predictions and face\nchallenges in multi-step scenarios, such as reliance on real-time data and\nlimited scalability. This highlights the need for CP methods specifically\ntailored to multi-step forecasting. We propose the Dual-Splitting Conformal\nPrediction (DSCP) method, a novel CP approach designed to capture inherent\ndependencies within time-series data for multi-step forecasting. Experimental\nresults on real-world datasets from four different domains demonstrate that the\nproposed DSCP significantly outperforms existing CP variants in terms of the\nWinkler Score, achieving a performance improvement of up to 23.59% compared to\nstate-of-the-art methods. Furthermore, we deployed the DSCP approach for\nrenewable energy generation and IT load forecasting in power management of a\nreal-world trajectory-based application, achieving an 11.25% reduction in\ncarbon emissions through predictive optimization of data center operations and\ncontrols.\n","authors":["Qingdi Yu","Zhiwei Cao","Ruihang Wang","Zhen Yang","Lijun Deng","Min Hu","Yong Luo","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21251v1.pdf","comment":"28 pages, 13 figures, 3 tables. Submitted to Applied Soft Computing.\n  With Editor This is the first public release of the work"},{"id":"http://arxiv.org/abs/2503.21248v1","updated":"2025-03-27T08:09:15Z","published":"2025-03-27T08:09:15Z","title":"ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition","summary":"  Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.\n","authors":["Yujie Liu","Zonglin Yang","Tong Xie","Jinjie Ni","Ben Gao","Yuqiang Li","Shixiang Tang","Wanli Ouyang","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21244v1","updated":"2025-03-27T08:07:39Z","published":"2025-03-27T08:07:39Z","title":"Improving $(α, f)$-Byzantine Resilience in Federated Learning via\n  layerwise aggregation and cosine distance","summary":"  The rapid development of artificial intelligence systems has amplified\nsocietal concerns regarding their usage, necessitating regulatory frameworks\nthat encompass data privacy. Federated Learning (FL) is posed as potential\nsolution to data privacy challenges in distributed machine learning by enabling\ncollaborative model training {without data sharing}. However, FL systems remain\nvulnerable to Byzantine attacks, where malicious nodes contribute corrupted\nmodel updates. While Byzantine Resilient operators have emerged as a widely\nadopted robust aggregation algorithm to mitigate these attacks, its efficacy\ndiminishes significantly in high-dimensional parameter spaces, sometimes\nleading to poor performing models. This paper introduces Layerwise Cosine\nAggregation, a novel aggregation scheme designed to enhance robustness of these\nrules in such high-dimensional settings while preserving computational\nefficiency. A theoretical analysis is presented, demonstrating the superior\nrobustness of the proposed Layerwise Cosine Aggregation compared to original\nrobust aggregation operators. Empirical evaluation across diverse image\nclassification datasets, under varying data distributions and Byzantine attack\nscenarios, consistently demonstrates the improved performance of Layerwise\nCosine Aggregation, achieving up to a 16% increase in model accuracy.\n","authors":["Mario García-Márquez","Nuria Rodríguez-Barroso","M. Victoria Luzón","Francisco Herrera"],"pdf_url":"https://arxiv.org/pdf/2503.21244v1.pdf","comment":"Submitted to Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2410.14138v2","updated":"2025-03-27T08:07:19Z","published":"2024-10-18T03:22:06Z","title":"ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom","summary":"  Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones.\n","authors":["Jingqi Zhou","Sheng Wang","Jingwei Dong","Lei Li","Jiahui Gao","Jiyue Jiang","Lingpeng Kong","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.14138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21241v1","updated":"2025-03-27T08:04:42Z","published":"2025-03-27T08:04:42Z","title":"Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in\n  Healthcare Data","summary":"  Accurate patient mortality prediction enables effective risk stratification,\nleading to personalized treatment plans and improved patient outcomes. However,\npredicting mortality in healthcare remains a significant challenge, with\nexisting studies often focusing on specific diseases or limited predictor sets.\nThis study evaluates machine learning models for all-cause in-hospital\nmortality prediction using the MIMIC-III database, employing a comprehensive\nfeature engineering approach. Guided by clinical expertise and literature, we\nextracted key features such as vital signs (e.g., heart rate, blood pressure),\nlaboratory results (e.g., creatinine, glucose), and demographic information.\nThe Random Forest model achieved the highest performance with an AUC of 0.94,\nsignificantly outperforming other machine learning and deep learning\napproaches. This demonstrates Random Forest's robustness in handling\nhigh-dimensional, noisy clinical data and its potential for developing\neffective clinical decision support tools. Our findings highlight the\nimportance of careful feature engineering for accurate mortality prediction. We\nconclude by discussing implications for clinical adoption and propose future\ndirections, including enhancing model robustness and tailoring prediction\nmodels for specific diseases.\n","authors":["HyeYoung Lee","Pavel Tsoi"],"pdf_url":"https://arxiv.org/pdf/2503.21241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21237v1","updated":"2025-03-27T07:54:39Z","published":"2025-03-27T07:54:39Z","title":"Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval","summary":"  Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.\n","authors":["Karanbir Singh","William Ngu"],"pdf_url":"https://arxiv.org/pdf/2503.21237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12692v2","updated":"2025-03-27T07:52:45Z","published":"2024-08-22T19:12:52Z","title":"Rethinking Training for De-biasing Text-to-Image Generation: Unlocking\n  the Potential of Stable Diffusion","summary":"  Recent advancements in text-to-image models, such as Stable Diffusion, show\nsignificant demographic biases. Existing de-biasing techniques rely heavily on\nadditional training, which imposes high computational costs and risks of\ncompromising core image generation functionality. This hinders them from being\nwidely adopted to real-world applications. In this paper, we explore Stable\nDiffusion's overlooked potential to reduce bias without requiring additional\ntraining. Through our analysis, we uncover that initial noises associated with\nminority attributes form \"minority regions\" rather than scattered. We view\nthese \"minority regions\" as opportunities in SD to reduce bias. To unlock the\npotential, we propose a novel de-biasing method called 'weak guidance,'\ncarefully designed to guide a random noise to the minority regions without\ncompromising semantic integrity. Through analysis and experiments on various\nversions of SD, we demonstrate that our proposed approach effectively reduces\nbias without additional training, achieving both efficiency and preservation of\ncore image generation functionality.\n","authors":["Eunji Kim","Siwon Kim","Minjun Park","Rahim Entezari","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.12692v2.pdf","comment":"19 pages; First two authors contributed equally; Accepted at CVPR\n  2025"},{"id":"http://arxiv.org/abs/2502.12767v4","updated":"2025-03-27T07:49:51Z","published":"2025-02-18T11:31:52Z","title":"R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs","summary":"  Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.\n","authors":["Sumin Jo","Junseong Choi","Jiho Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2502.12767v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21232v1","updated":"2025-03-27T07:46:45Z","published":"2025-03-27T07:46:45Z","title":"Knowledge Graphs as World Models for Semantic Material-Aware Obstacle\n  Handling in Autonomous Vehicles","summary":"  The inability of autonomous vehicles (AVs) to infer the material properties\nof obstacles limits their decision-making capacity. While AVs rely on sensor\nsystems such as cameras, LiDAR, and radar to detect obstacles, this study\nsuggests combining sensors with a knowledge graph (KG)-based world model to\nimprove AVs' comprehension of physical material qualities. Beyond sensor data,\nAVs can infer qualities such as malleability, density, and elasticity using a\nsemantic KG that depicts the relationships between obstacles and their\nattributes. Using the CARLA autonomous driving simulator, we evaluated AV\nperformance with and without KG integration. The findings demonstrate that the\nKG-based method improves obstacle management, which allows AVs to use material\nqualities to make better decisions about when to change lanes or apply\nemergency braking. For example, the KG-integrated AV changed lanes for hard\nimpediments like traffic cones and successfully avoided collisions with\nflexible items such as plastic bags by passing over them. Compared to the\ncontrol system, the KG framework demonstrated improved responsiveness to\nobstacles by resolving conflicting sensor data, causing emergency stops for\n13.3% more cases. In addition, our method exhibits a 6.6% higher success rate\nin lane-changing maneuvers in experimental scenarios, particularly for larger,\nhigh-impact obstacles. While we focus particularly on autonomous driving, our\nwork demonstrates the potential of KG-based world models to improve\ndecision-making in embodied AI systems and scale to other domains, including\nrobotics, healthcare, and environmental simulation.\n","authors":["Ayush Bheemaiah","Seungyong Yang"],"pdf_url":"https://arxiv.org/pdf/2503.21232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16391v2","updated":"2025-03-27T07:41:02Z","published":"2025-01-26T08:22:22Z","title":"Inductive-Associative Meta-learning Pipeline with Human Cognitive\n  Patterns for Unseen Drug-Target Interaction Prediction","summary":"  Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.\n","authors":["Xiaoqing Lian","Jie Zhu","Tianxu Lv","Shiyun Nie","Hang Fan","Guosheng Wu","Yunjun Ge","Lihua Li","Xiangxiang Zeng","Xiang Pan"],"pdf_url":"https://arxiv.org/pdf/2501.16391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08972v2","updated":"2025-03-27T07:21:27Z","published":"2025-02-13T05:20:21Z","title":"Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning","summary":"  Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.\n","authors":["Hyundong Cho","Karishma Sharma","Nicolaas Jedema","Leonardo F. R. Ribeiro","Alessandro Moschitti","Ravi Krishnan","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2502.08972v2.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2503.21219v1","updated":"2025-03-27T07:16:24Z","published":"2025-03-27T07:16:24Z","title":"GenFusion: Closing the Loop between Reconstruction and Generation via\n  Videos","summary":"  Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach.\n","authors":["Sibo Wu","Congrong Xu","Binbin Huang","Andreas Geiger","Anpei Chen"],"pdf_url":"https://arxiv.org/pdf/2503.21219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10095v2","updated":"2025-03-27T07:14:15Z","published":"2025-03-13T06:42:37Z","title":"Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text","summary":"  Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements.\n","authors":["Avinash Patil","Amardeep Kour Gedhu"],"pdf_url":"https://arxiv.org/pdf/2503.10095v2.pdf","comment":"8 pages, 4 Figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.14963v5","updated":"2025-03-27T07:08:33Z","published":"2024-04-23T12:16:05Z","title":"Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems","summary":"  Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting.\n","authors":["Qihuang Zhong","Kang Wang","Ziyang Xu","Juhua Liu","Liang Ding","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2404.14963v5.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: { 10.1007/s11704-025-41102-z }"},{"id":"http://arxiv.org/abs/2503.11108v2","updated":"2025-03-27T07:02:19Z","published":"2025-03-14T06:01:42Z","title":"Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding","summary":"  The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.\n","authors":["Yifang Chen","Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2503.11108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v3","updated":"2025-03-27T06:45:16Z","published":"2025-02-13T08:10:45Z","title":"Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging -- An Open Recipe","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.09042v2","updated":"2025-03-27T06:45:15Z","published":"2025-02-13T07:55:54Z","title":"Typhoon T1: An Open Thai Reasoning Model","summary":"  This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.\n","authors":["Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2502.09042v2.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.02471v2","updated":"2025-03-27T06:39:45Z","published":"2025-01-05T07:46:51Z","title":"Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\n  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine","summary":"  Large language models (LLMs) primarily trained on English texts, often face\nbiases and inaccuracies in Chinese contexts. Their limitations are pronounced\nin fields like Traditional Chinese Medicine (TCM), where cultural and clinical\nsubtleties are vital, further hindered by a lack of domain-specific data, such\nas rheumatoid arthritis (RA). To address these issues, this paper introduces\nHengqin-RA-v1, the first large language model specifically tailored for TCM\nwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a\ncomprehensive RA-specific dataset curated from ancient Chinese medical\nliterature, classical texts, and modern clinical studies. This dataset empowers\nHengqin-RA-v1 to deliver accurate and culturally informed responses,\neffectively bridging the gaps left by general-purpose models. Extensive\nexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,\neven surpassing the diagnostic accuracy of TCM practitioners in certain cases.\n","authors":["Yishen Liu","Shengda Luo","Zishao Zhong","Tongtong Wu","Jianguo Zhang","Peiyao Ou","Yong Liang","Liang Liu","Hudan Pan"],"pdf_url":"https://arxiv.org/pdf/2501.02471v2.pdf","comment":"8 pages, 5 figures, AAAI-2025 Workshop"},{"id":"http://arxiv.org/abs/2502.06874v2","updated":"2025-03-27T06:37:40Z","published":"2025-02-08T09:02:43Z","title":"Group Reasoning Emission Estimation Networks","summary":"  Accurate greenhouse gas (GHG) emission reporting is critical for governments,\nbusinesses, and investors. However, adoption remains limited particularly among\nsmall and medium enterprises due to high implementation costs, fragmented\nemission factor databases, and a lack of robust sector classification methods.\nTo address these challenges, we introduce Group Reasoning Emission Estimation\nNetworks (GREEN), an AI-driven carbon accounting framework that standardizes\nenterprise-level emission estimation, constructs a large-scale benchmark\ndataset, and leverages a novel reasoning approach with large language models\n(LLMs). Specifically, we compile textual descriptions for 20,850 companies with\nvalidated North American Industry Classification System (NAICS) labels and\nalign these with an economic model of carbon intensity factors. By reframing\nsector classification as an information retrieval task, we fine-tune\nSentence-BERT models using a contrastive learning loss. To overcome the\nlimitations of single-stage models in handling thousands of hierarchical\ncategories, we propose a Group Reasoning method that ensembles LLM classifiers\nbased on the natural NAICS ontology, decomposing the task into multiple\nsub-classification steps. We theoretically prove that this approach reduces\nclassification uncertainty and computational complexity. Experiments on 1,114\nNAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%\nTop-10 accuracy), and case studies on 20 companies report a mean absolute\npercentage error (MAPE) of 45.88%. The project is available at:\nhttps://huggingface.co/datasets/Yvnminc/ExioNAICS.\n","authors":["Yanming Guo","Xiao Qian","Kevin Credit","Jin Ma"],"pdf_url":"https://arxiv.org/pdf/2502.06874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20685v2","updated":"2025-03-27T06:16:16Z","published":"2025-03-26T16:20:02Z","title":"Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast\n  Ultrasound","summary":"  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.\n","authors":["Yuhao Huang","Ao Chang","Haoran Dou","Xing Tao","Xinrui Zhou","Yan Cao","Ruobing Huang","Alejandro F Frangi","Lingyun Bao","Xin Yang","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2503.20685v2.pdf","comment":"Accepted by Medical Image Analysis. 24 pages, 13 figures, 20 tabels"},{"id":"http://arxiv.org/abs/2503.21178v1","updated":"2025-03-27T06:01:50Z","published":"2025-03-27T06:01:50Z","title":"Integrating Large Language Models For Monte Carlo Simulation of Chemical\n  Reaction Networks","summary":"  Chemical reaction network is an important method for modeling and exploring\ncomplex biological processes, bio-chemical interactions and the behavior of\ndifferent dynamics in system biology. But, formulating such reaction kinetics\ntakes considerable time. In this paper, we leverage the efficiency of modern\nlarge language models to automate the stochastic monte carlo simulation of\nchemical reaction networks and enable the simulation through the reaction\ndescription provided in the form of natural languages. We also integrate this\nprocess into widely used simulation tool Copasi to further give the edge and\nease to the modelers and researchers. In this work, we show the efficacy and\nlimitations of the modern large language models to parse and create reaction\nkinetics for modelling complex chemical reaction processes.\n","authors":["Sadikshya Gyawali","Ashwini Mandal","Manish Dahal","Manish Awale","Sanjay Rijal","Shital Adhikari","Vaghawan Ojha"],"pdf_url":"https://arxiv.org/pdf/2503.21178v1.pdf","comment":"Accepted on MadeAI 2025 Conference"},{"id":"http://arxiv.org/abs/2503.19470v2","updated":"2025-03-27T05:56:31Z","published":"2025-03-25T09:00:58Z","title":"ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.\n","authors":["Mingyang Chen","Tianpeng Li","Haoze Sun","Yijie Zhou","Chenzheng Zhu","Haofen Wang","Jeff Z. Pan","Wen Zhang","Huajun Chen","Fan Yang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2503.19470v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.19647v3","updated":"2025-03-27T05:44:45Z","published":"2024-03-28T17:56:07Z","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal\n  Graphs in Language Models","summary":"  We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.\n","authors":["Samuel Marks","Can Rager","Eric J. Michaud","Yonatan Belinkov","David Bau","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.19647v3.pdf","comment":"Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz"},{"id":"http://arxiv.org/abs/2412.02479v2","updated":"2025-03-27T05:40:57Z","published":"2024-12-03T14:42:31Z","title":"OODFace: Benchmarking Robustness of Face Recognition under Common\n  Corruptions and Appearance Variations","summary":"  With the rise of deep learning, facial recognition technology has seen\nextensive research and rapid development. Although facial recognition is\nconsidered a mature technology, we find that existing open-source models and\ncommercial algorithms lack robustness in certain complex Out-of-Distribution\n(OOD) scenarios, raising concerns about the reliability of these systems. In\nthis paper, we introduce OODFace, which explores the OOD challenges faced by\nfacial recognition models from two perspectives: common corruptions and\nappearance variations. We systematically design 30 OOD scenarios across 9 major\ncategories tailored for facial recognition. By simulating these challenges on\npublic datasets, we establish three robustness benchmarks: LFW-C/V, CFP-FP-C/V,\nand YTF-C/V. We then conduct extensive experiments on 19 facial recognition\nmodels and 3 commercial APIs, along with extended physical experiments on face\nmasks to assess their robustness. Next, we explore potential solutions from two\nperspectives: defense strategies and Vision-Language Models (VLMs). Based on\nthe results, we draw several key insights, highlighting the vulnerability of\nfacial recognition systems to OOD data and suggesting possible solutions.\nAdditionally, we offer a unified toolkit that includes all corruption and\nvariation types, easily extendable to other datasets. We hope that our\nbenchmarks and findings can provide guidance for future improvements in facial\nrecognition model robustness.\n","authors":["Caixin Kang","Yubo Chen","Shouwei Ruan","Shiji Zhao","Ruochen Zhang","Jiayi Wang","Shan Fu","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2412.02479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13985v2","updated":"2025-03-27T05:23:02Z","published":"2025-03-18T07:42:11Z","title":"DefectFill: Realistic Defect Generation with Inpainting Diffusion Model\n  for Visual Inspection","summary":"  Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.\n","authors":["Jaewoo Song","Daemin Park","Kanghyun Baek","Sangyub Lee","Jooyoung Choi","Eunji Kim","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.13985v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21164v1","updated":"2025-03-27T05:19:41Z","published":"2025-03-27T05:19:41Z","title":"Adversarial Wear and Tear: Exploiting Natural Damage for Generating\n  Physical-World Adversarial Examples","summary":"  The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.\n","authors":["Samra Irshad","Seungkyu Lee","Nassir Navab","Hong Joo Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2503.21164v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.06511v2","updated":"2025-03-27T05:18:09Z","published":"2024-04-09T17:59:31Z","title":"MoReVQA: Exploring Modular Reasoning Models for Video Question Answering","summary":"  This paper addresses the task of video question answering (videoQA) via a\ndecomposed multi-stage, modular reasoning framework. Previous modular methods\nhave shown promise with a single planning stage ungrounded in visual content.\nHowever, through a simple and effective baseline, we find that such systems can\nlead to brittle behavior in practice for challenging videoQA settings. Thus,\nunlike traditional single-stage planning methods, we propose a multi-stage\nsystem consisting of an event parser, a grounding stage, and a final reasoning\nstage in conjunction with an external memory. All stages are training-free, and\nperformed using few-shot prompting of large models, creating interpretable\nintermediate outputs at each stage. By decomposing the underlying planning and\ntask complexity, our method, MoReVQA, improves over prior work on standard\nvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with\nstate-of-the-art results, and extensions to related tasks (grounded videoQA,\nparagraph captioning).\n","authors":["Juhong Min","Shyamal Buch","Arsha Nagrani","Minsu Cho","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2404.06511v2.pdf","comment":"CVPR 2024; updated NExT-GQA results in Appendix"},{"id":"http://arxiv.org/abs/2503.17125v4","updated":"2025-03-27T05:17:19Z","published":"2025-03-21T13:20:39Z","title":"LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in\n  Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.\n","authors":["Chan Kim","Seung-Woo Seo","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17125v4.pdf","comment":"This paper is currently under security review and will be re-released\n  once the review is complete"},{"id":"http://arxiv.org/abs/2501.09766v3","updated":"2025-03-27T05:05:03Z","published":"2025-01-15T04:52:34Z","title":"iTool: Boosting Tool Use of Large Language Models via Iterative\n  Reinforced Fine-Tuning","summary":"  Augmenting large language models (LLMs) with external tools is known as a\npromising approach to enhancing their capabilities, especially for complex\ntasks. Synthesizing tool-use data through real-world simulations is an\neffective way to achieve it. Nevertheless, our investigation reveals that (1)\ntraining gains significantly decay as synthetic data increases. The model\nstruggles to benefit from more synthetic data due to potential data diversity\nissues, resulting in poor performance in complex scenarios. Moreover, we find\nthat (2) this challenge primarily manifests as minor discrepancies between the\nmodel's output and the ground truth response (termed as deficiency), such as\nerrors in parameter values that require complex reasoning from the context to\nresolve. To this end, we propose an iterative reinforced fine-tuning strategy\ndesigned to alleviate these challenges. This strategy involves: (1) enhancing\nthe diversity of synthetic data through path exploration of Monte Carlo Tree\nSearch. (2) iteratively identifying deficiency-related data, constructing\nfine-grained preference pairs to pinpoint deficiencies, and then applying\npreference optimization to optimize these deficiencies. Our experiments show\nthat models trained using our method achieve about 12\\% better performance than\nbaseline models, outperforming larger open-source and closed-source models.\n","authors":["Yirong Zeng","Xiao Ding","Yuxian Wang","Weiwen Liu","Wu Ning","Yutai Hou","Xu Huang","Bing Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09766v3.pdf","comment":"under review ACL"},{"id":"http://arxiv.org/abs/2503.21159v1","updated":"2025-03-27T04:57:05Z","published":"2025-03-27T04:57:05Z","title":"Multi-Objective Optimization for Privacy-Utility Balance in\n  Differentially Private Federated Learning","summary":"  Federated learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning. However, ensuring differential privacy\n(DP) in FL presents challenges due to the trade-off between model utility and\nprivacy protection. Clipping gradients before aggregation is a common strategy\nto limit privacy loss, but selecting an optimal clipping norm is non-trivial,\nas excessively high values compromise privacy, while overly restrictive\nclipping degrades model performance. In this work, we propose an adaptive\nclipping mechanism that dynamically adjusts the clipping norm using a\nmulti-objective optimization framework. By integrating privacy and utility\nconsiderations into the optimization objective, our approach balances privacy\npreservation with model accuracy. We theoretically analyze the convergence\nproperties of our method and demonstrate its effectiveness through extensive\nexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show\nthat adaptive clipping consistently outperforms fixed-clipping baselines,\nachieving improved accuracy under the same privacy constraints. This work\nhighlights the potential of dynamic clipping strategies to enhance\nprivacy-utility trade-offs in differentially private federated learning.\n","authors":["Kanishka Ranaweera","David Smith","Pubudu N. Pathirana","Ming Ding","Thierry Rakotoarivelo","Aruna Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2503.21159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21154v1","updated":"2025-03-27T04:48:29Z","published":"2025-03-27T04:48:29Z","title":"Federated Learning with Differential Privacy: An Utility-Enhanced\n  Approach","summary":"  Federated learning has emerged as an attractive approach to protect data\nprivacy by eliminating the need for sharing clients' data while reducing\ncommunication costs compared with centralized machine learning algorithms.\nHowever, recent studies have shown that federated learning alone does not\nguarantee privacy, as private data may still be inferred from the uploaded\nparameters to the central server. In order to successfully avoid data leakage,\nadopting differential privacy (DP) in the local optimization process or in the\nlocal update aggregation process has emerged as two feasible ways for achieving\nsample-level or user-level privacy guarantees respectively, in federated\nlearning models. However, compared to their non-private equivalents, these\napproaches suffer from a poor utility. To improve the privacy-utility\ntrade-off, we present a modification to these vanilla differentially private\nalgorithms based on a Haar wavelet transformation step and a novel noise\ninjection scheme that significantly lowers the asymptotic bound of the noise\nvariance. We also present a holistic convergence analysis of our proposed\nalgorithm, showing that our method yields better convergence performance than\nthe vanilla DP algorithms. Numerical experiments on real-world datasets\ndemonstrate that our method outperforms existing approaches in model utility\nwhile maintaining the same privacy guarantees.\n","authors":["Kanishka Ranaweera","Dinh C. Nguyen","Pubudu N. Pathirana","David Smith","Ming Ding","Thierry Rakotoarivelo","Aruna Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2503.21154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21150v1","updated":"2025-03-27T04:37:52Z","published":"2025-03-27T04:37:52Z","title":"The Devil is in Low-Level Features for Cross-Domain Few-Shot\n  Segmentation","summary":"  Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively.\n","authors":["Yuhan Liu","Yixiong Zou","Yuhua Li","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.21150v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21138v1","updated":"2025-03-27T04:00:49Z","published":"2025-03-27T04:00:49Z","title":"A computational theory of evaluation for parameterisable subject","summary":"  Evaluation is critical to advance decision making across domains, yet\nexisting methodologies often struggle to balance theoretical rigor and\npractical scalability. In order to reduce the cost of experimental evaluation,\nwe introduce a computational theory of evaluation for parameterisable subjects.\nWe prove upper bounds of generalized evaluation error and generalized causal\neffect error of evaluation metric on subject. We also prove efficiency, and\nconsistency to estimated causal effect of subject on metric by prediction. To\noptimize evaluation models, we propose a meta-learner to handle heterogeneous\nevaluation subjects space. Comparing with other computational approaches, our\n(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12\nscenes, including individual medicine, scientific simulation, business\nactivities, and quantum trade. The evaluation time is reduced 3-7 order of\nmagnitude comparing with experiments or simulations.\n","authors":["Hedong Yan"],"pdf_url":"https://arxiv.org/pdf/2503.21138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06602v2","updated":"2025-03-27T03:56:00Z","published":"2024-12-09T15:50:25Z","title":"Towards Controllable Speech Synthesis in the Era of Large Language\n  Models: A Survey","summary":"  Text-to-speech (TTS), also known as speech synthesis, is a prominent research\narea that aims to generate natural-sounding human speech from text. Recently,\nwith the increasing industrial demand, TTS technologies have evolved beyond\nsynthesizing human-like speech to enabling controllable speech generation. This\nincludes fine-grained control over various attributes of synthesized speech\nsuch as emotion, prosody, timbre, and duration. In addition, advancements in\ndeep learning, such as diffusion and large language models, have significantly\nenhanced controllable TTS over the past several years. In this work, we conduct\na comprehensive survey of controllable TTS, covering approaches ranging from\nbasic control techniques to methods utilizing natural language prompts, aiming\nto provide a clear understanding of the current state of research. We examine\nthe general controllable TTS pipeline, challenges, model architectures, and\ncontrol strategies, offering a comprehensive and clear taxonomy of existing\nmethods. Additionally, we provide a detailed summary of datasets and evaluation\nmetrics and shed some light on the applications and future directions of\ncontrollable TTS. To the best of our knowledge, this survey paper provides the\nfirst comprehensive review of emerging controllable TTS methods, which can\nserve as a beneficial resource for both academic researchers and industrial\npractitioners.\n","authors":["Tianxin Xie","Yan Rong","Pengfei Zhang","Wenwu Wang","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2412.06602v2.pdf","comment":"A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6\n  figures, 317 references. Under review"},{"id":"http://arxiv.org/abs/2503.20752v2","updated":"2025-03-27T03:13:00Z","published":"2025-03-26T17:38:06Z","title":"Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning","summary":"  Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https://tanhuajie.github.io/ReasonRFT\n","authors":["Huajie Tan","Yuheng Ji","Xiaoshuai Hao","Minglan Lin","Pengwei Wang","Zhongyuan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20752v2.pdf","comment":"35 pages, 22 figures"},{"id":"http://arxiv.org/abs/2503.21109v1","updated":"2025-03-27T03:03:09Z","published":"2025-03-27T03:03:09Z","title":"Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous\n  Processor Co-Execution","summary":"  Deep Neural Networks (DNNs) are increasingly deployed across diverse\nindustries, driving demand for mobile device support. However, existing mobile\ninference frameworks often rely on a single processor per model, limiting\nhardware utilization and causing suboptimal performance and energy efficiency.\nExpanding DNN accessibility on mobile platforms requires adaptive,\nresource-efficient solutions to meet rising computational needs without\ncompromising functionality. Parallel inference of multiple DNNs on\nheterogeneous processors remains challenging. Some works partition DNN\noperations into subgraphs for parallel execution across processors, but these\noften create excessive subgraphs based only on hardware compatibility,\nincreasing scheduling complexity and memory overhead.\n  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)\nstrategy for optimizing multi-DNN inference on mobile heterogeneous processors.\nADMS constructs an optimal subgraph partitioning strategy offline, balancing\nhardware operation support and scheduling granularity, and uses a\nprocessor-state-aware algorithm to dynamically adjust workloads based on\nreal-time conditions. This ensures efficient workload distribution and\nmaximizes processor utilization. Experiments show ADMS reduces multi-DNN\ninference latency by 4.04 times compared to vanilla frameworks.\n","authors":["Yunquan Gao","Zhiguo Zhang","Praveen Kumar Donta","Chinmaya Kumar Dehury","Xiujun Wang","Dusit Niyato","Qiyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21109v1.pdf","comment":"14 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.14734v2","updated":"2025-03-27T02:52:43Z","published":"2025-03-18T21:06:21Z","title":"GR00T N1: An Open Foundation Model for Generalist Humanoid Robots","summary":"  General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.\n","authors":[" NVIDIA"," :","Johan Bjorck","Fernando Castañeda","Nikita Cherniadev","Xingye Da","Runyu Ding","Linxi \"Jim\" Fan","Yu Fang","Dieter Fox","Fengyuan Hu","Spencer Huang","Joel Jang","Zhenyu Jiang","Jan Kautz","Kaushil Kundalia","Lawrence Lao","Zhiqi Li","Zongyu Lin","Kevin Lin","Guilin Liu","Edith Llontop","Loic Magne","Ajay Mandlekar","Avnish Narayan","Soroush Nasiriany","Scott Reed","You Liang Tan","Guanzhi Wang","Zu Wang","Jing Wang","Qi Wang","Jiannan Xiang","Yuqi Xie","Yinzhen Xu","Zhenjia Xu","Seonghyeon Ye","Zhiding Yu","Ao Zhang","Hao Zhang","Yizhou Zhao","Ruijie Zheng","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.14734v2.pdf","comment":"Authors are listed alphabetically. Project leads are Linxi \"Jim\" Fan\n  and Yuke Zhu. For more information, see\n  https://developer.nvidia.com/isaac/gr00t"},{"id":"http://arxiv.org/abs/2410.21897v2","updated":"2025-03-27T02:39:50Z","published":"2024-10-29T09:42:07Z","title":"Semi-Supervised Self-Learning Enhanced Music Emotion Recognition","summary":"  Music emotion recognition (MER) aims to identify the emotions conveyed in a\ngiven musical piece. However, currently, in the field of MER, the available\npublic datasets have limited sample sizes. Recently, segment-based methods for\nemotion-related tasks have been proposed, which train backbone networks on\nshorter segments instead of entire audio clips, thereby naturally augmenting\ntraining samples without requiring additional resources. Then, the predicted\nsegment-level results are aggregated to obtain the entire song prediction. The\nmost commonly used method is that the segment inherits the label of the clip\ncontaining it, but music emotion is not constant during the whole clip. Doing\nso will introduce label noise and make the training easy to overfit. To handle\nthe noisy label issue, we propose a semi-supervised self-learning (SSSL)\nmethod, which can differentiate between samples with correct and incorrect\nlabels in a self-learning manner, thus effectively utilizing the augmented\nsegment-level data. Experiments on three public emotional datasets demonstrate\nthat the proposed method can achieve better or comparable performance.\n","authors":["Yifu Sun","Xulong Zhang","Monan Zhou","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2410.21897v2.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.21098v1","updated":"2025-03-27T02:36:48Z","published":"2025-03-27T02:36:48Z","title":"Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search","summary":"  Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.\n","authors":["Yedan Shen","Kaixin Wu","Yuechen Ding","Jingyuan Wen","Hong Liu","Mingjie Zhong","Zhouhan Lin","Jia Xu","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2503.21098v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2412.06779v2","updated":"2025-03-27T02:34:48Z","published":"2024-12-09T18:58:43Z","title":"AnyBimanual: Transferring Unimanual Policy for General Bimanual\n  Manipulation","summary":"  Performing general language-conditioned bimanual manipulation tasks is of\ngreat importance for many applications ranging from household service to\nindustrial assembly. However, collecting bimanual manipulation data is\nexpensive due to the high-dimensional action space, which poses challenges for\nconventional methods to handle general bimanual manipulation tasks. In\ncontrast, unimanual policy has recently demonstrated impressive\ngeneralizability across a wide range of tasks because of scaled model\nparameters and training data, which can provide sharable manipulation knowledge\nfor bimanual systems. To this end, we propose a plug-and-play method named\nAnyBimanual, which transfers pre-trained unimanual policy to general bimanual\nmanipulation policy with few bimanual demonstrations. Specifically, we first\nintroduce a skill manager to dynamically schedule the skill representations\ndiscovered from pre-trained unimanual policy for bimanual manipulation tasks,\nwhich linearly combines skill primitives with task-oriented compensation to\nrepresent the bimanual manipulation instruction. To mitigate the observation\ndiscrepancy between unimanual and bimanual systems, we present a visual aligner\nto generate soft masks for visual embedding of the workspace, which aims to\nalign visual input of unimanual policy model for each arm with those during\npretraining stage. AnyBimanual shows superiority on 12 simulated tasks from\nRLBench2 with a sizable 12.67% improvement in success rate over previous\nmethods. Experiments on 9 real-world tasks further verify its practicality with\nan average success rate of 84.62%.\n","authors":["Guanxing Lu","Tengbo Yu","Haoyuan Deng","Season Si Chen","Yansong Tang","Ziwei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06779v2.pdf","comment":"Project page: https://anybimanual.github.io/"},{"id":"http://arxiv.org/abs/2310.04722v2","updated":"2025-03-27T02:31:56Z","published":"2023-10-07T07:51:34Z","title":"A Holistic Evaluation of Piano Sound Quality","summary":"  This paper aims to develop a holistic evaluation method for piano sound\nquality to assist in purchasing decisions. Unlike previous studies that focused\non the effect of piano performance techniques on sound quality, this study\nevaluates the inherent sound quality of different pianos. To derive quality\nevaluation systems, the study uses subjective questionnaires based on a piano\nsound quality dataset. The method selects the optimal piano classification\nmodels by comparing the fine-tuning results of different pre-training models of\nConvolutional Neural Networks (CNN). To improve the interpretability of the\nmodels, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The\nresults reveal that musically trained individuals are better able to\ndistinguish between the sound quality differences of different pianos. The best\nfine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the\npiano classifier. However, the dataset is limited, and the audio is sliced to\nincrease its quantity, resulting in a lack of diversity and balance, so we use\nfocal loss to reduce the impact of data imbalance. To optimize the method, the\ndataset will be expanded, or few-shot learning techniques will be employed in\nfuture research.\n","authors":["Monan Zhou","Shangda Wu","Shaohua Ji","Zijin Li","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2310.04722v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.01095v2","updated":"2025-03-27T02:26:40Z","published":"2024-12-02T04:10:14Z","title":"VERA: Explainable Video Anomaly Detection via Verbalized Learning of\n  Vision-Language Models","summary":"  The rapid advancement of vision-language models (VLMs) has established a new\nparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneously\ndetect anomalies and provide comprehendible explanations for the decisions.\nExisting work in this direction often assumes the complex reasoning required\nfor VAD exceeds the capabilities of pretrained VLMs. Consequently, these\napproaches either incorporate specialized reasoning modules during inference or\nrely on instruction tuning datasets through additional training to adapt VLMs\nfor VAD. However, such strategies often incur substantial computational costs\nor data annotation overhead. To address these challenges in explainable VAD, we\nintroduce a verbalized learning framework named VERA that enables VLMs to\nperform VAD without model parameter modifications. Specifically, VERA\nautomatically decomposes the complex reasoning required for VAD into\nreflections on simpler, more focused guiding questions capturing distinct\nabnormal patterns. It treats these reflective questions as learnable parameters\nand optimizes them through data-driven verbal interactions between learner and\noptimizer VLMs, using coarsely labeled training data. During inference, VERA\nembeds the learned questions into model prompts to guide VLMs in generating\nsegment-level anomaly scores, which are then refined into frame-level scores\nvia the fusion of scene and temporal contexts. Experimental results on\nchallenging benchmarks demonstrate that the learned questions of VERA are\nhighly adaptable, significantly improving both detection performance and\nexplainability of VLMs for VAD.\n","authors":["Muchao Ye","Weiyang Liu","Pan He"],"pdf_url":"https://arxiv.org/pdf/2412.01095v2.pdf","comment":"Accepted in CVPR 2025"},{"id":"http://arxiv.org/abs/2503.21095v1","updated":"2025-03-27T02:21:42Z","published":"2025-03-27T02:21:42Z","title":"Confidence Adjusted Surprise Measure for Active Resourceful Trials\n  (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material\n  Discovery under Resource Constraints","summary":"  Accelerating the discovery and manufacturing of advanced materials with\nspecific properties is a critical yet formidable challenge due to vast search\nspace, high costs of experiments, and time-intensive nature of material\ncharacterization. In recent years, active learning, where a surrogate machine\nlearning (ML) model mimics the scientific discovery process of a human\nscientist, has emerged as a promising approach to address these challenges by\nguiding experimentation toward high-value outcomes with a limited budget. Among\nthe diverse active learning philosophies, the concept of surprise (capturing\nthe divergence between expected and observed outcomes) has demonstrated\nsignificant potential to drive experimental trials and refine predictive\nmodels. Scientific discovery often stems from surprise thereby making it a\nnatural driver to guide the search process. Despite its promise, prior studies\nleveraging surprise metrics such as Shannon and Bayesian surprise lack\nmechanisms to account for prior confidence, leading to excessive exploration of\nuncertain regions that may not yield useful information. To address this, we\npropose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials\n(CA-SMART), a novel Bayesian active learning framework tailored for optimizing\ndata-driven experimentation. On a high level, CA-SMART incorporates\nConfidence-Adjusted Surprise (CAS) to dynamically balance exploration and\nexploitation by amplifying surprises in regions where the model is more certain\nwhile discounting them in highly uncertain areas. We evaluated CA-SMART on two\nbenchmark functions (Six-Hump Camelback and Griewank) and in predicting the\nfatigue strength of steel. The results demonstrate superior accuracy and\nefficiency compared to traditional surprise metrics, standard Bayesian\nOptimization (BO) acquisition functions and conventional ML methods.\n","authors":["Ahmed Shoyeb Raihan","Zhichao Liu","Tanveer Hossain Bhuiyan","Imtiaz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2503.21095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20104v4","updated":"2025-03-27T02:17:08Z","published":"2024-12-28T10:12:12Z","title":"SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis","summary":"  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n","authors":["Wenkun He","Yun Liu","Ruitao Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20104v4.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.21088v1","updated":"2025-03-27T02:03:25Z","published":"2025-03-27T02:03:25Z","title":"ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging","summary":"  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.\n","authors":["Haoming Xu","Shuxun Wang","Yanqiu Zhao","Yi Zhong","Ziyan Jiang","Ningyuan Zhao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21088v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.13768v2","updated":"2025-03-27T02:02:18Z","published":"2024-11-21T00:34:30Z","title":"Evaluation-Driven Development of LLM Agents: A Process Model and\n  Reference Architecture","summary":"  Large Language Models (LLMs) have enabled the emergence of LLM agents:\nautonomous systems capable of achieving under-specified goals and adapting\npost-deployment, often without explicit code or model changes. Evaluating these\nagents is critical to ensuring their performance and safety, especially given\ntheir dynamic, probabilistic, and evolving nature. However, traditional\napproaches such as predefined test cases and standard redevelopment pipelines\nstruggle to address the unique challenges of LLM agent evaluation. These\nchallenges include capturing open-ended behaviors, handling emergent outcomes,\nand enabling continuous adaptation over the agent's lifecycle. To address these\nissues, we propose an evaluation-driven development approach, inspired by\ntest-driven and behavior-driven development but reimagined for the unique\ncharacteristics of LLM agents. Through a multivocal literature review (MLR), we\nsynthesize the limitations of existing LLM evaluation methods and introduce a\nnovel process model and reference architecture tailored for evaluation-driven\ndevelopment of LLM agents. Our approach integrates online (runtime) and offline\n(redevelopment) evaluations, enabling adaptive runtime adjustments and\nsystematic iterative refinement of pipelines, artifacts, system architecture,\nand LLMs themselves. By continuously incorporating evaluation results,\nincluding fine-grained feedback from human and AI evaluators, into each stage\nof development and operation, this framework ensures that LLM agents remain\naligned with evolving goals, user needs, and governance standards.\n","authors":["Boming Xia","Qinghua Lu","Liming Zhu","Zhenchang Xing","Dehai Zhao","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23749v4","updated":"2025-03-27T02:00:07Z","published":"2024-10-31T09:09:39Z","title":"LSEAttention is All You Need for Time Series Forecasting","summary":"  Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often falls short compared to simpler linear\nbaselines. Previous research has identified the traditional attention mechanism\nas a key factor limiting their effectiveness in this domain. To bridge this\ngap, we introduce LATST, a novel approach designed to mitigate entropy collapse\nand training instability common challenges in Transformer-based time series\nforecasting. We rigorously evaluate LATST across multiple real-world\nmultivariate time series datasets, demonstrating its ability to outperform\nexisting state-of-the-art Transformer models. Notably, LATST manages to achieve\ncompetitive performance with fewer parameters than some linear models on\ncertain datasets, highlighting its efficiency and effectiveness.\n","authors":["Dizhen Liang"],"pdf_url":"https://arxiv.org/pdf/2410.23749v4.pdf","comment":"8 pages with referencing, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2503.21074v1","updated":"2025-03-27T01:19:47Z","published":"2025-03-27T01:19:47Z","title":"Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual\n  Similarity Between Indus and Tibetan-Yi Corridor Writing Systems","summary":"  This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized.\n","authors":["Ooha Lakkadi Reddy"],"pdf_url":"https://arxiv.org/pdf/2503.21074v1.pdf","comment":"106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21\n  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this\n  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at\n  Duke Kunshan University; accepted for presentation at the 2025 Computer\n  Applications and Quantitative Methods in Archaeology Conference, Athens"},{"id":"http://arxiv.org/abs/2503.21067v1","updated":"2025-03-27T00:57:27Z","published":"2025-03-27T00:57:27Z","title":"AskSport: Web Application for Sports Question-Answering","summary":"  This paper introduces AskSport, a question-answering web application about\nsports. It allows users to ask questions using natural language and retrieve\nthe three most relevant answers, including related information and documents.\nThe paper describes the characteristics and functionalities of the application,\nincluding use cases demonstrating its ability to return names and numerical\nvalues. AskSport and its implementation are available for public access on\nHuggingFace.\n","authors":["Enzo B Onofre","Leonardo M P Moraes","Cristina D Aguiar"],"pdf_url":"https://arxiv.org/pdf/2503.21067v1.pdf","comment":"for accessing the application, see\n  https://huggingface.co/spaces/leomaurodesenv/qasports-website"},{"id":"http://arxiv.org/abs/2503.19176v2","updated":"2025-03-27T00:51:02Z","published":"2025-03-24T21:57:59Z","title":"SoK: How Robust is Audio Watermarking in Generative AI models?","summary":"  Audio watermarking is increasingly used to verify the provenance of\nAI-generated content, enabling applications such as detecting AI-generated\nspeech, protecting music IP, and defending against voice cloning. To be\neffective, audio watermarks must resist removal attacks that distort signals to\nevade detection. While many schemes claim robustness, these claims are\ntypically tested in isolation and against a limited set of attacks. A\nsystematic evaluation against diverse removal attacks is lacking, hindering\npractical deployment. In this paper, we investigate whether recent watermarking\nschemes that claim robustness can withstand a broad range of removal attacks.\nFirst, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we\nsummarize their underlying technologies and potential vulnerabilities. We then\npresent a large-scale empirical study to assess their robustness. To support\nthis, we build an evaluation framework encompassing 22 types of removal attacks\n(109 configurations) including signal-level, physical-level, and AI-induced\ndistortions. We reproduce 9 watermarking schemes using open-source code,\nidentify 8 new highly effective attacks, and highlight 11 key findings that\nexpose the fundamental limitations of these methods across 3 public datasets.\nOur results reveal that none of the surveyed schemes can withstand all tested\ndistortions. This evaluation offers a comprehensive view of how current\nwatermarking methods perform under real-world threats. Our demo and code are\navailable at https://sokaudiowm.github.io/.\n","authors":["Yizhu Wen","Ashwin Innuganti","Aaron Bien Ramos","Hanqing Guo","Qiben Yan"],"pdf_url":"https://arxiv.org/pdf/2503.19176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12249v2","updated":"2025-03-27T00:09:03Z","published":"2024-09-18T18:14:00Z","title":"GCA-SUNet: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting","summary":"  Exemplar-Free Counting aims to count objects of interest without intensive\nannotations of objects or exemplars. To achieve this, we propose a Gated\nContext-Aware Swin-UNet (GCA-SUNet) to directly map an input image to the\ndensity map of countable objects. Specifically, a set of Swin transformers form\nan encoder to derive a robust feature representation, and a Gated Context-Aware\nModulation block is designed to suppress irrelevant objects or background\nthrough a gate mechanism and exploit the attentive support of objects of\ninterest through a self-similarity matrix. The gate strategy is also\nincorporated into the bottleneck network and the decoder of the Swin-UNet to\nhighlight the features most relevant to objects of interest. By explicitly\nexploiting the attentive support among countable objects and eliminating\nirrelevant features through the gate mechanisms, the proposed GCA-SUNet focuses\non and counts objects of interest without relying on predefined categories or\nexemplars. Experimental results on the real-world datasets such as FSC-147 and\nCARPK demonstrate that GCA-SUNet significantly and consistently outperforms\nstate-of-the-art methods. The code is available at\nhttps://github.com/Amordia/GCA-SUNet.\n","authors":["Yuzhe Wu","Yipeng Xu","Tianyu Xu","Jialu Zhang","Jianfeng Ren","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.12249v2.pdf","comment":"Accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2402.03664v5","updated":"2025-03-27T17:59:59Z","published":"2024-02-06T03:36:05Z","title":"Partial Gromov-Wasserstein Metric","summary":"  The Gromov-Wasserstein (GW) distance has gained increasing interest in the\nmachine learning community in recent years, as it allows for the comparison of\nmeasures in different metric spaces. To overcome the limitations imposed by the\nequal mass requirements of the classical GW problem, researchers have begun\nexploring its application in unbalanced settings. However, Unbalanced GW (UGW)\ncan only be regarded as a discrepancy rather than a rigorous metric/distance\nbetween two metric measure spaces (mm-spaces). In this paper, we propose a\nparticular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We\nestablish that PGW is a well-defined metric between mm-spaces and discuss its\ntheoretical properties, including the existence of a minimizer for the PGW\nproblem and the relationship between PGW and GW, among others. We then propose\ntwo variants of the Frank-Wolfe algorithm for solving the PGW problem and show\nthat they are mathematically and computationally equivalent. Moreover, based on\nour PGW metric, we introduce the analogous concept of barycenters for\nmm-spaces. Finally, we validate the effectiveness of our PGW metric and related\nsolvers in applications such as shape matching, shape retrieval, and shape\ninterpolation, comparing them against existing baselines. Our code is available\nat https://github.com/mint-vu/PGW_Metric.\n","authors":["Yikun Bai","Rocio Diaz Martin","Abihith Kothapalli","Hengrong Du","Xinran Liu","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2402.03664v5.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.21777v1","updated":"2025-03-27T17:59:52Z","published":"2025-03-27T17:59:52Z","title":"Test-Time Visual In-Context Tuning","summary":"  Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.\n","authors":["Jiahao Xie","Alessio Tonioni","Nathalie Rauschmayr","Federico Tombari","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2503.21777v1.pdf","comment":"CVPR 2025. Code: https://github.com/Jiahao000/VICT"},{"id":"http://arxiv.org/abs/2503.21756v1","updated":"2025-03-27T17:57:03Z","published":"2025-03-27T17:57:03Z","title":"A Unified Framework for Diffusion Bridge Problems: Flow Matching and\n  Schrödinger Matching into One","summary":"  The bridge problem is to find an SDE (or sometimes an ODE) that bridges two\ngiven distributions. The application areas of the bridge problem are enormous,\namong which the recent generative modeling (e.g., conditional or unconditional\nimage generation) is the most popular. Also the famous Schr\\\"{o}dinger bridge\nproblem, a widely known problem for a century, is a special instance of the\nbridge problem. Two most popular algorithms to tackle the bridge problems in\nthe deep learning era are: (conditional) flow matching and iterative fitting\nalgorithms, where the former confined to ODE solutions, and the latter\nspecifically for the Schr\\\"{o}dinger bridge problem. The main contribution of\nthis article is in two folds: i) We provide concise reviews of these algorithms\nwith technical details to some extent; ii) We propose a novel unified\nperspective and framework that subsumes these seemingly unrelated algorithms\n(and their variants) into one. In particular, we show that our unified\nframework can instantiate the Flow Matching (FM) algorithm, the (mini-batch)\noptimal transport FM algorithm, the (mini-batch) Schr\\\"{o}dinger bridge FM\nalgorithm, and the deep Schr\\\"{o}dinger bridge matching (DSBM) algorithm as its\nspecial cases. We believe that this unified framework will be useful for\nviewing the bridge problems in a more general and flexible perspective, and in\nturn can help researchers and practitioners to develop new bridge algorithms in\ntheir fields.\n","authors":["Minyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2503.21756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21730v1","updated":"2025-03-27T17:45:06Z","published":"2025-03-27T17:45:06Z","title":"Effective Skill Unlearning through Intervention and Abstention","summary":"  Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning\n","authors":["Yongce Li","Chung-En Sun","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2503.21730v1.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2501.12911v3","updated":"2025-03-27T17:44:27Z","published":"2025-01-22T14:37:44Z","title":"A Selective Homomorphic Encryption Approach for Faster\n  Privacy-Preserving Federated Learning","summary":"  Federated learning is a machine learning method that supports training models\non decentralized devices or servers, where each holds its local data, removing\nthe need for data exchange. This approach is especially useful in healthcare,\nas it enables training on sensitive data without needing to share them. The\nnature of federated learning necessitates robust security precautions due to\ndata leakage concerns during communication. To address this issue, we propose a\nnew approach that employs selective encryption, homomorphic encryption,\ndifferential privacy, and bit-wise scrambling to minimize data leakage while\nachieving good execution performance. Our technique , FAS (fast and secure\nfederated learning) is used to train deep learning models on medical imaging\ndata. We implemented our technique using the Flower framework and compared with\na state-of-the-art federated learning approach that also uses selective\nhomomorphic encryption. Our experiments were run in a cluster of eleven\nphysical machines to create a real-world federated learning scenario on\ndifferent datasets. We observed that our approach is up to 90\\% faster than\napplying fully homomorphic encryption on the model weights. In addition, we can\navoid the pretraining step that is required by our competitor and can save up\nto 46% in terms of total execution time. While our approach was faster, it\nobtained similar security results as the competitor.\n","authors":["Abdulkadir Korkmaz","Praveen Rao"],"pdf_url":"https://arxiv.org/pdf/2501.12911v3.pdf","comment":"23 pages, 32 figures"},{"id":"http://arxiv.org/abs/2503.21722v1","updated":"2025-03-27T17:35:38Z","published":"2025-03-27T17:35:38Z","title":"Energy Minimization for Participatory Federated Learning in IoT Analyzed\n  via Game Theory","summary":"  The Internet of Things requires intelligent decision making in many\nscenarios. To this end, resources available at the individual nodes for sensing\nor computing, or both, can be leveraged. This results in approaches known as\nparticipatory sensing and federated learning, respectively. We investigate the\nsimultaneous implementation of both, through a distributed approach based on\nempowering local nodes with game theoretic decision making. A global objective\nof energy minimization is combined with the individual node's optimization of\nlocal expenditure for sensing and transmitting data over multiple learning\nrounds. We present extensive evaluations of this technique, based on both a\ntheoretical framework and experiments in a simulated network scenario with real\ndata. Such a distributed approach can reach a desired level of accuracy for\nfederated learning without a centralized supervision of the data collector.\nHowever, depending on the weight attributed to the local costs of the single\nnode, it may also result in a significantly high Price of Anarchy (from 1.28\nonwards). Thus, we argue for the need of incentive mechanisms, possibly based\non Age of Information of the single nodes.\n","authors":["Alessandro Buratto","Elia Guerra","Marco Miozzo","Paolo Dini","Leonardo Badia"],"pdf_url":"https://arxiv.org/pdf/2503.21722v1.pdf","comment":"6 pages, 6 figures, 2 tables, conference"},{"id":"http://arxiv.org/abs/2503.20639v2","updated":"2025-03-27T17:19:38Z","published":"2025-03-26T15:33:26Z","title":"PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction","summary":"  Reliable drug safety reference databases are essential for pharmacovigilance,\nyet existing resources like SIDER are outdated and static. We introduce PVLens,\nan automated system that extracts labeled safety information from FDA\nStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates\nautomation with expert oversight through a web-based review tool. In validation\nagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall\n(0.983) and moderate precision (0.799). By offering a scalable, more accurate\nand continuously updated alternative to SIDER, PVLens enhances real-time\npharamcovigilance with improved accuracy and contemporaneous insights.\n","authors":["Jeffery L Painter","Gregory E Powell","Andrew Bate"],"pdf_url":"https://arxiv.org/pdf/2503.20639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21704v1","updated":"2025-03-27T17:10:05Z","published":"2025-03-27T17:10:05Z","title":"Learning to Represent Individual Differences for Choice Decision Making","summary":"  Human decision making can be challenging to predict because decisions are\naffected by a number of complex factors. Adding to this complexity,\ndecision-making processes can differ considerably between individuals, and\nmethods aimed at predicting human decisions need to take individual differences\ninto account. Behavioral science offers methods by which to measure individual\ndifferences (e.g., questionnaires, behavioral models), but these are often\nnarrowed down to low dimensions and not tailored to specific prediction tasks.\nThis paper investigates the use of representation learning to measure\nindividual differences from behavioral experiment data. Representation learning\noffers a flexible approach to create individual embeddings from data that are\nboth structured (e.g., demographic information) and unstructured (e.g., free\ntext), where the flexibility provides more options for individual difference\nmeasures for personalization, e.g., free text responses may allow for\nopen-ended questions that are less privacy-sensitive. In the current paper we\nuse representation learning to characterize individual differences in human\nperformance on an economic decision-making task. We demonstrate that models\nusing representation learning to capture individual differences consistently\nimprove decision predictions over models without representation learning, and\neven outperform well-known theory-based behavioral models used in these\nenvironments. Our results propose that representation learning offers a useful\nand flexible tool to capture individual differences.\n","authors":["Yan-Ying Chen","Yue Weng","Alexandre Filipowicz","Rumen Iliev","Francine Chen","Shabnam Hakimi","Yanxia Zhang","Matthew Lee","Kent Lyons","Charlene Wu"],"pdf_url":"https://arxiv.org/pdf/2503.21704v1.pdf","comment":"Published in IJCAI MRC 2022"},{"id":"http://arxiv.org/abs/2503.21686v1","updated":"2025-03-27T16:54:15Z","published":"2025-03-27T16:54:15Z","title":"Molecular Quantum Transformer","summary":"  The Transformer model, renowned for its powerful attention mechanism, has\nachieved state-of-the-art performance in various artificial intelligence tasks\nbut faces challenges such as high computational cost and memory usage.\nResearchers are exploring quantum computing to enhance the Transformer's\ndesign, though it still shows limited success with classical data. With a\ngrowing focus on leveraging quantum machine learning for quantum data,\nparticularly in quantum chemistry, we propose the Molecular Quantum Transformer\n(MQT) for modeling interactions in molecular quantum systems. By utilizing\nquantum circuits to implement the attention mechanism on the molecular\nconfigurations, MQT can efficiently calculate ground-state energies for all\nconfigurations. Numerical demonstrations show that in calculating ground-state\nenergies for H_2, LiH, BeH_2, and H_4, MQT outperforms the classical\nTransformer, highlighting the promise of quantum effects in Transformer\nstructures. Furthermore, its pretraining capability on diverse molecular data\nfacilitates the efficient learning of new molecules, extending its\napplicability to complex molecular systems with minimal additional effort. Our\nmethod offers an alternative to existing quantum algorithms for estimating\nground-state energies, opening new avenues in quantum chemistry and materials\nscience.\n","authors":["Yuichi Kamata","Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2503.21686v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.21681v1","updated":"2025-03-27T16:49:31Z","published":"2025-03-27T16:49:31Z","title":"A Comprehensive Benchmark for RNA 3D Structure-Function Modeling","summary":"  The RNA structure-function relationship has recently garnered significant\nattention within the deep learning community, promising to grow in importance\nas nucleic acid structure models advance. However, the absence of standardized\nand accessible benchmarks for deep learning on RNA 3D structures has impeded\nthe development of models for RNA functional characteristics.\n  In this work, we introduce a set of seven benchmarking datasets for RNA\nstructure-function prediction, designed to address this gap. Our library builds\non the established Python library rnaglib, and offers easy data distribution\nand encoding, splitters and evaluation methods, providing a convenient\nall-in-one framework for comparing models. Datasets are implemented in a fully\nmodular and reproducible manner, facilitating for community contributions and\ncustomization. Finally, we provide initial baseline results for all tasks using\na graph neural network.\n  Source code: https://github.com/cgoliver/rnaglib\n  Documentation: https://rnaglib.org\n","authors":["Luis Wyss","Vincent Mallet","Wissam Karroucha","Karsten Borgwardt","Carlos Oliver"],"pdf_url":"https://arxiv.org/pdf/2503.21681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21677v1","updated":"2025-03-27T16:47:46Z","published":"2025-03-27T16:47:46Z","title":"A tale of two goals: leveraging sequentiality in multi-goal scenarios","summary":"  Several hierarchical reinforcement learning methods leverage planning to\ncreate a graph or sequences of intermediate goals, guiding a lower-level\ngoal-conditioned (GC) policy to reach some final goals. The low-level policy is\ntypically conditioned on the current goal, with the aim of reaching it as\nquickly as possible. However, this approach can fail when an intermediate goal\ncan be reached in multiple ways, some of which may make it impossible to\ncontinue toward subsequent goals. To address this issue, we introduce two\ninstances of Markov Decision Process (MDP) where the optimization objective\nfavors policies that not only reach the current goal but also subsequent ones.\nIn the first, the agent is conditioned on both the current and final goals,\nwhile in the second, it is conditioned on the next two goals in the sequence.\nWe conduct a series of experiments on navigation and pole-balancing tasks in\nwhich sequences of intermediate goals are given. By evaluating policies trained\nwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,\nin most cases, conditioning on the next two goals improves stability and sample\nefficiency over other approaches.\n","authors":["Olivier Serris","Stéphane Doncieux","Olivier Sigaud"],"pdf_url":"https://arxiv.org/pdf/2503.21677v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.21676v1","updated":"2025-03-27T16:43:45Z","published":"2025-03-27T16:43:45Z","title":"How do language models learn facts? Dynamics, curricula and\n  hallucinations","summary":"  Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.\n","authors":["Nicolas Zucchet","Jörg Bornschein","Stephanie Chan","Andrew Lampinen","Razvan Pascanu","Soham De"],"pdf_url":"https://arxiv.org/pdf/2503.21676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00068v3","updated":"2025-03-27T16:25:38Z","published":"2024-09-30T09:38:47Z","title":"Denoising VAE as an Explainable Feature Reduction and Diagnostic\n  Pipeline for Autism Based on Resting state fMRI","summary":"  Autism spectrum disorders (ASDs) are developmental conditions characterized\nby restricted interests and difficulties in communication. The complexity of\nASD has resulted in a deficiency of objective diagnostic biomarkers. Deep\nlearning methods have gained recognition for addressing these challenges in\nneuroimaging analysis, but finding and interpreting such diagnostic biomarkers\nare still challenging computationally. Here, we propose a feature reduction\npipeline using resting-state fMRI data. We used Craddock atlas and Power atlas\nto extract functional connectivity data from rs-fMRI, resulting in over 30\nthousand features. By using a denoising variational autoencoder, our proposed\npipeline further compresses the connectivity features into 5 latent Gaussian\ndistributions, providing is a low-dimensional representation of the data to\npromote computational efficiency and interpretability. To test the method, we\nemployed the extracted latent representations to classify ASD using traditional\nclassifiers such as SVM on a large multi-site dataset. The 95% confidence\ninterval for the prediction accuracy of SVM is [0.63, 0.76] after site\nharmonization using the extracted latent distributions. Without using DVAE for\ndimensionality reduction, the prediction accuracy is 0.70, which falls within\nthe interval. The DVAE successfully encoded the diagnostic information from\nrs-fMRI data without sacrificing prediction performance. The runtime for\ntraining the DVAE and obtaining classification results from its extracted\nlatent features was 7 times shorter compared to training classifiers directly\non the raw data. Our findings suggest that the Power atlas provides more\neffective brain connectivity insights for diagnosing ASD than Craddock atlas.\nAdditionally, we visualized the latent representations to gain insights into\nthe brain networks contributing to the differences between ASD and neurotypical\nbrains.\n","authors":["Xinyuan Zheng","Orren Ravid","Robert A. J. Barry","Yoojean Kim","Qian Wang","Young-geun Kim","Xi Zhu","Xiaofu He"],"pdf_url":"https://arxiv.org/pdf/2410.00068v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17712v2","updated":"2025-03-27T16:22:43Z","published":"2024-05-28T00:08:29Z","title":"A Context-Aware Approach for Enhancing Data Imputation with Pre-trained\n  Language Models","summary":"  This paper presents a novel approach named \\textbf{C}ontextually\n\\textbf{R}elevant \\textbf{I}mputation leveraging pre-trained \\textbf{L}anguage\n\\textbf{M}odels (\\textbf{CRILM}) for handling missing data in tabular datasets.\nInstead of relying on traditional numerical estimations, CRILM uses pre-trained\nlanguage models (LMs) to create contextually relevant descriptors for missing\nvalues. This method aligns datasets with LMs' strengths, allowing large LMs to\ngenerate these descriptors and small LMs to be fine-tuned on the enriched\ndatasets for enhanced downstream task performance. Our evaluations demonstrate\nCRILM's superior performance and robustness across MCAR, MAR, and challenging\nMNAR scenarios, with up to a 10\\% improvement over the best-performing\nbaselines. By mitigating biases, particularly in MNAR settings, CRILM improves\ndownstream task performance and offers a cost-effective solution for\nresource-constrained environments.\n","authors":["Ahatsham Hayat","Mohammad Rashedul Hasan"],"pdf_url":"https://arxiv.org/pdf/2405.17712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02087v3","updated":"2025-03-27T16:06:12Z","published":"2024-11-04T13:49:26Z","title":"An Exponential Separation Between Quantum and Quantum-Inspired Classical\n  Algorithms for Linear Systems","summary":"  Achieving a provable exponential quantum speedup for an important machine\nlearning task has been a central research goal since the seminal HHL quantum\nalgorithm for solving linear systems and the subsequent quantum recommender\nsystems algorithm by Kerenidis and Prakash. These algorithms were initially\nbelieved to be strong candidates for exponential speedups, but a lower bound\nruling out similar classical improvements remained absent. In breakthrough work\nby Tang, it was demonstrated that this lack of progress in classical lower\nbounds was for good reasons. Concretely, she gave a classical counterpart of\nthe quantum recommender systems algorithm, reducing the quantum advantage to a\nmere polynomial. Her approach is quite general and was named quantum-inspired\nclassical algorithms. Since then, almost all the initially exponential quantum\nmachine learning speedups have been reduced to polynomial via new\nquantum-inspired classical algorithms. From the current state-of-affairs, it is\nunclear whether we can hope for exponential quantum speedups for any natural\nmachine learning task.\n  In this work, we present the first such provable exponential separation\nbetween quantum and quantum-inspired classical algorithms for the basic problem\nof solving a linear system when the input matrix is well-conditioned and has\nsparse rows and columns.\n","authors":["Allan Grønlund","Kasper Green Larsen"],"pdf_url":"https://arxiv.org/pdf/2411.02087v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21638v1","updated":"2025-03-27T16:03:46Z","published":"2025-03-27T16:03:46Z","title":"Data-Driven Extreme Response Estimation","summary":"  A method to rapidly estimate extreme ship response events is developed in\nthis paper. The method involves training by a Long Short-Term Memory (LSTM)\nneural network to correct a lower-fidelity hydrodynamic model to the level of a\nhigher-fidelity simulation. More focus is placed on larger responses by\nisolating the time-series near peak events identified in the lower-fidelity\nsimulations and training on only the shorter time-series around the large\nevent. The method is tested on the estimation of pitch time-series maxima in\nSea State 5 (significant wave height of 4.0 meters and modal period of 15.0\nseconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCode\nand a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP).\nThe results are also compared with an LSTM trained without special\nconsiderations for large events.\n","authors":["Samuel J. Edwards","Michael D. Levine"],"pdf_url":"https://arxiv.org/pdf/2503.21638v1.pdf","comment":"From the 35th Symposium on Naval Hydrodynamics"},{"id":"http://arxiv.org/abs/2503.21629v1","updated":"2025-03-27T15:50:32Z","published":"2025-03-27T15:50:32Z","title":"ClusterSC: Advancing Synthetic Control with Donor Selection","summary":"  In causal inference with observational studies, synthetic control (SC) has\nemerged as a prominent tool. SC has traditionally been applied to\naggregate-level datasets, but more recent work has extended its use to\nindividual-level data. As they contain a greater number of observed units, this\nshift introduces the curse of dimensionality to SC. To address this, we propose\nCluster Synthetic Control (ClusterSC), based on the idea that groups of\nindividuals may exist where behavior aligns internally but diverges between\ngroups. ClusterSC incorporates a clustering step to select only the relevant\ndonors for the target. We provide theoretical guarantees on the improvements\ninduced by ClusterSC, supported by empirical demonstrations on synthetic and\nreal-world datasets. The results indicate that ClusterSC consistently\noutperforms classical SC approaches.\n","authors":["Saeyoung Rho","Andrew Tang","Noah Bergam","Rachel Cummings","Vishal Misra"],"pdf_url":"https://arxiv.org/pdf/2503.21629v1.pdf","comment":"35 pages, 11 figures, to be published in Proceedings of The 28th\n  International Conference on Artificial Intelligence and Statistics (AIStats)\n  2025"},{"id":"http://arxiv.org/abs/2503.21627v1","updated":"2025-03-27T15:48:34Z","published":"2025-03-27T15:48:34Z","title":"Provable Reduction in Communication Rounds for Non-Smooth Convex\n  Federated Learning","summary":"  Multiple local steps are key to communication-efficient federated learning.\nHowever, theoretical guarantees for such algorithms, without data\nheterogeneity-bounding assumptions, have been lacking in general non-smooth\nconvex problems. Leveraging projection-efficient optimization methods, we\npropose FedMLS, a federated learning algorithm with provable improvements from\nmultiple local steps. FedMLS attains an $\\epsilon$-suboptimal solution in\n$\\mathcal{O}(1/\\epsilon)$ communication rounds, requiring a total of\n$\\mathcal{O}(1/\\epsilon^2)$ stochastic subgradient oracle calls.\n","authors":["Karlo Palenzuela","Ali Dadras","Alp Yurtsever","Tommy Löfstedt"],"pdf_url":"https://arxiv.org/pdf/2503.21627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00058v5","updated":"2025-03-27T15:47:29Z","published":"2023-01-31T19:48:01Z","title":"Graph Anomaly Detection in Time Series: A Survey","summary":"  With the recent advances in technology, a wide range of systems continue to\ncollect a large amount of data over time and thus generate time series.\nTime-Series Anomaly Detection (TSAD) is an important task in various\ntime-series applications such as e-commerce, cybersecurity, vehicle\nmaintenance, and healthcare monitoring. However, this task is very challenging\nas it requires considering both the intra-variable dependency (relationships\nwithin a variable over time) and the inter-variable dependency (relationships\nbetween multiple variables) existing in time-series data. Recent graph-based\napproaches have made impressive progress in tackling the challenges of this\nfield. In this survey, we conduct a comprehensive and up-to-date review of TSAD\nusing graphs, referred to as G-TSAD. First, we explore the significant\npotential of graph representation for time-series data and and its\ncontributions to facilitating anomaly detection. Then, we review\nstate-of-the-art graph anomaly detection techniques, mostly leveraging deep\nlearning architectures, in the context of time series. For each method, we\ndiscuss its strengths, limitations, and the specific applications where it\nexcels. Finally, we address both the technical and application challenges\ncurrently facing the field, and suggest potential future directions for\nadvancing research and improving practical outcomes.\n","authors":["Thi Kieu Khanh Ho","Ali Karami","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2302.00058v5.pdf","comment":"20 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2405.12802v2","updated":"2025-03-27T15:40:00Z","published":"2024-05-21T13:53:58Z","title":"Stochastic Inference of Plate Bending from Heterogeneous Data:\n  Physics-informed Gaussian Processes via Kirchhoff-Love Theory","summary":"  Advancements in machine learning and an abundance of structural monitoring\ndata have inspired the integration of mechanical models with probabilistic\nmodels to identify a structure's state and quantify the uncertainty of its\nphysical parameters and response. In this paper, we propose an inference\nmethodology for classical Kirchhoff-Love plates via physics-informed Gaussian\nProcesses (GP). A probabilistic model is formulated as a multi-output GP by\nplacing a GP prior on the deflection and deriving the covariance function using\nthe linear differential operators of the plate governing equations. The\nposteriors of the flexural rigidity, hyperparameters, and plate response are\ninferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling\nfrom noisy measurements. We demonstrate the applicability with two examples: a\nsimply supported plate subjected to a sinusoidal load and a fixed plate\nsubjected to a uniform load. The results illustrate how the proposed\nmethodology can be employed to perform stochastic inference for plate rigidity\nand physical quantities by integrating measurements from various sensor types\nand qualities. Potential applications of the presented methodology are in\nstructural health monitoring and uncertainty quantification of plate-like\nstructures.\n","authors":["Igor Kavrakov","Gledson Rodrigo Tondo","Guido Morgenthal"],"pdf_url":"https://arxiv.org/pdf/2405.12802v2.pdf","comment":"25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.21617v1","updated":"2025-03-27T15:37:23Z","published":"2025-03-27T15:37:23Z","title":"Leveraging Language Models for Analyzing Longitudinal Experiential Data\n  in Education","summary":"  We propose a novel approach to leveraging pre-trained language models (LMs)\nfor early forecasting of academic trajectories in STEM students using\nhigh-dimensional longitudinal experiential data. This data, which captures\nstudents' study-related activities, behaviors, and psychological states, offers\nvaluable insights for forecasting-based interventions. Key challenges in\nhandling such data include high rates of missing values, limited dataset size\ndue to costly data collection, and complex temporal variability across\nmodalities. Our approach addresses these issues through a comprehensive data\nenrichment process, integrating strategies for managing missing values,\naugmenting data, and embedding task-specific instructions and contextual cues\nto enhance the models' capacity for learning temporal patterns. Through\nextensive experiments on a curated student learning dataset, we evaluate both\nencoder-decoder and decoder-only LMs. While our findings show that LMs\neffectively integrate data across modalities and exhibit resilience to missing\ndata, they primarily rely on high-level statistical patterns rather than\ndemonstrating a deeper understanding of temporal dynamics. Furthermore, their\nability to interpret explicit temporal information remains limited. This work\nadvances educational data science by highlighting both the potential and\nlimitations of LMs in modeling student trajectories for early intervention\nbased on longitudinal experiential data.\n","authors":["Ahatsham Hayat","Bilal Khan","Mohammad Rashedul Hasan"],"pdf_url":"https://arxiv.org/pdf/2503.21617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03384v2","updated":"2025-03-27T15:32:05Z","published":"2025-03-05T11:02:29Z","title":"GNNMerge: Merging of GNN Models Without Accessing Training Data","summary":"  Model merging has gained prominence in machine learning as a method to\nintegrate multiple trained models into a single model without accessing the\noriginal training data. While existing approaches have demonstrated success in\ndomains such as computer vision and NLP, their application to Graph Neural\nNetworks (GNNs) remains unexplored. These methods often rely on the assumption\nof shared initialization, which is seldom applicable to GNNs. In this work, we\nundertake the first benchmarking study of model merging algorithms for GNNs,\nrevealing their limited effectiveness in this context. To address these\nchallenges, we propose GNNMerge, which utilizes a task-agnostic node embedding\nalignment strategy to merge GNNs. Furthermore, we establish that under a mild\nrelaxation, the proposed optimization objective admits direct analytical\nsolutions for widely used GNN architectures, significantly enhancing its\ncomputational efficiency. Empirical evaluations across diverse datasets, tasks,\nand architectures establish GNNMerge to be up to 24% more accurate than\nexisting methods while delivering over 2 orders of magnitude speed-up compared\nto training from scratch.\n","authors":["Vipul Garg","Ishita Thakre","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2503.03384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21608v1","updated":"2025-03-27T15:28:06Z","published":"2025-03-27T15:28:06Z","title":"Nonlinear Multiple Response Regression and Learning of Latent Spaces","summary":"  Identifying low-dimensional latent structures within high-dimensional data\nhas long been a central topic in the machine learning community, driven by the\nneed for data compression, storage, transmission, and deeper data\nunderstanding. Traditional methods, such as principal component analysis (PCA)\nand autoencoders (AE), operate in an unsupervised manner, ignoring label\ninformation even when it is available. In this work, we introduce a unified\nmethod capable of learning latent spaces in both unsupervised and supervised\nsettings. We formulate the problem as a nonlinear multiple-response regression\nwithin an index model context. By applying the generalized Stein's lemma, the\nlatent space can be estimated without knowing the nonlinear link functions. Our\nmethod can be viewed as a nonlinear generalization of PCA. Moreover, unlike AE\nand other neural network methods that operate as \"black boxes\", our approach\nnot only offers better interpretability but also reduces computational\ncomplexity while providing strong theoretical guarantees. Comprehensive\nnumerical experiments and real data analyses demonstrate the superior\nperformance of our method.\n","authors":["Ye Tian","Sanyou Wu","Long Feng"],"pdf_url":"https://arxiv.org/pdf/2503.21608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16400v2","updated":"2025-03-27T15:12:43Z","published":"2025-03-20T17:54:37Z","title":"ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos","summary":"  Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.\n","authors":["Haolin Yang","Feilong Tang","Ming Hu","Yulong Li","Yexin Liu","Zelin Peng","Junjun He","Zongyuan Ge","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.16400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21588v1","updated":"2025-03-27T15:04:52Z","published":"2025-03-27T15:04:52Z","title":"Generalizable Implicit Neural Representations via Parameterized Latent\n  Dynamics for Baroclinic Ocean Forecasting","summary":"  Mesoscale ocean dynamics play a critical role in climate systems, governing\nheat transport, hurricane genesis, and drought patterns. However, simulating\nthese processes at high resolution remains computationally prohibitive due to\ntheir nonlinear, multiscale nature and vast spatiotemporal domains. Implicit\nneural representations (INRs) reduce the computational costs as\nresolution-independent surrogates but fail in many-query scenarios (inverse\nmodeling) requiring rapid evaluations across diverse parameters. We present\nPINROD, a novel framework combining dynamics-aware implicit neural\nrepresentations with parameterized neural ordinary differential equations to\naddress these limitations. By integrating parametric dependencies into latent\ndynamics, our method efficiently captures nonlinear oceanic behavior across\nvarying boundary conditions and physical parameters. Experiments on ocean\nmesoscale activity data show superior accuracy over existing baselines and\nimproved computational efficiency compared to standard numerical simulations.\n","authors":["Guang Zhao","Xihaier Luo","Seungjun Lee","Yihui Ren","Shinjae Yoo","Luke Van Roekel","Balu Nadiga","Sri Hari Krishna Narayanan","Yixuan Sun","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2503.21588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21585v1","updated":"2025-03-27T15:01:37Z","published":"2025-03-27T15:01:37Z","title":"Probabilistic Functional Neural Networks","summary":"  High-dimensional functional time series (HDFTS) are often characterized by\nnonlinear trends and high spatial dimensions. Such data poses unique challenges\nfor modeling and forecasting due to the nonlinearity, nonstationarity, and high\ndimensionality. We propose a novel probabilistic functional neural network\n(ProFnet) to address these challenges. ProFnet integrates the strengths of\nfeedforward and deep neural networks with probabilistic modeling. The model\ngenerates probabilistic forecasts using Monte Carlo sampling and also enables\nthe quantification of uncertainty in predictions. While capturing both temporal\nand spatial dependencies across multiple regions, ProFnet offers a scalable and\nunified solution for large datasets. Applications to Japan's mortality rates\ndemonstrate superior performance. This approach enhances predictive accuracy\nand provides interpretable uncertainty estimates, making it a valuable tool for\nforecasting complex high-dimensional functional data and HDFTS.\n","authors":["Haixu Wang","Jiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2503.21585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21579v1","updated":"2025-03-27T14:59:54Z","published":"2025-03-27T14:59:54Z","title":"Fusion of Graph Neural Networks via Optimal Transport","summary":"  In this paper, we explore the idea of combining GCNs into one model. To that\nend, we align the weights of different models layer-wise using optimal\ntransport (OT). We present and evaluate three types of transportation costs and\nshow that the studied fusion method consistently outperforms the performance of\nvanilla averaging. Finally, we present results suggesting that model fusion\nusing OT is harder in the case of GCNs than MLPs and that incorporating the\ngraph structure into the process does not improve the performance of the\nmethod.\n","authors":["Weronika Ormaniec","Michael Vollenweider","Elisa Hoskovec"],"pdf_url":"https://arxiv.org/pdf/2503.21579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21563v1","updated":"2025-03-27T14:47:27Z","published":"2025-03-27T14:47:27Z","title":"Consistent Multigroup Low-Rank Approximation","summary":"  We consider the problem of consistent low-rank approximation for multigroup\ndata: we ask for a sequence of $k$ basis vectors such that projecting the data\nonto their spanned subspace treats all groups as equally as possible, by\nminimizing the maximum error among the groups. Additionally, we require that\nthe sequence of basis vectors satisfies the natural consistency property: when\nlooking for the best $k$ vectors, the first $d<k$ vectors are the best possible\nsolution to the problem of finding $d$ basis vectors. Thus, this multigroup\nlow-rank approximation method naturally generalizes \\svd and reduces to \\svd\nfor data with a single group. We give an iterative algorithm for this task that\nsequentially adds to the basis the vector that gives the best rank$-1$\nprojection according to the min-max criterion, and then projects the data onto\nthe orthogonal complement of that vector. For finding the best rank$-1$\nprojection, we use primal-dual approaches or semidefinite programming. We\nanalyze the theoretical properties of the algorithms and demonstrate\nempirically that the proposed methods compare favorably to existing methods for\nmultigroup (or fair) PCA.\n","authors":["Antonis Matakos","Martino Ciaperoni","Heikki Mannila"],"pdf_url":"https://arxiv.org/pdf/2503.21563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21555v1","updated":"2025-03-27T14:40:53Z","published":"2025-03-27T14:40:53Z","title":"SyncSDE: A Probabilistic Framework for Diffusion Synchronization","summary":"  There have been many attempts to leverage multiple diffusion models for\ncollaborative generation, extending beyond the original domain. A prominent\napproach involves synchronizing multiple diffusion trajectories by mixing the\nestimated scores to artificially correlate the generation processes. However,\nexisting methods rely on naive heuristics, such as averaging, without\nconsidering task specificity. These approaches do not clarify why such methods\nwork and often fail when a heuristic suitable for one task is blindly applied\nto others. In this paper, we present a probabilistic framework for analyzing\nwhy diffusion synchronization works and reveal where heuristics should be\nfocused - modeling correlations between multiple trajectories and adapting them\nto each specific task. We further identify optimal correlation models per task,\nachieving better results than previous approaches that apply a single heuristic\nacross all tasks without justification.\n","authors":["Hyunjun Lee","Hyunsoo Lee","Sookwan Han"],"pdf_url":"https://arxiv.org/pdf/2503.21555v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.21538v1","updated":"2025-03-27T14:29:31Z","published":"2025-03-27T14:29:31Z","title":"Formation Shape Control using the Gromov-Wasserstein Metric","summary":"  This article introduces a formation shape control algorithm, in the optimal\ncontrol framework, for steering an initial population of agents to a desired\nconfiguration via employing the Gromov-Wasserstein distance. The underlying\ndynamical system is assumed to be a constrained linear system and the objective\nfunction is a sum of quadratic control-dependent stage cost and a\nGromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost\ntransforms the resulting optimal control problem into a well-known NP-hard\nproblem, making it both numerically demanding and difficult to solve with high\naccuracy. Towards that end, we employ a recent semi-definite relaxation-driven\ntechnique to tackle the Gromov-Wasserstein distance. A numerical example is\nprovided to illustrate our results.\n","authors":["Haruto Nakashima","Siddhartha Ganguly","Kohei Morimoto","Kenji Kashima"],"pdf_url":"https://arxiv.org/pdf/2503.21538v1.pdf","comment":"To appear in the proceedings of Learning for Dynamics and Control\n  (L4DC) conference, PMLR, 2025"},{"id":"http://arxiv.org/abs/2503.21536v1","updated":"2025-03-27T14:28:37Z","published":"2025-03-27T14:28:37Z","title":"Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into\n  Bosons, Hierarchical Learning and Symmetry Breaking","summary":"  Deep generative models have become ubiquitous due to their ability to learn\nand sample from complex distributions. Despite the proliferation of various\nframeworks, the relationships among these models remain largely unexplored, a\ngap that hinders the development of a unified theory of AI learning. We address\ntwo central challenges: clarifying the connections between different deep\ngenerative models and deepening our understanding of their learning mechanisms.\nWe focus on Restricted Boltzmann Machines (RBMs), known for their universal\napproximation capabilities for discrete distributions. By introducing a\nreciprocal space formulation, we reveal a connection between RBMs, diffusion\nprocesses, and coupled Bosons. We show that at initialization, the RBM operates\nat a saddle point, where the local curvature is determined by the singular\nvalues, whose distribution follows the Marcenko-Pastur law and exhibits\nrotational symmetry. During training, this rotational symmetry is broken due to\nhierarchical learning, where different degrees of freedom progressively capture\nfeatures at multiple levels of abstraction. This leads to a symmetry breaking\nin the energy landscape, reminiscent of Landau theory. This symmetry breaking\nin the energy landscape is characterized by the singular values and the weight\nmatrix eigenvector matrix. We derive the corresponding free energy in a\nmean-field approximation. We show that in the limit of infinite size RBM, the\nreciprocal variables are Gaussian distributed. Our findings indicate that in\nthis regime, there will be some modes for which the diffusion process will not\nconverge to the Boltzmann distribution. To illustrate our results, we trained\nreplicas of RBMs with different hidden layer sizes using the MNIST dataset. Our\nfindings bridge the gap between disparate generative frameworks and also shed\nlight on the processes underpinning learning in generative models.\n","authors":["J. Quetzalcóatl Toledo-Marin","Anindita Maiti","Geoffrey C. Fox","Roger G. Melko"],"pdf_url":"https://arxiv.org/pdf/2503.21536v1.pdf","comment":"19pp, 8figs, research article"},{"id":"http://arxiv.org/abs/2503.20711v2","updated":"2025-03-27T14:28:31Z","published":"2025-03-26T16:47:14Z","title":"Demand Estimation with Text and Image Data","summary":"  We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon and consistently find that text and image data help\nidentify close substitutes within each category.\n","authors":["Giovanni Compiani","Ilya Morozov","Stephan Seiler"],"pdf_url":"https://arxiv.org/pdf/2503.20711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21528v1","updated":"2025-03-27T14:17:05Z","published":"2025-03-27T14:17:05Z","title":"Bayesian Pseudo Posterior Mechanism for Differentially Private Machine\n  Learning","summary":"  Differential privacy (DP) is becoming increasingly important for deployed\nmachine learning applications because it provides strong guarantees for\nprotecting the privacy of individuals whose data is used to train models.\nHowever, DP mechanisms commonly used in machine learning tend to struggle on\nmany real world distributions, including highly imbalanced or small labeled\ntraining sets. In this work, we propose a new scalable DP mechanism for deep\nlearning models, SWAG-PPM, by using a pseudo posterior distribution that\ndownweights by-record likelihood contributions proportionally to their\ndisclosure risks as the randomized mechanism. As a motivating example from\nofficial statistics, we demonstrate SWAG-PPM on a workplace injury text\nclassification task using a highly imbalanced public dataset published by the\nU.S. Occupational Safety and Health Administration (OSHA). We find that\nSWAG-PPM exhibits only modest utility degradation against a non-private\ncomparator while greatly outperforming the industry standard DP-SGD for a\nsimilar privacy budget.\n","authors":["Robert Chew","Matthew R. Williams","Elan A. Segarra","Alexander J. Preiss","Amanda Konet","Terrance D. Savitsky"],"pdf_url":"https://arxiv.org/pdf/2503.21528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21526v1","updated":"2025-03-27T14:14:21Z","published":"2025-03-27T14:14:21Z","title":"Constraint-based causal discovery with tiered background knowledge and\n  latent variables in single or overlapping datasets","summary":"  In this paper we consider the use of tiered background knowledge within\nconstraint based causal discovery. Our focus is on settings relaxing causal\nsufficiency, i.e. allowing for latent variables which may arise because\nrelevant information could not be measured at all, or not jointly, as in the\ncase of multiple overlapping datasets. We first present novel insights into the\nproperties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce\na new extension of the IOD (integrating overlapping datasets) algorithm\nincorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm.\nWe show that under full usage of the tiered background knowledge tFCI and tIOD\nare sound, while simple versions of the tIOD and tFCI are sound and complete.\nWe further show that the tIOD algorithm can often be expected to be\nconsiderably more efficient and informative than the IOD algorithm even beyond\nthe obvious restriction of the Markov equivalence classes. We provide a formal\nresult on the conditions for this gain in efficiency and informativeness. Our\nresults are accompanied by a series of examples illustrating the exact role and\nusefulness of tiered background knowledge.\n","authors":["Christine W. Bang","Vanessa Didelez"],"pdf_url":"https://arxiv.org/pdf/2503.21526v1.pdf","comment":"Accepted for the 4th Conference on Causal Learning and Reasoning\n  (CLeaR 2025)"},{"id":"http://arxiv.org/abs/2503.21510v1","updated":"2025-03-27T13:59:19Z","published":"2025-03-27T13:59:19Z","title":"Uncertainty-aware Bayesian machine learning modelling of land cover\n  classification","summary":"  Land cover classification involves the production of land cover maps, which\ndetermine the type of land through remote sensing imagery. Over recent years,\nsuch classification is being performed by machine learning classification\nmodels, which can give highly accurate predictions on land cover per pixel\nusing large quantities of input training data. However, such models do not\ncurrently take account of input measurement uncertainty, which is vital for\ntraceability in metrology. In this work we propose a Bayesian classification\nframework using generative modelling to take account of input measurement\nuncertainty. We take the specific case of Bayesian quadratic discriminant\nanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in\n2020 and 2021. We benchmark the performance of the model against more popular\nclassification models used in land cover maps such as random forests and neural\nnetworks. We find that such Bayesian models are more trustworthy, in the sense\nthat they are more interpretable, explicitly model the input measurement\nuncertainty, and maintain predictive performance of class probability outputs\nacross datasets of different years and sizes, whilst also being computationally\nefficient.\n","authors":["Samuel Bilson","Anna Pustogvar"],"pdf_url":"https://arxiv.org/pdf/2503.21510v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.21507v1","updated":"2025-03-27T13:51:31Z","published":"2025-03-27T13:51:31Z","title":"F-INR: Functional Tensor Decomposition for Implicit Neural\n  Representations","summary":"  Implicit Neural Representation (INR) has emerged as a powerful tool for\nencoding discrete signals into continuous, differentiable functions using\nneural networks. However, these models often have an unfortunate reliance on\nmonolithic architectures to represent high-dimensional data, leading to\nprohibitive computational costs as dimensionality grows. We propose F-INR, a\nframework that reformulates INR learning through functional tensor\ndecomposition, breaking down high-dimensional tasks into lightweight,\naxis-specific sub-networks. Each sub-network learns a low-dimensional data\ncomponent (e.g., spatial or temporal). Then, we combine these components via\ntensor operations, reducing forward pass complexity while improving accuracy\nthrough specialized learning. F-INR is modular and, therefore,\narchitecture-agnostic, compatible with MLPs, SIREN, WIRE, or other\nstate-of-the-art INR architecture. It is also decomposition-agnostic,\nsupporting CP, TT, and Tucker modes with user-defined rank for speed-accuracy\ncontrol. In our experiments, F-INR trains $100\\times$ faster than existing\napproaches on video tasks while achieving higher fidelity (+3.4 dB PSNR).\nSimilar gains hold for image compression, physics simulations, and 3D geometry\nreconstruction. Through this, F-INR offers a new scalable, flexible solution\nfor high-dimensional signal modeling.\n","authors":["Sai Karthikeya Vemuri","Tim Büchner","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2503.21507v1.pdf","comment":"26 pages, 33 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.16218v4","updated":"2025-03-27T13:44:56Z","published":"2024-12-18T09:20:12Z","title":"GNN-Transformer Cooperative Architecture for Trustworthy Graph\n  Contrastive Learning","summary":"  Graph contrastive learning (GCL) has become a hot topic in the field of graph\nrepresentation learning. In contrast to traditional supervised learning relying\non a large number of labels, GCL exploits augmentation strategies to generate\nmultiple views and positive/negative pairs, both of which greatly influence the\nperformance. Unfortunately, commonly used random augmentations may disturb the\nunderlying semantics of graphs. Moreover, traditional GNNs, a type of widely\nemployed encoders in GCL, are inevitably confronted with over-smoothing and\nover-squashing problems. To address these issues, we propose GNN-Transformer\nCooperative Architecture for Trustworthy Graph Contrastive Learning (GTCA),\nwhich inherits the advantages of both GNN and Transformer, incorporating graph\ntopology to obtain comprehensive graph representations. Theoretical analysis\nverifies the trustworthiness of the proposed method. Extensive experiments on\nbenchmark datasets demonstrate state-of-the-art empirical performance.\n","authors":["Jianqing Liang","Xinkai Wei","Min Chen","Zhiqiang Wang","Jiye Liang"],"pdf_url":"https://arxiv.org/pdf/2412.16218v4.pdf","comment":"In Proceedings of AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07428v2","updated":"2025-03-27T13:35:16Z","published":"2024-12-10T11:39:27Z","title":"Latency Minimization for UAV-Enabled Federated Learning: Trajectory\n  Design and Resource Allocation","summary":"  Federated learning (FL) has become a transformative paradigm for distributed\nmachine learning across wireless networks. However, the performance of FL is\noften hindered by the unreliable communication links between\nresource-constrained Internet of Things (IoT) devices and the central server.\nTo overcome this challenge, we propose a novel framework that employs an\nunmanned aerial vehicle (UAV) as a mobile server to enhance the FL training\nprocess. By capitalizing on the UAV's mobility, we establish strong\nline-of-sight connections with IoT devices, thereby enhancing communication\nreliability and capacity. To maximize training efficiency, we formulate a\nlatency minimization problem that jointly optimizes bandwidth allocation,\ncomputing frequencies, transmit power for both the UAV and IoT devices, and the\nUAV's flight trajectory. Subsequently, we analyze the required rounds of the\nIoT devices training and the UAV aggregation for FL convergence. Based on the\nconvergence constraint, we transform the problem into three subproblems and\ndevelop an efficient alternating optimization algorithm to solve this problem\neffectively. Additionally, we provide a thorough analysis of the algorithm's\nconvergence and computational complexity. Extensive numerical results\ndemonstrate that our proposed scheme not only surpasses existing benchmark\nschemes in reducing latency up to 15.29%, but also achieves training efficiency\nthat nearly matches the ideal scenario.\n","authors":["Xuhui Zhang","Wenchao Liu","Jinke Ren","Huijun Xing","Gui Gui","Yanyan Shen","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2412.07428v2.pdf","comment":"This manuscript has been submitted to IEEE"},{"id":"http://arxiv.org/abs/2408.16315v2","updated":"2025-03-27T13:34:40Z","published":"2024-08-29T07:32:30Z","title":"Passenger hazard perception based on EEG signals for highly automated\n  driving vehicles","summary":"  Enhancing the safety of autonomous vehicles is crucial, especially given\nrecent accidents involving automated systems. As passengers in these vehicles,\nhumans' sensory perception and decision-making can be integrated with\nautonomous systems to improve safety. This study explores neural mechanisms in\npassenger-vehicle interactions, leading to the development of a Passenger\nCognitive Model (PCM) and the Passenger EEG Decoding Strategy (PEDS). Central\nto PEDS is a novel Convolutional Recurrent Neural Network (CRNN) that captures\nspatial and temporal EEG data patterns. The CRNN, combined with stacking\nalgorithms, achieves an accuracy of $85.0\\% \\pm 3.18\\%$. Our findings highlight\nthe predictive power of pre-event EEG data, enhancing the detection of\nhazardous scenarios and offering a network-driven framework for safer\nautonomous vehicles.\n","authors":["Ashton Yu Xuan Tan","Yingkai Yang","Xiaofei Zhang","Bowen Li","Xiaorong Gao","Sifa Zheng","Jianqiang Wang","Xinyu Gu","Jun Li","Yang Zhao","Yuxin Zhang","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2408.16315v2.pdf","comment":"We have decided to withdraw this submission due to ongoing revisions\n  and further refinements in our research. A revised version may be resubmitted\n  in the future. We appreciate the feedback and interest from the community"},{"id":"http://arxiv.org/abs/2412.02340v2","updated":"2025-03-27T13:25:14Z","published":"2024-12-03T10:03:12Z","title":"PAPAYA Federated Analytics Stack: Engineering Privacy, Scalability and\n  Practicality","summary":"  Cross-device Federated Analytics (FA) is a distributed computation paradigm\ndesigned to answer analytics queries about and derive insights from data held\nlocally on users' devices. On-device computations combined with other privacy\nand security measures ensure that only minimal data is transmitted off-device,\nachieving a high standard of data protection. Despite FA's broad relevance, the\napplicability of existing FA systems is limited by compromised accuracy; lack\nof flexibility for data analytics; and an inability to scale effectively. In\nthis paper, we describe our approach to combine privacy, scalability, and\npracticality to build and deploy a system that overcomes these limitations. Our\nFA system leverages trusted execution environments (TEEs) and optimizes the use\nof on-device computing resources to facilitate federated data processing across\nlarge fleets of devices, while ensuring robust, defensible, and verifiable\nprivacy safeguards. We focus on federated analytics (statistics and\nmonitoring), in contrast to systems for federated learning (ML workloads), and\nwe flag the key differences.\n","authors":["Harish Srinivas","Graham Cormode","Mehrdad Honarkhah","Samuel Lurye","Jonathan Hehir","Lunwen He","George Hong","Ahmed Magdy","Dzmitry Huba","Kaikai Wang","Shen Guo","Shoubhik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2412.02340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12902v2","updated":"2025-03-27T13:19:41Z","published":"2025-02-18T14:42:11Z","title":"Probabilistic neural operators for functional uncertainty quantification","summary":"  Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.\n","authors":["Christopher Bülte","Philipp Scholl","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2502.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21476v1","updated":"2025-03-27T13:06:26Z","published":"2025-03-27T13:06:26Z","title":"Robust DNN Partitioning and Resource Allocation Under Uncertain\n  Inference Time","summary":"  In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices.\n","authors":["Zhaojun Nan","Yunchu Han","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2503.21476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21473v1","updated":"2025-03-27T13:04:41Z","published":"2025-03-27T13:04:41Z","title":"DeepRV: pre-trained spatial priors for accelerated disease mapping","summary":"  Recently introduced prior-encoding deep generative models (e.g., PriorVAE,\n$\\pi$VAE, and PriorCVAE) have emerged as powerful tools for scalable Bayesian\ninference by emulating complex stochastic processes like Gaussian processes\n(GPs). However, these methods remain largely a proof-of-concept and\ninaccessible to practitioners. We propose DeepRV, a lightweight, decoder-only\napproach that accelerates training, and enhances real-world applicability in\ncomparison to current VAE-based prior encoding approaches. Leveraging\nprobabilistic programming frameworks (e.g., NumPyro) for inference, DeepRV\nachieves significant speedups while also improving the quality of parameter\ninference, closely matching full MCMC sampling. We showcase its effectiveness\nin process emulation and spatial analysis of the UK using simulated data,\ngender-wise cancer mortality rates for individuals under 50, and HIV prevalence\nin Zimbabwe. To bridge the gap between theory and practice, we provide a\nuser-friendly API, enabling scalable and efficient Bayesian inference.\n","authors":["Jhonathan Navott","Daniel Jenson","Seth Flaxman","Elizaveta Semenova"],"pdf_url":"https://arxiv.org/pdf/2503.21473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21458v1","updated":"2025-03-27T12:46:12Z","published":"2025-03-27T12:46:12Z","title":"DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker\n  Availability Windows","summary":"  With the rapid advancement of mobile networks and the widespread use of\nmobile devices, spatial crowdsourcing, which involves assigning location-based\ntasks to mobile workers, has gained significant attention. However, most\nexisting research focuses on task assignment at the current moment, overlooking\nthe fluctuating demand and supply between tasks and workers over time. To\naddress this issue, we introduce an adaptive task assignment problem, which\naims to maximize the number of assigned tasks by dynamically adjusting task\nassignments in response to changing demand and supply. We develop a spatial\ncrowdsourcing framework, namely demand-based adaptive task assignment with\ndynamic worker availability windows, which consists of two components including\ntask demand prediction and task assignment. In the first component, we\nconstruct a graph adjacency matrix representing the demand dependency\nrelationships in different regions and employ a multivariate time series\nlearning approach to predict future task demands. In the task assignment\ncomponent, we adjust tasks to workers based on these predictions, worker\navailability windows, and the current task assignments, where each worker has\nan availability window that indicates the time periods they are available for\ntask assignments. To reduce the search space of task assignments and be\nefficient, we propose a worker dependency separation approach based on graph\npartition and a task value function with reinforcement learning. Experiments on\nreal data demonstrate that our proposals are both effective and efficient.\n","authors":["Jinwen Chen","Jiannan Guo","Dazhuo Qiu","Yawen Li","Guanhua Ye","Yan Zhao","Kai Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.21458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15474v2","updated":"2025-03-27T12:41:08Z","published":"2024-05-24T11:53:13Z","title":"Unlearning during Learning: An Efficient Federated Machine Unlearning\n  Method","summary":"  In recent years, Federated Learning (FL) has garnered significant attention\nas a distributed machine learning paradigm. To facilitate the implementation of\nthe right to be forgotten, the concept of federated machine unlearning (FMU)\nhas also emerged. However, current FMU approaches often involve additional\ntime-consuming steps and may not offer comprehensive unlearning capabilities,\nwhich renders them less practical in real FL scenarios. In this paper, we\nintroduce FedAU, an innovative and efficient FMU framework aimed at overcoming\nthese limitations. Specifically, FedAU incorporates a lightweight auxiliary\nunlearning module into the learning process and employs a straightforward\nlinear operation to facilitate unlearning. This approach eliminates the\nrequirement for extra time-consuming steps, rendering it well-suited for FL.\nFurthermore, FedAU exhibits remarkable versatility. It not only enables\nmultiple clients to carry out unlearning tasks concurrently but also supports\nunlearning at various levels of granularity, including individual data samples,\nspecific classes, and even at the client level. We conducted extensive\nexperiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the\nperformance of FedAU. The results demonstrate that FedAU effectively achieves\nthe desired unlearning effect while maintaining model accuracy. Our code is\navailiable at https://github.com/Liar-Mask/FedAU.\n","authors":["Hanlin Gu","Gongxi Zhu","Jie Zhang","Xinyuan Zhao","Yuxing Han","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.15474v2.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2402.06289v3","updated":"2025-03-27T12:38:46Z","published":"2024-02-09T09:58:35Z","title":"FedMIA: An Effective Membership Inference Attack Exploiting \"All for\n  One\" Principle in Federated Learning","summary":"  Federated Learning (FL) is a promising approach for training machine learning\nmodels on decentralized data while preserving privacy. However, privacy risks,\nparticularly Membership Inference Attacks (MIAs), which aim to determine\nwhether a specific data point belongs to a target client's training set, remain\na significant concern. Existing methods for implementing MIAs in FL primarily\nanalyze updates from the target client, focusing on metrics such as loss,\ngradient norm, and gradient difference. However, these methods fail to leverage\nupdates from non-target clients, potentially underutilizing available\ninformation. In this paper, we first formulate a one-tailed likelihood-ratio\nhypothesis test based on the likelihood of updates from non-target clients.\nBuilding upon this formulation, we introduce a three-step Membership Inference\nAttack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging\nupdates from all clients across multiple communication rounds to enhance MIA\neffectiveness. Both theoretical analysis and extensive experimental results\ndemonstrate that FedMIA outperforms existing MIAs in both classification and\ngenerative tasks. Additionally, it can be integrated as an extension to\nexisting methods and is robust against various defense strategies, Non-IID\ndata, and different federated structures. Our code is available in\nhttps://github.com/Liar-Mask/FedMIA.\n","authors":["Gongxi Zhu","Donghao Li","Hanlin Gu","Yuan Yao","Lixin Fan","Yuxing Han"],"pdf_url":"https://arxiv.org/pdf/2402.06289v3.pdf","comment":"14 pages, 6 figures; Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2412.08453v2","updated":"2025-03-27T12:29:38Z","published":"2024-12-11T15:16:16Z","title":"On best approximation by multivariate ridge functions with applications\n  to generalized translation networks","summary":"  We prove sharp upper and lower bounds for the approximation of Sobolev\nfunctions by sums of multivariate ridge functions, i.e., functions of the form\n$\\mathbb{R}^d \\ni x \\mapsto \\sum_{k=1}^n h_k(A_k x) \\in \\mathbb{R}$ with $h_k :\n\\mathbb{R}^\\ell \\to \\mathbb{R}$ and $A_k \\in \\mathbb{R}^{\\ell \\times d}$. We\nshow that the order of approximation asymptotically behaves as\n$n^{-r/(d-\\ell)}$, where $r$ is the regularity of the Sobolev functions to be\napproximated. Our lower bound even holds when approximating $L^\\infty$-Sobolev\nfunctions of regularity $r$ with error measured in $L^1$, while our upper bound\napplies to the approximation of $L^p$-Sobolev functions in $L^p$ for any $1\n\\leq p \\leq \\infty$. These bounds generalize well-known results about the\napproximation properties of univariate ridge functions to the multivariate\ncase. Moreover, we use these bounds to obtain sharp asymptotic bounds for the\napproximation of Sobolev functions using generalized translation networks and\ncomplex-valued neural networks.\n","authors":["Paul Geuchen","Palina Salanevich","Olov Schavemaker","Felix Voigtlaender"],"pdf_url":"https://arxiv.org/pdf/2412.08453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21436v1","updated":"2025-03-27T12:21:00Z","published":"2025-03-27T12:21:00Z","title":"Stochastic Engrams for Efficient Continual Learning with Binarized\n  Neural Networks","summary":"  The ability to learn continuously in artificial neural networks (ANNs) is\noften limited by catastrophic forgetting, a phenomenon in which new knowledge\nbecomes dominant. By taking mechanisms of memory encoding in neuroscience (aka.\nengrams) as inspiration, we propose a novel approach that integrates\nstochastically-activated engrams as a gating mechanism for metaplastic\nbinarized neural networks (mBNNs). This method leverages the computational\nefficiency of mBNNs combined with the robustness of probabilistic memory traces\nto mitigate forgetting and maintain the model's reliability. Previously\nvalidated metaplastic optimization techniques have been incorporated to enhance\nsynaptic stability further. Compared to baseline binarized models and benchmark\nfully connected continual learning approaches, our method is the only strategy\ncapable of reaching average accuracies over 20% in class-incremental scenarios\nand achieving comparable domain-incremental results to full precision\nstate-of-the-art methods. Furthermore, we achieve a significant reduction in\npeak GPU and RAM usage, under 5% and 20%, respectively. Our findings\ndemonstrate (A) an improved stability vs. plasticity trade-off, (B) a reduced\nmemory intensiveness, and (C) an enhanced performance in binarized\narchitectures. By uniting principles of neuroscience and efficient computing,\nwe offer new insights into the design of scalable and robust deep learning\nsystems.\n","authors":["Isabelle Aguilar","Luis Fernando Herbozo Contreras","Omid Kavehei"],"pdf_url":"https://arxiv.org/pdf/2503.21436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21432v1","updated":"2025-03-27T12:17:00Z","published":"2025-03-27T12:17:00Z","title":"Exploring the flavor structure of leptons via diffusion models","summary":"  We propose a method to explore the flavor structure of leptons using\ndiffusion models, which are known as one of generative artificial intelligence\n(generative AI). We consider a simple extension of the Standard Model with the\ntype I seesaw mechanism and train a neural network to generate the neutrino\nmass matrix. By utilizing transfer learning, the diffusion model generates 104\nsolutions that are consistent with the neutrino mass squared differences and\nthe leptonic mixing angles. The distributions of the CP phases and the sums of\nneutrino masses, which are not included in the conditional labels but are\ncalculated from the solutions, exhibit non-trivial tendencies. In addition, the\neffective mass in neutrinoless double beta decay is concentrated near the\nboundaries of the existing confidence intervals, allowing us to verify the\nobtained solutions through future experiments. An inverse approach using the\ndiffusion model is expected to facilitate the experimental verification of\nflavor models from a perspective distinct from conventional analytical methods.\n","authors":["Satsuki Nishimura","Hajime Otsuka","Haruki Uchiyama"],"pdf_url":"https://arxiv.org/pdf/2503.21432v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.21431v1","updated":"2025-03-27T12:16:54Z","published":"2025-03-27T12:16:54Z","title":"Nearest Neighbour Equilibrium Clustering","summary":"  A novel and intuitive nearest neighbours based clustering algorithm is\nintroduced, in which a cluster is defined in terms of an equilibrium condition\nwhich balances its size and cohesiveness. The formulation of the equilibrium\ncondition allows for a quantification of the strength of alignment of each\npoint to a cluster, with these cluster alignment strengths leading naturally to\na model selection criterion which renders the proposed approach fully\nautomatable. The algorithm is simple to implement and computationally\nefficient, and produces clustering solutions of extremely high quality in\ncomparison with relevant benchmarks from the literature. R code to implement\nthe approach is available from https://github.com/DavidHofmeyr/NNEC.\n","authors":["David P. Hofmeyr"],"pdf_url":"https://arxiv.org/pdf/2503.21431v1.pdf","comment":"Currently being considered for publication by IEEE"},{"id":"http://arxiv.org/abs/2503.21426v1","updated":"2025-03-27T12:13:28Z","published":"2025-03-27T12:13:28Z","title":"AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram\n  Model","summary":"  The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks.\n","authors":["Sen Zhang","Qingqing Ye","Haibo Hu","Jianliang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.21426v1.pdf","comment":"Accepted by ICDE 2025"},{"id":"http://arxiv.org/abs/2409.02482v2","updated":"2025-03-27T12:12:46Z","published":"2024-09-04T07:18:26Z","title":"Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes","summary":"  High-quality view synthesis relies on volume rendering, splatting, or surface\nrendering. While surface rendering is typically the fastest, it struggles to\naccurately model fuzzy geometry like hair. In turn, alpha-blending techniques\nexcel at representing fuzzy materials but require an unbounded number of\nsamples per ray (P1). Further overheads are induced by empty space skipping in\nvolume rendering (P2) and sorting input primitives in splatting (P3). We\npresent a novel representation for real-time view synthesis where the (P1)\nnumber of sampling locations is small and bounded, (P2) sampling locations are\nefficiently found via rasterization, and (P3) rendering is sorting-free. We\nachieve this by representing objects as semi-transparent multi-layer meshes\nrendered in a fixed order. First, we model surface layers as signed distance\nfunction (SDF) shells with optimal spacing learned during training. Then, we\nbake them as meshes and fit UV textures. Unlike single-surface methods, our\nmulti-layer representation effectively models fuzzy objects. In contrast to\nvolume and splatting-based methods, our approach enables real-time rendering on\nlow-power laptops and smartphones.\n","authors":["Stefano Esposito","Anpei Chen","Christian Reiser","Samuel Rota Bulò","Lorenzo Porzi","Katja Schwarz","Christian Richardt","Michael Zollhöfer","Peter Kontschieder","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2409.02482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21422v1","updated":"2025-03-27T12:10:15Z","published":"2025-03-27T12:10:15Z","title":"From Deep Learning to LLMs: A survey of AI in Quantitative Investment","summary":"  Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.\n","authors":["Bokai Cao","Saizhuo Wang","Xinyi Lin","Xiaojun Wu","Haohan Zhang","Lionel M. Ni","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2503.21422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21401v1","updated":"2025-03-27T11:47:20Z","published":"2025-03-27T11:47:20Z","title":"AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control","summary":"  Quadrupedal robots can learn versatile locomotion skills but remain\nvulnerable when one or more joints lose power. In contrast, dogs and cats can\nadopt limping gaits when injured, demonstrating their remarkable ability to\nadapt to physical conditions. Inspired by such adaptability, this paper\npresents Action Learner (AcL), a novel teacher-student reinforcement learning\nframework that enables quadrupeds to autonomously adapt their gait for stable\nwalking under multiple joint faults. Unlike conventional teacher-student\napproaches that enforce strict imitation, AcL leverages teacher policies to\ngenerate style rewards, guiding the student policy without requiring precise\nreplication. We train multiple teacher policies, each corresponding to a\ndifferent fault condition, and subsequently distill them into a single student\npolicy with an encoder-decoder architecture. While prior works primarily\naddress single-joint faults, AcL enables quadrupeds to walk with up to four\nfaulty joints across one or two legs, autonomously switching between different\nlimping gaits when faults occur. We validate AcL on a real Go2 quadruped robot\nunder single- and double-joint faults, demonstrating fault-tolerant, stable\nwalking, smooth gait transitions between normal and lamb gaits, and robustness\nagainst external disturbances.\n","authors":["Tianyu Xu","Yaoyu Cheng","Pinxi Shen","Lin Zhao"," Electrical","Computer Engineering","National University of Singapore"," Singapore","Mechanical Engineering","National University of Singapore"," Singapore"],"pdf_url":"https://arxiv.org/pdf/2503.21401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17258v3","updated":"2025-03-27T11:41:54Z","published":"2024-08-30T12:56:17Z","title":"Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach","summary":"  The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability.\n","authors":["Tong Nie","Junlin He","Yuewen Mei","Guoyang Qin","Guilong Li","Jian Sun","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17258v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21397v1","updated":"2025-03-27T11:39:55Z","published":"2025-03-27T11:39:55Z","title":"ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification\n  via Multi-Depth Networks","summary":"  Out-of-distribution (OOD) detection in deep learning has traditionally been\nframed as a binary task, where samples are either classified as belonging to\nthe known classes or marked as OOD, with little attention given to the semantic\nrelationships between OOD samples and the in-distribution (ID) classes. We\npropose a framework for detecting and classifying OOD samples in a given class\nhierarchy. Specifically, we aim to predict OOD data to their correct internal\nnodes of the class hierarchy, whereas the known ID classes should be predicted\nas their corresponding leaf nodes. Our approach leverages the class hierarchy\nto create a probabilistic model and we implement this model by using networks\ntrained for ID classification at multiple hierarchy depths. We conduct\nexperiments on three datasets with predefined class hierarchies and show the\neffectiveness of our method. Our code is available at\nhttps://github.com/walline/prohoc.\n","authors":["Erik Wallin","Fredrik Kahl","Lars Hammarstrand"],"pdf_url":"https://arxiv.org/pdf/2503.21397v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.21383v1","updated":"2025-03-27T11:25:22Z","published":"2025-03-27T11:25:22Z","title":"Controlling Large Language Model with Latent Actions","summary":"  Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications.\n","authors":["Chengxing Jia","Ziniu Li","Pengyuan Wang","Yi-Chen Li","Zhenyu Hou","Yuxiao Dong","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2503.21383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20660v2","updated":"2025-03-27T11:07:27Z","published":"2025-03-26T15:55:44Z","title":"DR-PETS: Learning-Based Control With Planning in Adversarial\n  Environments","summary":"  Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates.\n","authors":["Hozefa Jesawada","Antonio Acernese","Giovanni Russo","Carmen Del Vecchio"],"pdf_url":"https://arxiv.org/pdf/2503.20660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00379v2","updated":"2025-03-27T10:37:18Z","published":"2025-03-01T07:11:30Z","title":"Improving clustering quality evaluation in noisy Gaussian mixtures","summary":"  Clustering is a well-established technique in machine learning and data\nanalysis, widely used across various domains. Cluster validity indices, such as\nthe Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldin indices,\nplay a crucial role in assessing clustering quality when external ground truth\nlabels are unavailable. However, these measures can be affected by the feature\nrelevance issue, potentially leading to unreliable evaluations in\nhigh-dimensional or noisy data sets.\n  We introduce a theoretically grounded Feature Importance Rescaling (FIR)\nmethod that enhances the quality of clustering validation by adjusting feature\ncontributions based on their dispersion. It attenuates noise features,\nclarifies clustering compactness and separation, and thereby aligns clustering\nvalidation more closely with the ground truth. Through extensive experiments on\nsynthetic data sets under different configurations, we demonstrate that FIR\nconsistently improves the correlation between the values of cluster validity\nindices and the ground truth, particularly in settings with noisy or irrelevant\nfeatures.\n  The results show that FIR increases the robustness of clustering evaluation,\nreduces variability in performance across different data sets, and remains\neffective even when clusters exhibit significant overlap. These findings\nhighlight the potential of FIR as a valuable enhancement of clustering\nvalidation, making it a practical tool for unsupervised learning tasks where\nlabelled data is unavailable.\n","authors":["Renato Cordeiro de Amorim","Vladimir Makarenkov"],"pdf_url":"https://arxiv.org/pdf/2503.00379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21349v1","updated":"2025-03-27T10:35:56Z","published":"2025-03-27T10:35:56Z","title":"Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records","summary":"  We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text.\n","authors":["Noah Losch","Lucas Plagwitz","Antonius Büscher","Julian Varghese"],"pdf_url":"https://arxiv.org/pdf/2503.21349v1.pdf","comment":"4 pages, 2 tables,"},{"id":"http://arxiv.org/abs/2503.21346v1","updated":"2025-03-27T10:25:03Z","published":"2025-03-27T10:25:03Z","title":"Scalable Expectation Estimation with Subtractive Mixture Models","summary":"  Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models\n(MMs) for their simplicity and ability to capture multimodal distributions.\nRecently, subtractive mixture models (SMMs), i.e. MMs with negative\ncoefficients, have shown greater expressiveness and success in generative\nmodeling. However, their negative parameters complicate sampling, requiring\ncostly auto-regressive techniques or accept-reject algorithms that do not scale\nin high dimensions. In this work, we use the difference representation of SMMs\nto construct an unbiased IS estimator ($\\Delta\\text{Ex}$) that removes the need\nto sample from the SMM, enabling high-dimensional expectation estimation with\nSMMs. In our experiments, we show that $\\Delta\\text{Ex}$ can achieve comparable\nestimation quality to auto-regressive sampling while being considerably faster\nin MC estimation. Moreover, we conduct initial experiments with\n$\\Delta\\text{Ex}$ using hand-crafted proposals, gaining first insights into how\nto construct safe proposals for $\\Delta\\text{Ex}$.\n","authors":["Lena Zellinger","Nicola Branchini","Víctor Elvira","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2503.21346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21323v1","updated":"2025-03-27T10:02:30Z","published":"2025-03-27T10:02:30Z","title":"DuckSegmentation: A segmentation model based on the AnYue Hemp Duck\n  Dataset","summary":"  The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming.\n","authors":["Ling Feng","Tianyu Xie","Wei Ma","Ruijie Fu","Yingxiao Zhang","Jun Li","Bei Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.21323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21212v2","updated":"2025-03-27T09:59:55Z","published":"2024-10-28T16:57:02Z","title":"On learning higher-order cumulants in diffusion models","summary":"  To analyse how diffusion models learn correlations beyond Gaussian ones, we\nstudy the behaviour of higher-order cumulants, or connected n-point functions,\nunder both the forward and backward process. We derive explicit expressions for\nthe moment- and cumulant-generating functionals, in terms of the distribution\nof the initial data and properties of forward process. It is shown analytically\nthat during the forward process higher-order cumulants are conserved in models\nwithout a drift, such as the variance-expanding scheme, and that therefore the\nendpoint of the forward process maintains nontrivial correlations. We\ndemonstrate that since these correlations are encoded in the score function,\nhigher-order cumulants are learnt in the backward process, also when starting\nfrom a normal prior. We confirm our analytical results in an exactly solvable\ntoy model with nonzero cumulants and in scalar lattice field theory.\n","authors":["Gert Aarts","Diaa E. Habibi","Lingxiao Wang","Kai Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.21212v2.pdf","comment":"21 pages, many figures. Extended version of contribution awarded\n  \"best 'physics for AI' paper award\" in the NeurIPS 2024 workshop \"Machine\n  Learning and the Physical Sciences\"; v2: references and minor clarifications\n  added, version to appear in Machine Learning: Science and Technology"},{"id":"http://arxiv.org/abs/2503.21321v1","updated":"2025-03-27T09:59:45Z","published":"2025-03-27T09:59:45Z","title":"Explainable Boosting Machine for Predicting Claim Severity and Frequency\n  in Car Insurance","summary":"  In a context of constant increase in competition and heightened regulatory\npressure, accuracy, actuarial precision, as well as transparency and\nunderstanding of the tariff, are key issues in non-life insurance.\nTraditionally used generalized linear models (GLM) result in a multiplicative\ntariff that favors interpretability. With the rapid development of machine\nlearning and deep learning techniques, actuaries and the rest of the insurance\nindustry have adopted these techniques widely. However, there is a need to\nassociate them with interpretability techniques. In this paper, our study\nfocuses on introducing an Explainable Boosting Machine (EBM) model that\ncombines intrinsically interpretable characteristics and high prediction\nperformance. This approach is described as a glass-box model and relies on the\nuse of a Generalized Additive Model (GAM) and a cyclic gradient boosting\nalgorithm. It accounts for univariate and pairwise interaction effects between\nfeatures and provides naturally explanations on them. We implement this\napproach on car insurance frequency and severity data and extensively compare\nthe performance of this approach with classical competitors: a GLM, a GAM, a\nCART model and an Extreme Gradient Boosting (XGB) algorithm. Finally, we\nexamine the interpretability of these models to capture the main determinants\nof claim costs.\n","authors":["Markéta Krùpovà","Nabil Rachdi","Quentin Guibert"],"pdf_url":"https://arxiv.org/pdf/2503.21321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03215v2","updated":"2025-03-27T09:59:41Z","published":"2024-12-04T11:08:32Z","title":"Beyond [cls]: Exploring the true potential of Masked Image Modeling\n  representations","summary":"  Masked Image Modeling (MIM) has emerged as a promising approach for\nSelf-Supervised Learning (SSL) of visual representations. However, the\nout-of-the-box performance of MIMs is typically inferior to competing\napproaches. Most users cannot afford fine-tuning due to the need for large\namounts of data, high GPU consumption, and specialized user knowledge.\nTherefore, the practical use of MIM representations is limited. In this paper\nwe ask what is the reason for the poor out-of-the-box performance of MIMs. Is\nit due to weaker features produced by MIM models, or is it due to suboptimal\nusage? Through detailed analysis, we show that attention in MIMs is spread\nalmost uniformly over many patches, leading to ineffective aggregation by the\n[cls] token. Based on this insight, we propose Selective Aggregation to better\ncapture the rich semantic information retained in patch tokens, which\nsignificantly improves the out-of-the-box performance of MIM.\n","authors":["Marcin Przewięźlikowski","Randall Balestriero","Wojciech Jasiński","Marek Śmieja","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2412.03215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21315v1","updated":"2025-03-27T09:54:37Z","published":"2025-03-27T09:54:37Z","title":"Tricking Retrievers with Influential Tokens: An Efficient Black-Box\n  Corpus Poisoning Attack","summary":"  Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets.\n","authors":["Cheng Wang","Yiwei Wang","Yujun Cai","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2503.21315v1.pdf","comment":"Accepted to NAACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2501.05037v2","updated":"2025-03-27T09:39:11Z","published":"2025-01-09T07:51:14Z","title":"LongViTU: Instruction Tuning for Long-Form Video Understanding","summary":"  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),\nautomatically generated dataset for long-form video understanding. We propose a\nsystematic approach that organizes videos into a hierarchical tree structure\nfor QA generation and incorporates self-revision mechanisms to ensure\nhigh-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context\n(average certificate length of 4.6 minutes); 2) rich knowledge and condensed\nreasoning (commonsense, causality, planning, etc.)). We also offer explicit\ntimestamp annotations of relevant events for each QA pair. We have conducted\nextensive human studies on LongViTU, and the results prove the quality of our\ndataset. To better evaluate the challenges posed by LongViTU's emphasis on\nlong-term context and condensed reasoning, we manually curate a subset of\nLongViTU into a benchmark. Evaluations using a state-of-the-art open-source\nmodel (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators\nyield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the\nsubstantial difficulty presented by LongViTU questions. Performing supervised\nfine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average\nperformance gains of 2.5% and 3.7%, respectively, across a suite of long video\nunderstanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).\n","authors":["Rujie Wu","Xiaojian Ma","Hai Ci","Yue Fan","Yuxuan Wang","Haozhe Zhao","Qing Li","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2501.05037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13088v2","updated":"2025-03-27T09:29:43Z","published":"2024-05-21T11:42:15Z","title":"Combining Relevance and Magnitude for Resource-Aware DNN Pruning","summary":"  Pruning neural networks, i.e., removing some of their parameters whilst\nretaining their accuracy, is one of the main ways to reduce the latency of a\nmachine learning pipeline, especially in resource- and/or bandwidth-constrained\nscenarios. In this context, the pruning technique, i.e., how to choose the\nparameters to remove, is critical to the system performance. In this paper, we\npropose a novel pruning approach, called FlexRel and predicated upon combining\ntraining-time and inference-time information, namely, parameter magnitude and\nrelevance, in order to improve the resulting accuracy whilst saving both\ncomputational resources and bandwidth. Our performance evaluation shows that\nFlexRel is able to achieve higher pruning factors, saving over 35% bandwidth\nfor typical accuracy targets.\n","authors":["Carla Fabiana Chiasserini","Francesco Malandrino","Nuria Molner","Zhiqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.13088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21303v1","updated":"2025-03-27T09:29:33Z","published":"2025-03-27T09:29:33Z","title":"Simulation-informed deep learning for enhanced SWOT observations of\n  fine-scale ocean dynamics","summary":"  Oceanic processes at fine scales are crucial yet difficult to observe\naccurately due to limitations in satellite and in-situ measurements. The\nSurface Water and Ocean Topography (SWOT) mission provides high-resolution Sea\nSurface Height (SSH) data, though noise patterns often obscure fine scale\nstructures. Current methods struggle with noisy data or require extensive\nsupervised training, limiting their effectiveness on real-world observations.\nWe introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative\nEnsemble Networks), an unsupervised adversarial learning framework combining\nreal SWOT observations with simulated reference data. SIMPGEN leverages\nwavelet-informed neural metrics to distinguish noisy from clean fields, guiding\nrealistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively\nremoves noise, preserving fine-scale features better than existing neural\nmethods. This robust, unsupervised approach not only improves SWOT SSH data\ninterpretation but also demonstrates strong potential for broader oceanographic\napplications, including data assimilation and super-resolution.\n","authors":["Eugenio Cutolo","Carlos Granero-Belinchon","Ptashanna Thiraux","Jinbo Wang","Ronan Fablet"],"pdf_url":"https://arxiv.org/pdf/2503.21303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18886v3","updated":"2025-03-27T08:58:57Z","published":"2024-03-27T17:59:21Z","title":"Self-Expansion of Pre-trained Models with Mixture of Adapters for\n  Continual Learning","summary":"  Continual learning (CL) aims to continually accumulate knowledge from a\nnon-stationary data stream without catastrophic forgetting of learned\nknowledge, requiring a balance between stability and adaptability. Relying on\nthe generalizable representation in pre-trained models (PTMs), PTM-based CL\nmethods perform effective continual adaptation on downstream tasks by adding\nlearnable adapters or prompts upon the frozen PTMs. However, many existing\nPTM-based CL methods use restricted adaptation on a fixed set of these modules\nto avoid forgetting, suffering from limited CL ability. Periodically adding\ntask-specific modules results in linear model growth rate and impaired\nknowledge reuse. We propose Self-Expansion of pre-trained models with\nModularized Adaptation (SEMA), a novel approach to enhance the control of\nstability-plasticity balance in PTM-based CL. SEMA automatically decides to\nreuse or add adapter modules on demand in CL, depending on whether significant\ndistribution shift that cannot be handled is detected at different\nrepresentation levels. We design modular adapter consisting of a functional\nadapter and a representation descriptor. The representation descriptors are\ntrained as a distribution shift indicator and used to trigger self-expansion\nsignals. For better composing the adapters, an expandable weighting router is\nlearned jointly for mixture of adapter outputs. SEMA enables better knowledge\nreuse and sub-linear expansion rate. Extensive experiments demonstrate the\neffectiveness of the proposed self-expansion method, achieving state-of-the-art\nperformance compared to PTM-based CL methods without memory rehearsal. Code is\navailable at https://github.com/huiyiwang01/SEMA-CL.\n","authors":["Huiyi Wang","Haodong Lu","Lina Yao","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2403.18886v3.pdf","comment":"Code available at https: https://github.com/huiyiwang01/SEMA-CL"},{"id":"http://arxiv.org/abs/2406.08756v3","updated":"2025-03-27T08:52:35Z","published":"2024-06-13T02:31:36Z","title":"Optimizing Large Model Training through Overlapped Activation\n  Recomputation","summary":"  Large model training often uses recomputation to alleviate memory pressure\nand pipelines to exploit the parallelism of data, tensors, and devices.\nHowever, existing recomputation approaches may incur high overhead when\ntraining real-world models, as they are executed on demand in the critical\ntraining path. In this paper, we present Lynx, a new recomputation framework to\nreduce overhead by overlapping recomputation with communication in training\npipelines. To reduce the large search space for recomputation strategies, we\npropose a heuristic-based recomputation scheduling algorithm, which is based on\nthe observation that there are identical structures in large DNN models so that\nwe can apply the same scheduling policy to all such structures. Additionally,\nwe propose a recomputation-aware model partitioning method to balance each\nstage's execution time for improved training throughput. Our comprehensive\nevaluation using GPT models with 1.3B-23B parameters shows that Lynx\noutperforms existing recomputation approaches by up to 1.37x.\n","authors":["Ping Chen","Wenjie Zhang","Shuibing He","Weijian Chen","Siling Yang","Kexin Huang","Yanlong Yin","Xuan Zhan","Yingjie Gu","Zhuwei Peng","Yi Zheng","Zhefeng Wang","Gang Chen Yingjie Gu","Zhuwei Peng","Kexin Huang","Xuan Zhan","Weijian Chen","Yi Zheng","Zhefeng Wang","Yanlong Yin","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08756v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.21858v5","updated":"2025-03-27T08:43:26Z","published":"2024-10-29T08:42:22Z","title":"Joint Estimation of Conditional Mean and Covariance for Unbalanced\n  Panels","summary":"  We develop a nonparametric, kernel-based joint estimator for conditional mean\nand covariance matrices in large and unbalanced panels. The estimator is\nsupported by rigorous consistency results and finite-sample guarantees,\nensuring its reliability for empirical applications. We apply it to an\nextensive panel of monthly US stock excess returns from 1962 to 2021, using\nmacroeconomic and firm-specific covariates as conditioning variables. The\nestimator effectively captures time-varying cross-sectional dependencies,\ndemonstrating robust statistical and economic performance. We find that\nidiosyncratic risk explains, on average, more than 75% of the cross-sectional\nvariance.\n","authors":["Damir Filipovic","Paul Schneider"],"pdf_url":"https://arxiv.org/pdf/2410.21858v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21261v1","updated":"2025-03-27T08:37:24Z","published":"2025-03-27T08:37:24Z","title":"HOT: Hadamard-based Optimized Training","summary":"  It has become increasingly important to optimize backpropagation to reduce\nmemory usage and computational overhead. Achieving this goal is highly\nchallenging, as multiple objectives must be considered jointly while\nmaintaining training quality. In this paper, we focus on matrix multiplication,\nwhich accounts for the largest portion of training costs, and analyze its\nbackpropagation in detail to identify lightweight techniques that offer the\nbest benefits. Based on this analysis, we introduce a novel method,\nHadamard-based Optimized Training (HOT). In this approach, we apply\nHadamard-based optimizations, such as Hadamard quantization and Hadamard\nlow-rank approximation, selectively and with awareness of the suitability of\neach optimization for different backward paths. Additionally, we introduce two\nenhancements: activation buffer compression and layer-wise quantizer selection.\nOur extensive analysis shows that HOT achieves up to 75% memory savings and a\n2.6 times acceleration on real GPUs, with negligible accuracy loss compared to\nFP32 precision.\n","authors":["Seonggon Kim","Juncheol Shin","Seung-taek Woo","Eunhyeok Park"],"pdf_url":"https://arxiv.org/pdf/2503.21261v1.pdf","comment":"Accepted in CVPR 2025"},{"id":"http://arxiv.org/abs/2411.19835v3","updated":"2025-03-27T08:34:04Z","published":"2024-11-29T16:45:25Z","title":"Feedback-driven object detection and iterative model improvement","summary":"  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub. To support the understanding of our labeling process, we\nhave created an explanatory video demonstrating the methodology using\nmicroscopy images of E. coli bacteria as an example.\n","authors":["Sönke Tenckhoff","Mario Koddenbrock","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2411.19835v3.pdf","comment":"Code: https://github.com/ml-lab-htw/iterative-annotate Video:\n  https://www.youtube.com/watch?v=CM9uhE8NN5E"},{"id":"http://arxiv.org/abs/2410.08522v2","updated":"2025-03-27T08:18:23Z","published":"2024-10-11T04:53:18Z","title":"Evaluating the effects of Data Sparsity on the Link-level Bicycling\n  Volume Estimation: A Graph Convolutional Neural Network Approach","summary":"  Accurate bicycling volume estimation is crucial for making informed decisions\nand planning about future investments in bicycling infrastructure. However,\ntraditional link-level volume estimation models are effective for motorized\ntraffic but face significant challenges when applied to the bicycling context\nbecause of sparse data and the intricate nature of bicycling mobility patterns.\nTo the best of our knowledge, we present the first study to utilize a Graph\nConvolutional Network (GCN) architecture to model link-level bicycling volumes\nand systematically investigate the impact of varying levels of data sparsity\n(0%--99%) on model performance, simulating real-world scenarios. We have\nleveraged Strava Metro data as the primary source of bicycling counts across\n15,933 road segments/links in the City of Melbourne, Australia. To evaluate the\neffectiveness of the GCN model, we benchmark it against traditional machine\nlearning models, such as linear regression, support vector machines, and random\nforest. Our results show that the GCN model outperforms these traditional\nmodels in predicting Annual Average Daily Bicycle (AADB) counts, demonstrating\nits ability to capture the spatial dependencies inherent in bicycle traffic\nnetworks. While GCN remains robust up to 80% sparsity, its performance declines\nsharply beyond this threshold, highlighting the challenges of extreme data\nsparsity. These findings underscore the potential of GCNs in enhancing\nbicycling volume estimation, while also emphasizing the need for further\nresearch on methods to improve model resilience under high-sparsity conditions.\nOur findings offer valuable insights for city planners aiming to improve\nbicycling infrastructure and promote sustainable transportation.\n","authors":["Mohit Gupta","Debjit Bhowmick","Meead Saberi","Shirui Pan","Ben Beck"],"pdf_url":"https://arxiv.org/pdf/2410.08522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21224v1","updated":"2025-03-27T07:35:23Z","published":"2025-03-27T07:35:23Z","title":"Efficient Learning for Entropy-regularized Markov Decision Processes via\n  Multilevel Monte Carlo","summary":"  Designing efficient learning algorithms with complexity guarantees for Markov\ndecision processes (MDPs) with large or continuous state and action spaces\nremains a fundamental challenge. We address this challenge for\nentropy-regularized MDPs with Polish state and action spaces, assuming access\nto a generative model of the environment. We propose a novel family of\nmultilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration\nwith MLMC techniques and a generic stochastic approximation of the Bellman\noperator. We quantify the precise impact of the chosen approximate Bellman\noperator on the accuracy of the resulting MLMC estimator. Leveraging this error\nanalysis, we show that using a biased plain MC estimate for the Bellman\noperator results in quasi-polynomial sample complexity, whereas an unbiased\nrandomized multilevel approximation of the Bellman operator achieves polynomial\nsample complexity in expectation. Notably, these complexity bounds are\nindependent of the dimensions or cardinalities of the state and action spaces,\ndistinguishing our approach from existing algorithms whose complexities scale\nwith the sizes of these spaces. We validate these theoretical performance\nguarantees through numerical experiments.\n","authors":["Matthieu Meunier","Christoph Reisinger","Yufei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21224v1.pdf","comment":"46 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.21223v1","updated":"2025-03-27T07:28:30Z","published":"2025-03-27T07:28:30Z","title":"Rethinking Graph Structure Learning in the Era of LLMs","summary":"  Recently, the emergence of large language models (LLMs) has prompted\nresearchers to explore the integration of language descriptions into graphs,\naiming to enhance model encoding capabilities from a data-centric perspective.\nThis graph representation is called text-attributed graphs (TAGs). A review of\nprior advancements highlights that graph structure learning (GSL) is a pivotal\ntechnique for improving data utility, making it highly relevant to efficient\nTAG learning. However, most GSL methods are tailored for traditional graphs\nwithout textual information, underscoring the necessity of developing a new GSL\nparadigm. Despite clear motivations, it remains challenging: (1) How can we\ndefine a reasonable optimization objective for GSL in the era of LLMs,\nconsidering the massive parameters in LLM? (2) How can we design an efficient\nmodel architecture that enables seamless integration of LLM for this\noptimization objective? For Question 1, we reformulate existing GSL\noptimization objectives as a tree optimization framework, shifting the focus\nfrom obtaining a well-trained edge predictor to a language-aware tree sampler.\nFor Question 2, we propose decoupled and training-free model design principles\nfor LLM integration, shifting the focus from computation-intensive fine-tuning\nto more efficient inference. Based on this, we propose Large Language and Tree\nAssistant (LLaTA), which leverages tree-based LLM in-context learning to\nenhance the understanding of topology and text, enabling reliable inference and\ngenerating improved graph structure. Extensive experiments on 10 TAG datasets\ndemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;\nscalability - outperforms other LLM-based GSL methods in terms of running\nefficiency; effectiveness - achieves SOTA performance.\n","authors":["Zhihan Zhang","Xunkai Li","Guang Zeng","Hongchao Qin","Ronghua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21223v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.21213v1","updated":"2025-03-27T07:05:22Z","published":"2025-03-27T07:05:22Z","title":"Resource-Efficient Federated Fine-Tuning Large Language Models for\n  Heterogeneous Data","summary":"  Fine-tuning large language models (LLMs) via federated learning, i.e.,\nFedLLM, has been proposed to adapt LLMs for various downstream applications in\na privacy-preserving way. To reduce the fine-tuning costs on\nresource-constrained devices, FedLoRA is proposed to fine-tune only a small\nsubset of model parameters by integrating low-rank adaptation (LoRA) into\nFedLLM. However, apart from resource constraints, there is still another\ncritical challenge, i.e., data heterogeneity, severely hindering the\nimplementation of FedLoRA in practical applications. Herein, inspired by the\nprevious group-based federated learning paradigm, we propose a hierarchical\nFedLoRA framework, termed HierFedLoRA, to address these challenges.\nSpecifically, HierFedLoRA partitions all devices into multiple near-IID groups\nand adjusts the intra-group aggregation frequency for each group to eliminate\nthe negative effects of non-IID data. Meanwhile, to reduce the computation and\ncommunication cost, HierFedLoRA dynamically assigns diverse and suitable\nfine-tuning depth (i.e., the number of continuous fine-tuning layers from the\noutput) for each group. HierFedLoRA explores jointly optimizing aggregation\nfrequency and depth upon their coupled relationship to better enhance the\nperformance of FedLoRA. Extensive experiments are conducted on a physical\nplatform with 80 commercial devices. The results show that HierFedLoRA improves\nthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process\nby at least 2.1$\\times$, compared to the strong baselines.\n","authors":["Jun Liu","Yunming Liao","Hongli Xu","Yang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.21213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21211v1","updated":"2025-03-27T06:55:29Z","published":"2025-03-27T06:55:29Z","title":"Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO\n  Skilfully Beyond 2 Years","summary":"  El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.\n","authors":["Rixu Hao","Yuxin Zhao","Shaoqing Zhang","Guihua Wang","Xiong Deng"],"pdf_url":"https://arxiv.org/pdf/2503.21211v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.21200v1","updated":"2025-03-27T06:35:59Z","published":"2025-03-27T06:35:59Z","title":"Learning Generalizable Skills from Offline Multi-Task Data for\n  Multi-Agent Cooperation","summary":"  Learning cooperative multi-agent policy from offline multi-task data that can\ngeneralize to unseen tasks with varying numbers of agents and targets is an\nattractive problem in many scenarios. Although aggregating general behavior\npatterns among multiple tasks as skills to improve policy transfer is a\npromising approach, two primary challenges hinder the further advancement of\nskill learning in offline multi-task MARL. Firstly, extracting general\ncooperative behaviors from various action sequences as common skills lacks\nbringing cooperative temporal knowledge into them. Secondly, existing works\nonly involve common skills and can not adaptively choose independent knowledge\nas task-specific skills in each task for fine-grained action execution. To\ntackle these challenges, we propose Hierarchical and Separate Skill Discovery\n(HiSSD), a novel approach for generalizable offline multi-task MARL through\nskill learning. HiSSD leverages a hierarchical framework that jointly learns\ncommon and task-specific skills. The common skills learn cooperative temporal\nknowledge and enable in-sample exploitation for offline multi-task MARL. The\ntask-specific skills represent the priors of each task and achieve a\ntask-guided fine-grained action execution. To verify the advancement of our\nmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After\ntraining the policy using HiSSD on offline multi-task data, the empirical\nresults show that HiSSD assigns effective cooperative behaviors and obtains\nsuperior performance in unseen tasks.\n","authors":["Sicong Liu","Yang Shu","Chenjuan Guo","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.21200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18924v3","updated":"2025-03-27T06:08:36Z","published":"2025-02-26T08:22:00Z","title":"Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot\n  Speech Synthesis","summary":"  While recent zero-shot text-to-speech (TTS) models have significantly\nimproved speech quality and expressiveness, mainstream systems still suffer\nfrom issues related to speech-text alignment modeling: 1) models without\nexplicit speech-text alignment modeling exhibit less robustness, especially for\nhard sentences in practical applications; 2) predefined alignment-based models\nsuffer from naturalness constraints of forced alignments. This paper introduces\n\\textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment\nalgorithm that guides the latent diffusion transformer (DiT). Specifically, we\nprovide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of\nalignment without limiting the search space, thereby achieving high\nnaturalness. Moreover, we employ a multi-condition classifier-free guidance\nstrategy for accent intensity adjustment and adopt the piecewise rectified flow\ntechnique to accelerate the generation process. Experiments demonstrate that\nMegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports\nhighly flexible control over accent intensity. Notably, our system can generate\nhigh-quality one-minute speech with only 8 sampling steps. Audio samples are\navailable at https://sditdemo.github.io/sditdemo/.\n","authors":["Ziyue Jiang","Yi Ren","Ruiqi Li","Shengpeng Ji","Boyang Zhang","Zhenhui Ye","Chen Zhang","Bai Jionghao","Xiaoda Yang","Jialong Zuo","Yu Zhang","Rui Liu","Xiang Yin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18924v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21166v1","updated":"2025-03-27T05:36:12Z","published":"2025-03-27T05:36:12Z","title":"Unveiling the Potential of Superexpressive Networks in Implicit Neural\n  Representations","summary":"  In this study, we examine the potential of one of the ``superexpressive''\nnetworks in the context of learning neural functions for representing complex\nsignals and performing machine learning downstream tasks. Our focus is on\nevaluating their performance on computer vision and scientific machine learning\ntasks including signal representation/inverse problems and solutions of partial\ndifferential equations. Through an empirical investigation in various benchmark\ntasks, we demonstrate that superexpressive networks, as proposed by [Zhang et\nal. NeurIPS, 2022], which employ a specialized network structure characterized\nby having an additional dimension, namely width, depth, and ``height'', can\nsurpass recent implicit neural representations that use highly-specialized\nnonlinear activation functions.\n","authors":["Uvini Balasuriya Mudiyanselage","Woojin Cho","Minju Jo","Noseong Park","Kookjin Lee"],"pdf_url":"https://arxiv.org/pdf/2503.21166v1.pdf","comment":"Accepted at ICLR 2025 Workshop on Neural Network Weights as a New\n  Data Modality"},{"id":"http://arxiv.org/abs/2503.21160v1","updated":"2025-03-27T04:59:45Z","published":"2025-03-27T04:59:45Z","title":"A Data Balancing and Ensemble Learning Approach for Credit Card Fraud\n  Detection","summary":"  This research introduces an innovative method for identifying credit card\nfraud by combining the SMOTE-KMEANS technique with an ensemble machine learning\nmodel. The proposed model was benchmarked against traditional models such as\nlogistic regression, decision trees, random forests, and support vector\nmachines. Performance was evaluated using metrics, including accuracy, recall,\nand area under the curve (AUC). The results demonstrated that the proposed\nmodel achieved superior performance, with an AUC of 0.96 when combined with the\nSMOTE-KMEANS algorithm. This indicates a significant improvement in detecting\nfraudulent transactions while maintaining high precision and recall. The study\nalso explores the application of different oversampling techniques to enhance\nthe performance of various classifiers. The findings suggest that the proposed\nmethod is robust and effective for classification tasks on balanced datasets.\nFuture research directions include further optimization of the SMOTE-KMEANS\napproach and its integration into existing fraud detection systems to enhance\nfinancial security and consumer protection.\n","authors":["Yuhan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.21160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16449v2","updated":"2025-03-27T04:53:03Z","published":"2024-10-21T19:20:34Z","title":"Robust Feature Learning for Multi-Index Models in High Dimensions","summary":"  Recently, there have been numerous studies on feature learning with neural\nnetworks, specifically on learning single- and multi-index models where the\ntarget is a function of a low-dimensional projection of the input. Prior works\nhave shown that in high dimensions, the majority of the compute and data\nresources are spent on recovering the low-dimensional projection; once this\nsubspace is recovered, the remainder of the target can be learned independently\nof the ambient dimension. However, implications of feature learning in\nadversarial settings remain unexplored. In this work, we take the first steps\ntowards understanding adversarially robust feature learning with neural\nnetworks. Specifically, we prove that the hidden directions of a multi-index\nmodel offer a Bayes optimal low-dimensional projection for robustness against\n$\\ell_2$-bounded adversarial perturbations under the squared loss, assuming\nthat the multi-index coordinates are statistically independent from the rest of\nthe coordinates. Therefore, robust learning can be achieved by first performing\nstandard feature learning, then robustly tuning a linear readout layer on top\nof the standard representations. In particular, we show that adversarially\nrobust learning is just as easy as standard learning. Specifically, the\nadditional number of samples needed to robustly learn multi-index models when\ncompared to standard learning does not depend on dimensionality.\n","authors":["Alireza Mousavi-Hosseini","Adel Javanmard","Murat A. Erdogdu"],"pdf_url":"https://arxiv.org/pdf/2410.16449v2.pdf","comment":"41 pages, 1 figure. To appear in the International Conference on\n  Learning Representations (ICLR), 2025"},{"id":"http://arxiv.org/abs/2503.21157v1","updated":"2025-03-27T04:50:14Z","published":"2025-03-27T04:50:14Z","title":"Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?","summary":"  This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall.\n","authors":["Ashish Sardana"],"pdf_url":"https://arxiv.org/pdf/2503.21157v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.21155v1","updated":"2025-03-27T04:48:58Z","published":"2025-03-27T04:48:58Z","title":"Embedding Domain-Specific Knowledge from LLMs into the Feature\n  Engineering Pipeline","summary":"  Feature engineering is mandatory in the machine learning pipeline to obtain\nrobust models. While evolutionary computation is well-known for its great\nresults both in feature selection and feature construction, its methods are\ncomputationally expensive due to the large number of evaluations required to\ninduce the final model. Part of the reason why these algorithms require a large\nnumber of evaluations is their lack of domain-specific knowledge, resulting in\na lot of random guessing during evolution. In this work, we propose using Large\nLanguage Models (LLMs) as an initial feature construction step to add knowledge\nto the dataset. By doing so, our results show that the evolution can converge\nfaster, saving us computational resources. The proposed approach only provides\nthe names of the features in the dataset and the target objective to the LLM,\nmaking it usable even when working with datasets containing private data. While\nconsistent improvements to test performance were only observed for one-third of\nthe datasets (CSS, PM, and IM10), possibly due to problems being easily\nexplored by LLMs, this approach only decreased the model performance in 1/77\ntest cases. Additionally, this work introduces the M6GP feature engineering\nalgorithm to symbolic regression, showing it can improve the results of the\nrandom forest regressor and produce competitive results with its predecessor,\nM3GP.\n","authors":["João Eduardo Batista"],"pdf_url":"https://arxiv.org/pdf/2503.21155v1.pdf","comment":"9 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.21135v1","updated":"2025-03-27T03:52:25Z","published":"2025-03-27T03:52:25Z","title":"MoQa: Rethinking MoE Quantization with Multi-stage Data-model\n  Distribution Awareness","summary":"  With the advances in artificial intelligence, Mix-of-Experts (MoE) has become\nthe main form of Large Language Models (LLMs), and its demand for model\ncompression is increasing. Quantization is an effective method that not only\ncompresses the models but also significantly accelerates their performance.\nExisting quantization methods have gradually shifted the focus from parameter\nscaling to the analysis of data distributions. However, their analysis is\ndesigned for dense LLMs and relies on the simple one-model-all-data mapping,\nwhich is unsuitable for MoEs. This paper proposes a new quantization framework\ncalled MoQa. MoQa decouples the data-model distribution complexity of MoEs in\nmultiple analysis stages, quantitively revealing the dynamics during sparse\ndata activation, data-parameter mapping, and inter-expert correlations. Based\non these, MoQa identifies particular experts' and parameters' significance with\noptimal data-model distribution awareness and proposes a series of fine-grained\nmix-quantization strategies adaptive to various data activation and expert\ncombination scenarios. Moreover, MoQa discusses the limitations of existing\nquantization and analyzes the impact of each stage analysis, showing novel\ninsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18\nperplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy\nimprovement in zero-shot inference tasks. We believe MoQa will play a role in\nfuture MoE construction, optimization, and compression.\n","authors":["Zihao Zheng","Xiuping Cui","Size Zheng","Maoliang Li","Jiayu Chen"," Yun"," Liang","Xiang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.21135v1.pdf","comment":"6 pages, 6 figures and 3 tables"},{"id":"http://arxiv.org/abs/2408.07254v2","updated":"2025-03-27T03:40:20Z","published":"2024-08-14T02:13:35Z","title":"Learning Multi-Index Models with Neural Networks via Mean-Field Langevin\n  Dynamics","summary":"  We study the problem of learning multi-index models in high-dimensions using\na two-layer neural network trained with the mean-field Langevin algorithm.\nUnder mild distributional assumptions on the data, we characterize the\neffective dimension $d_{\\mathrm{eff}}$ that controls both sample and\ncomputational complexity by utilizing the adaptivity of neural networks to\nlatent low-dimensional structures. When the data exhibit such a structure,\n$d_{\\mathrm{eff}}$ can be significantly smaller than the ambient dimension. We\nprove that the sample complexity grows almost linearly with $d_{\\mathrm{eff}}$,\nbypassing the limitations of the information and generative exponents that\nappeared in recent analyses of gradient-based feature learning. On the other\nhand, the computational complexity may inevitably grow exponentially with\n$d_{\\mathrm{eff}}$ in the worst-case scenario. Motivated by improving\ncomputational complexity, we take the first steps towards polynomial time\nconvergence of the mean-field Langevin algorithm by investigating a setting\nwhere the weights are constrained to be on a compact manifold with positive\nRicci curvature, such as the hypersphere. There, we study assumptions under\nwhich polynomial time convergence is achievable, whereas similar assumptions in\nthe Euclidean setting lead to exponential time complexity.\n","authors":["Alireza Mousavi-Hosseini","Denny Wu","Murat A. Erdogdu"],"pdf_url":"https://arxiv.org/pdf/2408.07254v2.pdf","comment":"36 pages, 2 figures. To appear in the International Conference on\n  Learning Representations (ICLR), 2025"},{"id":"http://arxiv.org/abs/2503.21128v1","updated":"2025-03-27T03:39:35Z","published":"2025-03-27T03:39:35Z","title":"Squared families: Searching beyond regular probability models","summary":"  We introduce squared families, which are families of probability densities\nobtained by squaring a linear transformation of a statistic. Squared families\nare singular, however their singularity can easily be handled so that they form\nregular models. After handling the singularity, squared families possess many\nconvenient properties. Their Fisher information is a conformal transformation\nof the Hessian metric induced from a Bregman generator. The Bregman generator\nis the normalising constant, and yields a statistical divergence on the family.\nThe normalising constant admits a helpful parameter-integral factorisation,\nmeaning that only one parameter-independent integral needs to be computed for\nall normalising constants in the family, unlike in exponential families.\nFinally, the squared family kernel is the only integral that needs to be\ncomputed for the Fisher information, statistical divergence and normalising\nconstant. We then describe how squared families are special in the broader\nclass of $g$-families, which are obtained by applying a sufficiently regular\nfunction $g$ to a linear transformation of a statistic. After removing special\nsingularities, positively homogeneous families and exponential families are the\nonly $g$-families for which the Fisher information is a conformal\ntransformation of the Hessian metric, where the generator depends on the\nparameter only through the normalising constant. Even-order monomial families\nalso admit parameter-integral factorisations, unlike exponential families. We\nstudy parameter estimation and density estimation in squared families, in the\nwell-specified and misspecified settings. We use a universal approximation\nproperty to show that squared families can learn sufficiently well-behaved\ntarget densities at a rate of $\\mathcal{O}(N^{-1/2})+C n^{-1/4}$, where $N$ is\nthe number of datapoints, $n$ is the number of parameters, and $C$ is some\nconstant.\n","authors":["Russell Tsuchida","Jiawei Liu","Cheng Soon Ong","Dino Sejdinovic"],"pdf_url":"https://arxiv.org/pdf/2503.21128v1.pdf","comment":"43 pages. Preprint"},{"id":"http://arxiv.org/abs/2503.21105v1","updated":"2025-03-27T02:58:28Z","published":"2025-03-27T02:58:28Z","title":"AugWard: Augmentation-Aware Representation Learning for Accurate Graph\n  Classification","summary":"  How can we accurately classify graphs? Graph classification is a pivotal task\nin data mining with applications in social network analysis, web analysis, drug\ndiscovery, molecular property prediction, etc. Graph neural networks have\nachieved the state-of-the-art performance in graph classification, but they\nconsistently struggle with overfitting. To mitigate overfitting, researchers\nhave introduced various representation learning methods utilizing graph\naugmentation. However, existing methods rely on simplistic use of graph\naugmentation, which loses augmentation-induced differences and limits the\nexpressiveness of representations.\n  In this paper, we propose AugWard (Augmentation-Aware Training with Graph\nDistance and Consistency Regularization), a novel graph representation learning\nframework that carefully considers the diversity introduced by graph\naugmentation. AugWard applies augmentation-aware training to predict the graph\ndistance between the augmented graph and its original one, aligning the\nrepresentation difference directly with graph distance at both feature and\nstructure levels. Furthermore, AugWard employs consistency regularization to\nencourage the classifier to handle richer representations. Experimental results\nshow that AugWard gives the state-of-the-art performance in supervised,\nsemi-supervised graph classification, and transfer learning.\n","authors":["Minjun Kim","Jaehyeon Choi","SeungJoo Lee","Jinhong Jung","U Kang"],"pdf_url":"https://arxiv.org/pdf/2503.21105v1.pdf","comment":"Accepted to PAKDD 2025 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2503.21103v1","updated":"2025-03-27T02:49:31Z","published":"2025-03-27T02:49:31Z","title":"Low Stein Discrepancy via Message-Passing Monte Carlo","summary":"  Message-Passing Monte Carlo (MPMC) was recently introduced as a novel\nlow-discrepancy sampling approach leveraging tools from geometric deep\nlearning. While originally designed for generating uniform point sets, we\nextend this framework to sample from general multivariate probability\ndistributions with known probability density function. Our proposed method,\nStein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein\ndiscrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC\noutperforms competing methods, such as Stein Variational Gradient Descent and\n(greedy) Stein Points, by achieving a lower Stein discrepancy.\n","authors":["Nathan Kirk","T. Konstantin Rusch","Jakob Zech","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2503.21103v1.pdf","comment":"8 pages, 2 figures, Accepted at the ICLR 2025 Workshop on Frontiers\n  in Probabilistic Inference"},{"id":"http://arxiv.org/abs/2503.11240v2","updated":"2025-03-27T02:34:59Z","published":"2025-03-14T09:45:19Z","title":"Towards Better Alignment: Training Diffusion Models with Reinforcement\n  Learning Against Sparse Rewards","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration. However, their practical applications are hindered by the\nmisalignment between generated images and corresponding text prompts. To tackle\nthis issue, reinforcement learning (RL) has been considered for diffusion model\nfine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse\nreward, where feedback is only available at the end of the generation process.\nThis makes it difficult to identify which actions during the denoising process\ncontribute positively to the final generated image, potentially leading to\nineffective or unnecessary denoising policies. To this end, this paper presents\na novel RL-based framework that addresses the sparse reward problem when\ntraining diffusion models. Our framework, named $\\text{B}^2\\text{-DiffuRL}$,\nemploys two strategies: \\textbf{B}ackward progressive training and\n\\textbf{B}ranch-based sampling. For one thing, backward progressive training\nfocuses initially on the final timesteps of denoising process and gradually\nextends the training interval to earlier timesteps, easing the learning\ndifficulty from sparse rewards. For another, we perform branch-based sampling\nfor each training interval. By comparing the samples within the same branch, we\ncan identify how much the policies of the current training interval contribute\nto the final image, which helps to learn effective policies instead of\nunnecessary ones. $\\text{B}^2\\text{-DiffuRL}$ is compatible with existing\noptimization algorithms. Extensive experiments demonstrate the effectiveness of\n$\\text{B}^2\\text{-DiffuRL}$ in improving prompt-image alignment and maintaining\ndiversity in generated images. The code for this work is available.\n","authors":["Zijing Hu","Fengda Zhang","Long Chen","Kun Kuang","Jiahui Li","Kaifeng Gao","Jun Xiao","Xin Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.11240v2.pdf","comment":"Accepted to CVPR 2025, add references"},{"id":"http://arxiv.org/abs/2503.20768v2","updated":"2025-03-27T02:16:06Z","published":"2025-03-26T17:52:30Z","title":"An Empirical Study of the Impact of Federated Learning on Machine\n  Learning Model Accuracy","summary":"  Federated Learning (FL) enables distributed ML model training on private user\ndata at the global scale. Despite the potential of FL demonstrated in many\ndomains, an in-depth view of its impact on model accuracy remains unclear. In\nthis paper, we investigate, systematically, how this learning paradigm can\naffect the accuracy of state-of-the-art ML models for a variety of ML tasks. We\npresent an empirical study that involves various data types: text, image,\naudio, and video, and FL configuration knobs: data distribution, FL scale,\nclient sampling, and local and global computations. Our experiments are\nconducted in a unified FL framework to achieve high fidelity, with substantial\nhuman efforts and resource investments. Based on the results, we perform a\nquantitative analysis of the impact of FL, and highlight challenging scenarios\nwhere applying FL degrades the accuracy of the model drastically and identify\ncases where the impact is negligible. The detailed and extensive findings can\nbenefit practical deployments and future development of FL.\n","authors":["Haotian Yang","Zhuoran Wang","Benson Chou","Sophie Xu","Hao Wang","Jingxian Wang","Qizhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21084v1","updated":"2025-03-27T01:59:24Z","published":"2025-03-27T01:59:24Z","title":"Geographical hotspot prediction based on point cloud-voxel-community\n  partition clustering","summary":"  Existing solutions to the hotspot prediction problem in the field of\ngeographic information remain at a relatively preliminary stage. This study\npresents a novel approach for detecting and predicting geographical hotspots,\nutilizing point cloud-voxel-community partition clustering. By analyzing\nhigh-dimensional data, we represent spatial information through point clouds,\nwhich are then subdivided into multiple voxels to enhance analytical\nefficiency. Our method identifies spatial voxels with similar characteristics\nthrough community partitioning, thereby revealing underlying patterns in\nhotspot distributions. Experimental results indicate that when applied to a\ndataset of archaeological sites in Turkey, our approach achieves a 19.31%\nincrease in processing speed, with an accuracy loss of merely 6%, outperforming\ntraditional clustering methods. This method not only provides a fresh\nperspective for hotspot prediction but also serves as an effective tool for\nhigh-dimensional data analysis.\n","authors":["Yan Tang"],"pdf_url":"https://arxiv.org/pdf/2503.21084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10672v3","updated":"2025-03-27T01:42:03Z","published":"2024-08-20T09:17:11Z","title":"Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization","summary":"  Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that\nmeta-trained neural networks can effectively guide the design of black-box\noptimizers, significantly reducing the need for expert tuning and delivering\nrobust performance across complex problem distributions. Despite their success,\na paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape\nAnalysis features to inform the meta-level agent about the low-level\noptimization progress. To address the gap, this paper proposes Neural\nExploratory Landscape Analysis (NeurELA), a novel framework that dynamically\nprofiles landscape features through a two-stage, attention-based neural\nnetwork, executed in an entirely end-to-end fashion. NeurELA is pre-trained\nover a variety of MetaBBO algorithms using a multi-task neuroevolution\nstrategy. Extensive experiments show that NeurELA achieves consistently\nsuperior performance when integrated into different and even unseen MetaBBO\ntasks and can be efficiently fine-tuned for further performance boost. This\nadvancement marks a pivotal step in making MetaBBO algorithms more autonomous\nand broadly applicable. The source code of NeurELA can be accessed at\nhttps://github.com/GMC-DRL/Neur-ELA.\n","authors":["Zeyuan Ma","Jiacheng Chen","Hongshu Guo","Yue-Jiao Gong"],"pdf_url":"https://arxiv.org/pdf/2408.10672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16678v2","updated":"2025-03-27T01:36:37Z","published":"2025-03-20T19:52:26Z","title":"QCPINN: Quantum Classical Physics-Informed Neural Networks for Solving\n  PDEs","summary":"  Physics-informed neural networks (PINNs) have emerged as promising methods\nfor solving partial differential equations (PDEs) by embedding physical laws\ninto neural architectures. However, these classical approaches often require\nlarge number of parameters for solving complex problems or achieving reasonable\naccuracy. We investigate whether quantum-enhanced architectures can achieve\ncomparable performance while significantly reducing model complexity. We\npropose a quantum-classical physics-informed neural network (QCPINN) combining\nquantum and classical components to solve PDEs with fewer parameters while\nmaintaining comparable accuracy and training convergence. Our approach\nsystematically evaluates two quantum circuit paradigms (e.g.,\ncontinuous-variable (CV) and discrete-variable (DV)) implementations with four\ncircuit topologies (e.g., alternate, cascade, cross-mesh, and layered), two\nembedding schemes (e.g., amplitude and angle) on five benchmark PDEs (e.g.,\nHelmholtz, lid-driven cavity, wave, Klein-Gordon, and convection-diffusion\nequations). Results demonstrate that QCPINNs achieve comparable accuracy to\nclassical PINNs while requiring approximately 10% trainable parameters across\ndifferent PDEs, and resulting in a further 40% reduction in relative L2 error\nfor the convection-diffusion equation. DV-based circuits with angle embedding\nand cascade configurations consistently exhibited enhanced convergence\nstability across all problem types. Our finding establishes parameter\nefficiency as a quantifiable quantum advantage in physics-informed machine\nlearning. By significantly reducing model complexity while maintaining solution\nquality, QCPINNs represent a potential direction for overcoming computational\nbottlenecks in scientific computing applications where traditional approaches\nrequire large parameter spaces.\n","authors":["Afrah Farea","Saiful Khan","Mustafa Serdar Celebi"],"pdf_url":"https://arxiv.org/pdf/2503.16678v2.pdf","comment":null}]},"2025-03-26T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2503.21033v1","updated":"2025-03-26T22:48:17Z","published":"2025-03-26T22:48:17Z","title":"Scalability Evaluation of HPC Multi-GPU Training for ECG-based LLMs","summary":"  Training large language models requires extensive processing, made possible\nby many high-performance computing resources. This study compares multi-node\nand multi-GPU environments for training large language models of\nelectrocardiograms. It provides a detailed mapping of current frameworks for\ndistributed deep learning in multinode and multi-GPU settings, including\nHorovod from Uber, DeepSpeed from Microsoft, and the built-in distributed\ncapabilities of PyTorch and TensorFlow. We compare various multi-GPU setups for\ndifferent dataset configurations, utilizing multiple HPC nodes independently\nand focusing on scalability, speedup, efficiency, and overhead. The analysis\nleverages HPC infrastructure with SLURM, Apptainer (Singularity) containers,\nCUDA, PyTorch, and shell scripts to support training workflows and automation.\nWe achieved a sub-linear speedup when scaling the number of GPUs, with values\nof 1.6x for two and 1.9x for four.\n","authors":["Dimitar Mileski","Nikola Petrovski","Marjan Gusev"],"pdf_url":"https://arxiv.org/pdf/2503.21033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21016v1","updated":"2025-03-26T22:04:41Z","published":"2025-03-26T22:04:41Z","title":"History-Independent Concurrent Hash Tables","summary":"  A history-independent data structure does not reveal the history of\noperations applied to it, only its current logical state, even if its internal\nstate is examined. This paper studies history-independent concurrent\ndictionaries, in particular, hash tables, and establishes inherent bounds on\ntheir space requirements.\n  This paper shows that there is a lock-free history-independent concurrent\nhash table, in which each memory cell stores two elements and two bits, based\non Robin Hood hashing. Our implementation is linearizable, and uses the shared\nmemory primitive LL/SC. The expected amortized step complexity of the hash\ntable is $O(c)$, where $c$ is an upper bound on the number of concurrent\noperations that access the same element, assuming the hash table is not\noverpopulated. We complement this positive result by showing that even if we\nhave only two concurrent processes, no history-independent concurrent\ndictionary that supports sets of any size, with wait-free membership queries\nand obstruction-free insertions and deletions, can store only two elements of\nthe set and a constant number of bits in each memory cell. This holds even if\nthe step complexity of operations on the dictionary is unbounded.\n","authors":["Hagit Attiya","Michael A. Bender","Martín Farach-Colton","Rotem Oshman","Noa Schiller"],"pdf_url":"https://arxiv.org/pdf/2503.21016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21013v1","updated":"2025-03-26T22:01:49Z","published":"2025-03-26T22:01:49Z","title":"AllReduce Scheduling with Hierarchical Deep Reinforcement Learning","summary":"  AllReduce is a technique in distributed computing which saw use in many\ncritical applications of deep learning. Existing methods of AllReduce\nscheduling oftentimes lack flexibility due to being topology-specific or\nrelying on extensive handcrafted designs that require domain-specific\nknowledge. In this work, we aim to alleviate this inflexibility by proposing a\ndeep-reinforcement-learning (DRL)-based pipeline that can generate AllReduce\nscheduling for various network topologies without topology-specific design\nfeatures. The flow scheduling module of this pipeline consists of two\nhierarchically-structured DRL policies that work cooperatively to find optimal\nscheduling. We showcase the performance of our method compared to the baseline\nmethods on three topologies: BCube, DCell, and Jellyfish. Finally, we\ncontributed a Python-based simulation environment simulating AllReduce\nscheduling on these network topologies.\n","authors":["Yufan Wei","Mickel Liu","Wenfei Wu"],"pdf_url":"https://arxiv.org/pdf/2503.21013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20884v1","updated":"2025-03-26T18:00:56Z","published":"2025-03-26T18:00:56Z","title":"Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense\n  Framework","summary":"  Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.\n","authors":["Usama Zafar","André Teixeira","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2503.20884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20868v1","updated":"2025-03-26T18:00:01Z","published":"2025-03-26T18:00:01Z","title":"Advances in Semantic Patching for HPC-oriented Refactorings with\n  Coccinelle","summary":"  Currently, the most energy-efficient hardware platforms for floating\npoint-intensive calculations (also known as High Performance Computing, or HPC)\nare graphical processing units (GPUs). However, porting existing scientific\ncodes to GPUs can be far from trivial. This article summarizes our recent\nadvances in enabling machine-assisted, HPC-oriented refactorings with reference\nto existing APIs and programming idioms available in C and C++. The tool we are\nextending and using for the purpose is called Coccinelle. An important workflow\nwe aim to support is that of writing and maintaining tersely written\napplication code, while deferring circumstantial, ad-hoc, performance-related\nchanges to specific, separate rules called semantic patches. GPUs currently\noffer very limited debugging facilities. The approach we are developing aims at\npreserving intelligibility, longevity, and relatedly, debuggability of existing\ncode on CPUs, while at the same time enabling HPC-oriented code evolutions such\nas introducing support for GPUs, in a scriptable and possibly parametric\nmanner. This article sketches a number of self-contained use cases, including\nfurther HPC-oriented cases which are independent from GPUs.\n","authors":["Michele Martone","Julia Lawall"],"pdf_url":"https://arxiv.org/pdf/2503.20868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20591v1","updated":"2025-03-26T14:41:52Z","published":"2025-03-26T14:41:52Z","title":"NotebookOS: A Notebook Operating System for Interactive Training with\n  On-Demand GPUs","summary":"  Interactive notebook programming is universal in modern ML (machine learning)\nand AI (artificial intelligence) workflows. Notebook software like Jupyter and\nGoogle Colab provides a user-friendly, interactive, web-based programming\ninterface and is widely used across science and engineering domains. A dominant\napplication of production notebook workloads is interactive deep learning\ntraining (IDLT). To guarantee high interactivity, modern notebook platforms\ntypically reserve GPU resources within actively running notebook sessions.\nThese notebook sessions are long-running but exhibit intermittent and sporadic\nGPU usage. Consequently, during most of their lifetimes, notebook sessions do\nnot use the reserved GPUs, resulting in extremely low GPU utilization and\nprohibitively high cost.\n  In this paper, we introduce NotebookOS, a GPU-efficient notebook platform\ndesigned to meet the unique requirements of IDLT. NotebookOS uses a replicated\nnotebook kernel design, where each kernel consists of three replicas\ndistributed across separate GPU servers and synchronized via Raft. To optimize\nGPU utilization, NotebookOS oversubscribes server resources via kernel\nreplication to leverage the relatively high task inter-arrival times in IDLT\nworkloads. By dynamically allocating GPUs to kernel replicas only while they\nare actively executing notebook cells, NotebookOS maximizes the likelihood of\nimmediate and interactive training upon notebook notebook-cell task submission.\nNotebookOS also migrates kernel replicas and automatically scales the GPU\ncluster under overload conditions. We evaluate NotebookOS extensively using\nproduction notebook workloads. Evaluation results show that NotebookOS saves\n1,187+ GPU hours over a 17.5-hour real-world IDLT workload while greatly\nenhancing interactivity.\n","authors":["Benjamin Carver","Jingyuan Zhang","Haoliang Wang","Kanak Mahadik","Yue Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.20591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12236v2","updated":"2025-03-26T14:25:26Z","published":"2024-05-15T23:44:06Z","title":"Fully Distributed Fog Load Balancing with Multi-Agent Reinforcement\n  Learning","summary":"  Real-time Internet of Things (IoT) applications require real-time support to\nhandle the ever-growing demand for computing resources to process IoT\nworkloads. Fog Computing provides high availability of such resources in a\ndistributed manner. However, these resources must be efficiently managed to\ndistribute unpredictable traffic demands among heterogeneous Fog resources.\nThis paper proposes a fully distributed load-balancing solution with\nMulti-Agent Reinforcement Learning (MARL) that intelligently distributes IoT\nworkloads to optimize the waiting time while providing fair resource\nutilization in the Fog network. These agents use transfer learning for\nlife-long self-adaptation to dynamic changes in the environment. By leveraging\ndistributed decision-making, MARL agents effectively minimize the waiting time\ncompared to a single centralized agent solution and other baselines, enhancing\nend-to-end execution delay. Besides performance gain, a fully distributed\nsolution allows for a global-scale implementation where agents can work\nindependently in small collaboration regions, leveraging nearby local\nresources. Furthermore, we analyze the impact of a realistic frequency to\nobserve the state of the environment, unlike the unrealistic common assumption\nin the literature of having observations readily available in real-time for\nevery required action. The findings highlight the trade-off between realism and\nperformance using an interval-based Gossip-based multi-casting protocol against\nassuming real-time observation availability for every generated workload.\n","authors":["Maad Ebrahim","Abdelhakim Hafid"],"pdf_url":"https://arxiv.org/pdf/2405.12236v2.pdf","comment":"Submitted to IEEE TNSM with 14 pages, 11 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2407.13996v3","updated":"2025-03-26T13:59:53Z","published":"2024-07-19T03:01:32Z","title":"SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs","summary":"  Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.\n","authors":["Yongkang Zhang","Haoxuan Yu","Chenxia Han","Cheng Wang","Baotong Lu","Yunzhe Li","Zhifeng Jiang","Yang Li","Xiaowen Chu","Huaicheng Li"],"pdf_url":"https://arxiv.org/pdf/2407.13996v3.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2503.20552v1","updated":"2025-03-26T13:48:35Z","published":"2025-03-26T13:48:35Z","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation","summary":"  In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems.\n","authors":["Yunkai Liang","Zhangyu Chen","Pengfei Zuo","Zhi Zhou","Xu Chen","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2503.20552v1.pdf","comment":"14 pages, 18 figures"},{"id":"http://arxiv.org/abs/2503.02550v3","updated":"2025-03-26T13:27:14Z","published":"2025-03-04T12:21:28Z","title":"SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling","summary":"  Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a Speculative Inference Filling method to exploit idle GPU resources. It\ncollocates each primary training instance with additional inference instances\non the same GPU, detects the training bubbles and adaptively fills with online\nor offline inference workloads. Our results show that SpecInF can effectively\nenhance GPU utilization under mainstream parallel training modes, delivering\nadditional up to 14$\\times$ offline inference throughputs than TGS and 67\\%\nreduction in online inference p95 latency than MPS, while guaranteeing\ncollocated training throughput.\n","authors":["Cunchi Lv","Xiao Shi","Dong Liang","Wenting Tan","Xiaofang Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.02550v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20507v1","updated":"2025-03-26T12:47:52Z","published":"2025-03-26T12:47:52Z","title":"Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems","summary":"  Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nsub-optimal HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique for HSS that optimizes both data-placement and data-migration\npolicies to fully exploit the potential of an HSS. We propose Harmonia, a\nmulti-agent reinforcement learning (RL)-based data-management technique that\nemploys two light-weight autonomous RL agents, a data-placement agent and a\ndata-migration agent, which adapt their policies for the current workload and\nHSS configuration, and coordinate with each other to improve overall HSS\nperformance. We evaluate Harmonia on a real HSS with up to four heterogeneous\nstorage devices with diverse characteristics. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia (1) outperforms the\nbest-performing prior approach by 49.5% (31.7%), (2) bridges the performance\ngap between the best-performing prior work and Oracle by 64.2% (64.3%). On an\nHSS with three (four) devices, Harmonia outperforms the best-performing prior\nwork by 37.0% (42.0%). Harmonia's performance benefits come with low latency\n(240ns for inference) and storage overheads (206 KiB for both RL agents\ntogether). We plan to open-source Harmonia's implementation to aid future\nresearch on HSS.\n","authors":["Rakesh Nadig","Vamanan Arulchelvan","Rahul Bera","Taha Shahroodi","Gagandeep Singh","Mohammad Sadrosadati","Jisung Park","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2503.20507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20497v1","updated":"2025-03-26T12:35:42Z","published":"2025-03-26T12:35:42Z","title":"A Blockchain-Enabled Framework for Storage and Retrieval of Social Data","summary":"  The increasing availability of data from diverse sources, including trusted\nentities such as governments, as well as untrusted crowd-sourced contributors,\ndemands a secure and trustworthy environment for storage and retrieval.\nBlockchain, as a distributed and immutable ledger, offers a promising solution\nto address these challenges. This short paper studies the feasibility of a\nblockchain-based framework for secure data storage and retrieval across trusted\nand untrusted sources, focusing on provenance, storage mechanisms, and smart\ncontract security. Through initial experiments using Hyper Ledger Fabric (HLF),\nwe evaluate the storage efficiency, scalability, and feasibility of the\nproposed approach. This study serves as a motivation for future research to\ndevelop a comprehensive blockchain-based storage and retrieval framework.\n","authors":["Aishwarya Parab","Prakhar Pradhan","Yogesh Simmhan","Arnab K. Paul"],"pdf_url":"https://arxiv.org/pdf/2503.20497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20344v1","updated":"2025-03-26T09:17:40Z","published":"2025-03-26T09:17:40Z","title":"GeoNimbus: A serverless framework to build earth observation and\n  environmental services","summary":"  Cloud computing has become a popular solution for organizations implementing\nEarth Observation Systems (EOS). However, this produces a dependency on\nprovider resources. Moreover, managing and executing tasks and data in these\nenvironments are challenges that commonly arise when building an EOS. This\npaper presents GeoNimbus, a serverless framework for composing and deploying\nspatio-temporal EOS on multiple infrastructures, e.g., on-premise resources and\npublic or private clouds. This framework organizes EOS tasks as functions and\nautomatically manages their deployment, invocation, scalability, and monitoring\nin the cloud. GeoNimbus framework enables organizations to reuse and share\navailable functions to compose multiple EOS. We use this framework to implement\nEOS as a service for conducting a case study focused on measuring water\nresource changes in a lake in the south of Mexico. The experimental evaluation\nrevealed the feasibility and efficiency of using GeoNimbus to build different\nearth observation studies.\n","authors":["Dante D. Sánchez-Gallegos","Diana Carrizales-Espinoza","Alejandro Zequeira","Catherine Torres-Charles","J. L. Gonzalez-Compean","Jesus Carretero"],"pdf_url":"https://arxiv.org/pdf/2503.20344v1.pdf","comment":"12 pages, 10 images. Presented at the 1st workshop about\n  High-Performance e-Science in the EuroPar2024 conference"},{"id":"http://arxiv.org/abs/2205.06073v4","updated":"2025-03-26T06:39:33Z","published":"2022-05-12T13:11:33Z","title":"Consensus Capacity of Noisy Broadcast Channels","summary":"  We study communication with consensus over a broadcast channel - the\nreceivers reliably decode the sender's message when the sender is honest, and\ntheir decoder outputs agree even if the sender acts maliciously. We\ncharacterize the broadcast channels which permit this byzantine consensus and\ndetermine their capacity. We show that communication with consensus is possible\nonly when the broadcast channel has embedded in it a natural ''common channel''\nwhose output both receivers can unambiguously determine from their own channel\noutputs. Interestingly, in general, the consensus capacity may be larger than\nthe point-to-point capacity of the common channel, i.e., while decoding, the\nreceivers may make use of parts of their output signals on which they may not\nhave consensus provided there are some parts (namely, the common channel\noutput) on which they can agree.\n","authors":["Neha Sangwan","Varun Narayanan","Vinod M. Prabhakaran"],"pdf_url":"https://arxiv.org/pdf/2205.06073v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20263v1","updated":"2025-03-26T06:09:55Z","published":"2025-03-26T06:09:55Z","title":"L4: Diagnosing Large-scale LLM Training Failures via Automated Log\n  Analysis","summary":"  As Large Language Models (LLMs) show their capabilities across various\napplications, training customized LLMs has become essential for modern\nenterprises. However, due to the complexity of LLM training, which requires\nmassive computational resources and extensive training time, failures are\ninevitable during the training process. These failures result in considerable\nwaste of resource and time, highlighting the critical need for effective and\nefficient failure diagnosis to reduce the cost of LLM training.\n  In this paper, we present the first empirical study on the failure reports of\n428 LLM training failures in our production Platform-X between May 2023 and\nApril 2024. Our study reveals that hardware and user faults are the predominant\nroot causes, and current diagnosis processes rely heavily on training logs.\nUnfortunately, existing log-based diagnostic methods fall short in handling LLM\ntraining logs. Considering the unique features of LLM training, we identify\nthree distinct patterns of LLM training logs: cross-job, spatial, and temporal\npatterns. We then introduce our Log-based Large-scale LLM training failure\ndiagnosis framework, L4, which can automatically extract failure-indicating\ninformation (i.e., log events, nodes, stages, and iterations) from extensive\ntraining logs, thereby reducing manual effort and facilitating failure\nrecovery. Experimental results on real-world datasets show that L4 outperforms\nexisting approaches in identifying failure-indicating logs and localizing\nfaulty nodes. Furthermore, L4 has been applied in Platform-X and demonstrated\nits effectiveness in enabling accurate and efficient failure diagnosis.\n","authors":["Zhihan Jiang","Junjie Huang","Zhuangbin Chen","Yichen Li","Guangba Yu","Cong Feng","Yongqiang Yang","Zengyin Yang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2503.20263v1.pdf","comment":"To appear in companion proceedings of the 33rd ACM International\n  Conference on the Foundations of Software Engineering (FSE'25). 13 pages"},{"id":"http://arxiv.org/abs/2402.09714v3","updated":"2025-03-26T05:42:20Z","published":"2024-02-15T05:15:22Z","title":"An Accelerated Distributed Stochastic Gradient Method with Momentum","summary":"  In this paper, we introduce an accelerated distributed stochastic gradient\nmethod with momentum for solving the distributed optimization problem, where a\ngroup of $n$ agents collaboratively minimize the average of the local objective\nfunctions over a connected network. The method, termed ``Distributed Stochastic\nMomentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the\nmomentum tracking technique as well as the Loopless Chebyshev Acceleration\n(LCA) method. We show that DSMT can asymptotically achieve comparable\nconvergence rates as centralized stochastic gradient descent (SGD) method under\na general variance condition regarding the stochastic gradients. Moreover, the\nnumber of iterations (transient times) required for DSMT to achieve such rates\nbehaves as $\\mathcal{O}(n^{5/3}/(1-\\lambda))$ for minimizing general smooth\nobjective functions, and $\\mathcal{O}(\\sqrt{n/(1-\\lambda)})$ under the\nPolyak-{\\L}ojasiewicz (PL) condition. Here, the term $1-\\lambda$ denotes the\nspectral gap of the mixing matrix related to the underlying network topology.\nNotably, the obtained results do not rely on multiple inter-node communications\nor stochastic gradient accumulation per iteration, and the transient times are\nthe shortest under the setting to the best of our knowledge.\n","authors":["Kun Huang","Shi Pu","Angelia Nedić"],"pdf_url":"https://arxiv.org/pdf/2402.09714v3.pdf","comment":"45 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.20191v1","updated":"2025-03-26T03:33:01Z","published":"2025-03-26T03:33:01Z","title":"Maya: Optimizing Deep Learning Training Workloads using Emulated Virtual\n  Accelerators","summary":"  Training large foundation models costs hundreds of millions of dollars,\nmaking deployment optimization critical. Current approaches require machine\nlearning engineers to manually craft training recipes through error-prone\ntrial-and-error on expensive compute clusters. To enable efficient exploration\nof training configurations, researchers have developed performance modeling\nsystems. However, these systems force users to translate their workloads into\ncustom specification languages, introducing a fundamental semantic gap between\nthe actual workload and its representation. This gap creates an inherent\ntradeoff: systems must either support a narrow set of workloads to maintain\nusability, require complex specifications that limit practical adoption, or\ncompromise prediction accuracy with simplified models.\n  We present Maya, a performance modeling system that eliminates these\ntradeoffs through transparent device emulation. By operating at the narrow\ninterface between training frameworks and accelerator devices, Maya can capture\ncomplete workload behavior without requiring code modifications or\ntranslations. Maya intercepts device API calls from unmodified training code to\ndirectly observe low-level operations, enabling accurate performance prediction\nwhile maintaining both ease of use and generality. Our evaluation shows Maya\nachieves less than 5% prediction error across diverse models and optimization\nstrategies, identifying configurations that reduce training costs by up to 56%\ncompared to existing approaches.\n","authors":["Srihas Yarlagadda","Amey Agrawal","Elton Pinto","Hakesh Darapaneni","Mitali Meratwal","Shivam Mittal","Pranavi Bajjuri","Srinivas Sridharan","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2503.20191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20166v1","updated":"2025-03-26T02:45:19Z","published":"2025-03-26T02:45:19Z","title":"AIGC-assisted Federated Learning for Edge Intelligence: Architecture\n  Design, Research Challenges and Future Directions","summary":"  Federated learning (FL) can fully leverage large-scale terminal data while\nensuring privacy and security, and is considered as a distributed alternative\nfor the centralized machine learning. However, the issue of data heterogeneity\nposes limitations on FL's performance. To address this challenge, artificial\nintelligence-generated content (AIGC) which is an innovative data synthesis\ntechnique emerges as one potential solution. In this article, we first provide\nan overview of the system architecture, performance metrics, and challenges\nassociated with AIGC-assistant FL system design. We then propose the Generative\nfederated learning (GenFL) architecture and present its workflow, including the\ndesign of aggregation and weight policy. Finally, using the CIFAR10 and\nCIFAR100 datasets, we employ diffusion models to generate dataset and improve\nFL performance. Experiments conducted under various non-independent and\nidentically distributed (non-IID) data distributions demonstrate the\neffectiveness of GenFL on overcoming the bottlenecks in FL caused by data\nheterogeneity. Open research directions in the research of AIGC-assisted FL are\nalso discussed.\n","authors":["Xianke Qiang","Zheng Chang","Ying-Chang Liang"],"pdf_url":"https://arxiv.org/pdf/2503.20166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17264v2","updated":"2025-03-26T01:58:40Z","published":"2024-09-25T18:21:05Z","title":"Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations","summary":"  As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.\n","authors":["Amey Agrawal","Haoran Qiu","Junda Chen","Íñigo Goiri","Ramachandran Ramjee","Chaojie Zhang","Alexey Tumanov","Esha Choukse"],"pdf_url":"https://arxiv.org/pdf/2409.17264v2.pdf","comment":null}],"Some new methods":[{"id":"http://arxiv.org/abs/2503.21036v1","updated":"2025-03-26T23:02:00Z","published":"2025-03-26T23:02:00Z","title":"The Art of Tool Interface Design","summary":"  We present an agentic framework, Thinker, which achieves state of art\nperformance in challenging reasoning tasks for realistic customer service\nscenarios that involve complex business logic and human interactions via long\nhorizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success\nrate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success\nrate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker\neffectively closes the gap in reasoning capabilities between the base models by\nintroducing proper structure.\n  The key features of the Thinker framework are: (1) State-Machine Augmented\nGeneration (SMAG), which represents business logic as state machines and the\nLLM uses state machines as tools. (2) Delegation of tasks from the main\nreasoning loop to LLM-powered tools. (3) Adaptive context management.\n  Our prompting-only solution achieves signficant gains, while still\nmaintaining a standard agentic architecture with a ReAct style reasoning loop.\nThe key is to innovate on the tool interface design, as exemplified by SMAG and\nthe LLM-powered tools.\n","authors":["Yunnan Wu","Paul Chen","Deshank Baranwal","Jinlong Zhou","Jian Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.21036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10581v2","updated":"2025-03-26T22:45:31Z","published":"2025-02-14T22:21:56Z","title":"Do We Need to Verify Step by Step? Rethinking Process Supervision from a\n  Theoretical Perspective","summary":"  As large language models have evolved, it has become crucial to distinguish\nbetween process supervision and outcome supervision -- two key reinforcement\nlearning approaches to complex reasoning tasks. While process supervision\noffers intuitive advantages for long-term credit assignment, the precise\nrelationship between these paradigms has remained an open question.\nConventional wisdom suggests that outcome supervision is fundamentally more\nchallenging due to the trajectory-level coverage problem, leading to\nsignificant investment in collecting fine-grained process supervision data.\n  In this paper, we take steps towards resolving this debate. Our main theorem\nshows that, under standard data coverage assumptions, reinforcement learning\nthrough outcome supervision is no more statistically difficult than through\nprocess supervision, up to polynomial factors in horizon. At the core of this\nresult lies the novel Change of Trajectory Measure Lemma -- a technical tool\nthat bridges return-based trajectory measure and step-level distribution shift.\nFurthermore, for settings with access to a verifier or a rollout capability, we\nprove that any policy's advantage function can serve as an optimal process\nreward model, providing a direct connection between outcome and process\nsupervision. These findings suggest that the empirically observed performance\ngap -- if any -- between outcome and process supervision likely stems from\nalgorithmic limitations rather than inherent statistical difficulties,\npotentially transforming how we approach data collection and algorithm design\nfor reinforcement learning.\n","authors":["Zeyu Jia","Alexander Rakhlin","Tengyang Xie"],"pdf_url":"https://arxiv.org/pdf/2502.10581v2.pdf","comment":null}]},"2025-03-25T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2503.20117v1","updated":"2025-03-25T23:54:23Z","published":"2025-03-25T23:54:23Z","title":"From Interpretation to Correction: A Decentralized Optimization\n  Framework for Exact Convergence in Federated Learning","summary":"  This work introduces a novel decentralized framework to interpret federated\nlearning (FL) and, consequently, correct the biases introduced by arbitrary\nclient participation and data heterogeneity, which are two typical traits in\npractical FL. Specifically, we first reformulate the core processes of FedAvg -\nclient participation, local updating, and model aggregation - as stochastic\nmatrix multiplications. This reformulation allows us to interpret FedAvg as a\ndecentralized algorithm. Leveraging the decentralized optimization framework,\nwe are able to provide a concise analysis to quantify the impact of arbitrary\nclient participation and data heterogeneity on FedAvg's convergence point. This\ninsight motivates the development of Federated Optimization with Exact\nConvergence via Push-pull Strategy (FOCUS), a novel algorithm inspired by the\ndecentralized algorithm that eliminates these biases and achieves exact\nconvergence without requiring the bounded heterogeneity assumption.\nFurthermore, we theoretically prove that FOCUS exhibits linear convergence\n(exponential decay) for both strongly convex and non-convex functions\nsatisfying the Polyak-Lojasiewicz condition, regardless of the arbitrary nature\nof client participation.\n","authors":["Bicheng Ying","Zhe Li","Haibo Yang"],"pdf_url":"https://arxiv.org/pdf/2503.20117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16984v6","updated":"2025-03-25T22:14:01Z","published":"2023-11-28T17:35:38Z","title":"FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings","summary":"  External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise gone unnoticed. By sharing\nour code, we hope FedECA will foster the creation of federated research\nnetworks and thus accelerate drug development.\n","authors":["Jean Ogier du Terrail","Quentin Klopfenstein","Honghao Li","Imke Mayer","Nicolas Loiseau","Mohammad Hallal","Michael Debouver","Thibault Camalon","Thibault Fouqueray","Jorge Arellano Castro","Zahia Yanes","Laetitia Dahan","Julien Taïeb","Pierre Laurent-Puig","Jean-Baptiste Bachet","Shulin Zhao","Remy Nicolle","Jérome Cros","Daniel Gonzalez","Robert Carreras-Torres","Adelaida Garcia Velasco","Kawther Abdilleh","Sudheer Doss","Félix Balazard","Mathieu Andreux"],"pdf_url":"https://arxiv.org/pdf/2311.16984v6.pdf","comment":"code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 fixed, many experiments on real data added +\n  fix in YODA experiments using imputed data instead of raw data (v3->v4) +\n  affiliations fix + more precise wording for acknowledgments, real-world\n  experiment results fixed by excluding data with bias + text polished (v5->v6)"},{"id":"http://arxiv.org/abs/2503.20079v1","updated":"2025-03-25T21:32:23Z","published":"2025-03-25T21:32:23Z","title":"ARGO-SLSA: Software Supply Chain Security in Argo Workflows","summary":"  Distributed systems widely adopt microservice architecture to handle growing\ncomplexity and scale. This approach breaks applications into independent,\nloosely coupled services. Kubernetes has become the de facto standard for\nmanaging microservices, and automating complex, multi-step workflows is a\ncommon requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine\nfor managing these workflows in an automated fashion. These workflows generate\nartifacts such as executables, logs, container images, and packages, which\noften require proper management through software supply chain security.\nHowever, Argo Workflows does not include built-in functionality for frameworks\nlike Supply-chain Levels for Software Artifacts (SLSA), which is essential for\nensuring artifact integrity, traceability, and security. This gap compels\npractitioners to rely on external tools to meet software supply chain security\nstandards. In response, this paper proposes a Kubernetes-native controller\nbuilt on top of existing open-source Argo Workflows to enhance artifact\nsecurity. By generating cryptographic signing and provenance attestations, the\ncontroller enables Argo Workflows to comply with SLSA standards. We demonstrate\nthat implementations can provide such cryptographic signing and provenance\nattestations for artifacts produced by the controller, allowing software\nartifacts built with Argo Workflows to adhere to SLSA requirements. The\nproposed validation model evaluates the proof of concept of the controller,\nincluding its ability to reconcile workflows, detect pods associated with\nworkflow nodes, operate without disrupting existing operations, enforce\nintegrity, and monitor software artifacts.\n","authors":["Mohomed Thariq","Indrajith Ekanayake"],"pdf_url":"https://arxiv.org/pdf/2503.20079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19886v1","updated":"2025-03-25T17:50:54Z","published":"2025-03-25T17:50:54Z","title":"RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized\n  Federated Learning","summary":"  We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction.\n","authors":["Abdulmoneam Ali","Ahmed Arafa"],"pdf_url":"https://arxiv.org/pdf/2503.19886v1.pdf","comment":"to appear in the 2025 IEEE International Conference on Communications"},{"id":"http://arxiv.org/abs/2503.19857v1","updated":"2025-03-25T17:23:34Z","published":"2025-03-25T17:23:34Z","title":"Comparing the Run-time Behavior of Modern PDES Engines on Alternative\n  Hardware Architectures","summary":"  The current trend of technology has brought parallel machines equipped with\nmultiple processors and multiple memory sockets to be available off-the-shelf\n-- or via renting through Iaas Clouds -- at reasonable costs. This has opened\nthe possibility of natively supporting HPC in diffused realities, like industry\nor academic labs. At the same time, the Parallel Discrete Event Simulation\n(PDES) area has given rise to attractive simulation engines, designed with\norientation to high performance and scalability, also targeting differentiated\nexploitation of the specific support offered by the underlying hardware. In\nthis article, we present an experimental study where we deploy two\nlast-generation open-source PDES platforms -- one optimistic (USE) and one\nconservative (PARSIR) -- on top of two significantly different hardware\nchipsets based on either {\\sf x86} CISC or {\\sf powerPC} RISC technology, both\noffering multiple Non-Uniform-Memory-Access (NUMA) nodes and multiple tens of\ncores and hardware-threads (logical CPUs). Also, we consider real-world\nsimulation models configured in a variety of different manners in order to\ninvestigate the actual execution profile of the PDES engines on the two\ndistinct hardware platforms. Our objective is the one of providing insights on\ncurrent performance trends, which can support decisions in terms of both\nstrategies -- for software platforms to adopt -- and investments -- in terms of\nhardware platforms -- in the area of discrete event simulation.\n","authors":["Romolo Marotta","Francesco Quaglia"],"pdf_url":"https://arxiv.org/pdf/2503.19857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05215v3","updated":"2025-03-25T14:48:01Z","published":"2023-12-08T18:07:05Z","title":"DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs","summary":"  Fine-tuning large language models (LLMs) greatly improves model quality for\ndownstream tasks. However, serving many fine-tuned LLMs concurrently is\nchallenging due to the sporadic, bursty, and varying request patterns of\ndifferent LLMs. To bridge this gap, we present DeltaZip, an LLM serving system\nthat efficiently serves multiple full-parameter fine-tuned models concurrently\nby aggressively compressing model deltas by up to 10x while maintaining high\nmodel quality. The key insight behind this design is that fine-tuning results\nin small-magnitude changes to the pre-trained model. By co-designing the\nserving system with the compression algorithm, DeltaZip achieves 2x to 12x\nimprovement in throughput compared to the state-of-the-art systems.\n","authors":["Xiaozhe Yao","Qinghao Hu","Ana Klimovic"],"pdf_url":"https://arxiv.org/pdf/2312.05215v3.pdf","comment":"EuroSys 2025'"},{"id":"http://arxiv.org/abs/2503.19676v1","updated":"2025-03-25T14:02:07Z","published":"2025-03-25T14:02:07Z","title":"AIGC-assisted Federated Learning for Vehicular Edge Intelligence:\n  Vehicle Selection, Resource Allocation and Model Augmentation","summary":"  To leverage the vast amounts of onboard data while ensuring privacy and\nsecurity, federated learning (FL) is emerging as a promising technology for\nsupporting a wide range of vehicular applications. Although FL has great\npotential to improve the architecture of intelligent vehicular networks,\nchallenges arise due to vehicle mobility, wireless channel instability, and\ndata heterogeneity. To mitigate the issue of heterogeneous data across\nvehicles, artificial intelligence-generated content (AIGC) can be employed as\nan innovative data synthesis technique to enhance FL model performance. In this\npaper, we propose AIGC-assisted Federated Learning for Vehicular Edge\nIntelligence (GenFV). We further propose a weighted policy using the Earth\nMover's Distance (EMD) to quantify data distribution heterogeneity and\nintroduce a convergence analysis for GenFV. Subsequently, we analyze system\ndelay and formulate a mixed-integer nonlinear programming (MINLP) problem to\nminimize system delay. To solve this MINLP NP-hard problem, we propose a\ntwo-scale algorithm. At large communication scale, we implement label sharing\nand vehicle selection based on velocity and data heterogeneity. At the small\ncomputation scale, we optimally allocate bandwidth, transmission power and\namount of generated data. Extensive experiments show that GenFV significantly\nimproves the performance and robustness of FL in dynamic, resource-constrained\nenvironments, outperforming other schemes and confirming the effectiveness of\nour approach.\n","authors":["Xianke Qiang","Zheng Chang","Geyong Min"],"pdf_url":"https://arxiv.org/pdf/2503.19676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19671v1","updated":"2025-03-25T13:58:08Z","published":"2025-03-25T13:58:08Z","title":"A Tight Meta-theorem for LOCAL Certification of MSO$_2$ Properties\n  within Bounded Treewidth Graphs","summary":"  Distributed networks are prone to errors so verifying their output is\ncritical. Hence, we develop LOCAL certification protocols for graph properties\nin which nodes are given certificates that allow them to check whether their\nnetwork as a whole satisfies some fixed property while only communicating with\ntheir local network. Most known LOCAL certification protocols are specifically\ntailored to the problem they work on and cannot be translated more generally.\nThus we target general protocols that can certify any property expressible\nwithin a certain logical framework. We consider Monadic Second Order Logic\n(MSO$_2$), a powerful framework that can express properties such as\nnon-$k$-colorability, Hamiltonicity, and $H$-minor-freeness. Unfortunately, in\ngeneral, there are MSO$_2$-expressible properties that cannot be certified\nwithout huge certificates. For instance, non-3-colorability requires\ncertificates of size $\\Omega(n^2/\\log n)$ on general $n$-vertex graphs\n(G\\\"o\\\"os, Suomela 2016). Hence, we impose additional structural restrictions\non the graph.\n  We provide a LOCAL certification protocol for certifying any\nMSO$_2$-expressible property on graphs of bounded treewidth and, consequently,\na LOCAL certification protocol for certifying bounded treewidth. That is for\neach integer $k$ and each MSO$_2$-expressible property $\\Pi$ we give a LOCAL\nCertification protocol to certify that a graph satisfies $\\Pi$ and has\ntreewidth at most $k$ using certificates of size $\\mathcal{O}(\\log n)$ (which\nis asymptotically optimal). Our LOCAL certification protocol requires only one\nround of distributed communication, hence it is also proof-labeling scheme.\n  Our result improves upon work by Fraigniaud, Montealegre, Rapaport, and\nTodinca (Algorithmica 2024), Bousquet, Feuilloley, Pierron (PODC 2022), and the\nvery recent work of Baterisna and Chang.\n","authors":["Linda Cook","Eun Jung Kim","Tomáš Masařík"],"pdf_url":"https://arxiv.org/pdf/2503.19671v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.03480v3","updated":"2025-03-25T13:10:28Z","published":"2024-10-04T14:52:18Z","title":"SeBS-Flow: Benchmarking Serverless Cloud Function Workflows","summary":"  Serverless computing has emerged as a prominent paradigm, with a significant\nadoption rate among cloud customers. While this model offers advantages such as\nabstraction from the deployment and resource scheduling, it also poses\nlimitations in handling complex use cases due to the restricted nature of\nindividual functions. Serverless workflows address this limitation by\norchestrating multiple functions into a cohesive application. However, existing\nserverless workflow platforms exhibit significant differences in their\nprogramming models and infrastructure, making fair and consistent performance\nevaluations difficult in practice. To address this gap, we propose the first\nserverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic\nworkflow model that enables consistent benchmarking across various platforms.\nSeBS-Flow includes six real-world application benchmarks and four\nmicrobenchmarks representing different computational patterns. We conduct\ncomprehensive evaluations on three major cloud platforms, assessing\nperformance, cost, scalability, and runtime deviations. We make our benchmark\nsuite open-source, enabling rigorous and comparable evaluations of serverless\nworkflows over time.\n","authors":["Larissa Schmid","Marcin Copik","Alexandru Calotoiu","Laurin Brandner","Anne Koziolek","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2410.03480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04595v2","updated":"2025-03-25T11:59:46Z","published":"2025-03-06T16:38:07Z","title":"Boosting Blockchain Throughput: Parallel EVM Execution with Asynchronous\n  Storage for Reddio","summary":"  The increasing adoption of blockchain technology has led to a growing demand\nfor higher transaction throughput. Traditional blockchain platforms, such as\nEthereum, execute transactions sequentially within each block, limiting\nscalability. Parallel execution has been proposed to enhance performance, but\nexisting approaches either impose strict dependency annotations, rely on\nconservative static analysis, or suffer from high contention due to inefficient\nstate management. Moreover, even when transaction execution is parallelized at\nthe upper layer, storage operations remain a bottleneck due to sequential state\naccess and I/O amplification. In this paper, we propose Reddio, a batch-based\nparallel transaction execution framework with asynchronous storage. Reddio\nprocesses transactions in parallel while addressing the storage bottleneck\nthrough three key techniques: (i) direct state reading, which enables efficient\nstate access without traversing the Merkle Patricia Trie (MPT); (ii)\nasynchronous parallel node loading, which preloads trie nodes concurrently with\nexecution to reduce I/O overhead; and (iii) pipelined workflow, which decouples\nexecution, state reading, and storage updates into overlapping phases to\nmaximize hardware utilization.\n","authors":["Xiaodong Qi","Xinran Chen"," Asiy","Neil Han"],"pdf_url":"https://arxiv.org/pdf/2503.04595v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18543v2","updated":"2025-03-25T10:24:55Z","published":"2025-03-24T10:55:29Z","title":"Monte Cimone v2: Down the Road of RISC-V High-Performance Computers","summary":"  Many RISC-V platforms and SoCs have been announced in recent years targeting\nthe HPC sector, but only a few of them are commercially available and\nengineered to fit the HPC requirements. The Monte Cimone project targeted\nassessing their capabilities and maturity, aiming to make RISC-V a competitive\nchoice when building a datacenter. Nowadays, RV SoCs with vector extension,\nform factor and memory capacity suitable for HPC applications are available in\nthe market, but it is unclear how compilers and open-source libraries can take\nadvantage of its performance. In this paper, we describe the performance\nassessment of the upgrade of the Monte Cimone (MCv2) cluster with the Sophgo\nSG2042 processor's HPC operations. The upgrade increases the attained node's\nperformance by 127x on HPL DP FLOP/s and 69x on Stream Memory Bandwidth.\n","authors":["Emanuele Venieri","Simone Manoni","Giacomo Madella","Federico Ficarelli","Daniele Gregori","Daniele Cesarini","Luca Benini","Andrea Bartolini"],"pdf_url":"https://arxiv.org/pdf/2503.18543v2.pdf","comment":"Accepted at RISC-V Summit Europe 2025"},{"id":"http://arxiv.org/abs/2407.00088v2","updated":"2025-03-25T09:27:16Z","published":"2024-06-25T08:38:38Z","title":"T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge","summary":"  The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC .\n","authors":["Jianyu Wei","Shijie Cao","Ting Cao","Lingxiao Ma","Lei Wang","Yanyong Zhang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2407.00088v2.pdf","comment":"EuroSys 2025"},{"id":"http://arxiv.org/abs/2503.18663v2","updated":"2025-03-25T05:30:25Z","published":"2025-03-24T13:29:40Z","title":"Efficient Distributed Algorithms for Shape Reduction via Reconfigurable\n  Circuits","summary":"  In this paper, we study the problem of efficiently reducing geometric shapes\ninto other such shapes in a distributed setting through size-changing\noperations. We develop distributed algorithms using the reconfigurable circuit\nmodel to enable fast node-to-node communication. Our study considers two graph\nupdate models: the connectivity model and the adjacency model. Let $n$ denote\nthe number of nodes and $k$ the number of turning points in the initial shape.\nIn the connectivity model, we show that the system of nodes can reduce itself\nfrom any tree to a single node using only shrinking operations in $O(k \\log n)$\nrounds w.h.p. and any tree to its minimal (incompressible) form in $O(\\log n)$\nrounds with additional knowledge or $O(k \\log n)$ without, w.h.p. We also give\nan algorithm to transform any tree to any topologically equivalent tree in $O(k\n\\log n+\\log^2 n)$ rounds w.h.p. if both shrinking and growth operations are\navailable to the nodes. On the negative side, we show that one cannot hope for\n$o(\\log^2 n)$-round transformations for all shapes of $O(\\log n)$ turning\npoints: for all reasonable values of $k$, there exists a pair of geometrically\nequivalent paths of $k$ turning points each, such that $\\Omega(k\\log n)$ rounds\nare required to reduce one to the other. In the adjacency model, we show that\nthe system can reduce itself from any connected shape to a single node using\nonly shrinking in $O(\\log n)$ rounds w.h.p.\n","authors":["Nada Almalki","Siddharth Gupta","Othon Michail","Andreas Padalkin"],"pdf_url":"https://arxiv.org/pdf/2503.18663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19301v1","updated":"2025-03-25T03:03:37Z","published":"2025-03-25T03:03:37Z","title":"Fairness in Proof of Team Sprint (PoTS): Evaluating Reward Distribution\n  Across Performance Levels","summary":"  Blockchain consensus mechanisms must balance security, decentralization, and\nefficiency while ensuring fair participation. Proof of Team Sprint (PoTS) is a\ncooperative consensus mechanism designed to address the energy inefficiencies\nand centralization tendencies of traditional Proof of Work (PoW). Unlike PoW,\nwhere rewards disproportionately favor high-performance nodes, PoTS encourages\ncollaboration by forming teams and distributing rewards more equitably among\nparticipants. In this study, we evaluate the fairness properties of PoTS by\nanalyzing reward distribution under varying computational power distributions.\nThrough extensive simulations, we compare equal-share allocation and\nproportional reward allocation, highlighting their impact on decentralization\nand participation. Our results demonstrate that PoTS significantly reduces\nreward disparity between high-performance and low-performance nodes, fostering\na more inclusive ecosystem. Additionally, we observe that as team sizes\nincrease, the influence of individual computational power is mitigated,\nallowing lower-performance nodes to contribute meaningfully. Moreover, our\nfindings reveal that the marginal benefit of investing in extremely\nhigh-performance hardware diminishes, which discourages centralization and\naligns incentives toward sustainable participation. We also discuss the\neconomic implications of PoTS, particularly its potential to reshape blockchain\nmining strategies by balancing fairness with computational efficiency. These\ninsights contribute to the broader discussion on blockchain fairness and\nprovide a foundation for further research into cooperative consensus\nmechanisms.\n","authors":["Naoki Yonezawa"],"pdf_url":"https://arxiv.org/pdf/2503.19301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19293v1","updated":"2025-03-25T02:48:35Z","published":"2025-03-25T02:48:35Z","title":"Robustness of Proof of Team Sprint (PoTS) Against Attacks: A\n  Simulation-Based Analysis","summary":"  This study evaluates the robustness of Proof of Team Sprint (PoTS) against\nadversarial attacks through simulations, focusing on the attacker win rate and\ncomputational efficiency under varying team sizes (\\( N \\)) and attacker ratios\n(\\( \\alpha \\)). Our results demonstrate that PoTS effectively reduces an\nattacker's ability to dominate the consensus process. For instance, when \\(\n\\alpha = 0.5 \\), the attacker win rate decreases from 50.7\\% at \\( N = 1 \\) to\nbelow 0.4\\% at \\( N = 8 \\), effectively neutralizing adversarial influence.\nSimilarly, at \\( \\alpha = 0.8 \\), the attacker win rate drops from 80.47\\% at\n\\( N = 1 \\) to only 2.79\\% at \\( N = 16 \\). In addition to its strong security\nproperties, PoTS maintains high computational efficiency. We introduce the\nconcept of Normalized Computation Efficiency (NCE) to quantify this efficiency\ngain, showing that PoTS significantly improves resource utilization as team\nsize increases. The results indicate that as \\( N \\) grows, PoTS not only\nenhances security but also achieves better computational efficiency due to the\naveraging effects of execution time variations. These findings highlight PoTS\nas a promising alternative to traditional consensus mechanisms, offering both\nrobust security and efficient resource utilization. By leveraging team-based\nblock generation and randomized participant reassignment, PoTS provides a\nscalable and resilient approach to decentralized consensus.\n","authors":["Naoki Yonezawa"],"pdf_url":"https://arxiv.org/pdf/2503.19293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19289v1","updated":"2025-03-25T02:41:35Z","published":"2025-03-25T02:41:35Z","title":"Empirical Evaluation and Scalability Analysis of Proof of Team Sprint\n  (PoTS): Reward Fairness, Energy Efficiency, and System Stability","summary":"  This paper presents an empirical evaluation of the Proof of Team Sprint\n(PoTS) consensus algorithm, focusing on reward fairness, energy efficiency,\nsystem stability, and scalability. We conducted large-scale simulations\ncomparing PoTS with conventional Proof of Work (PoW) across various team sizes\nand computational conditions. In PoW, the highest-performance node ranked first\nin all 100 trials, demonstrating extreme centralization. In contrast, PoTS\nreduced this dominance: the same node ranked first only 54 times, indicating\nfairer reward distribution. Statistical analysis showed that as team size\nincreased, skewness and kurtosis of reward distributions decreased, confirming\nimproved equity among participants. PoTS also demonstrated significant energy\nsavings. The total active computation time followed a near $1/N$ scaling trend,\nreducing energy use by up to 64 times when team size was 64, while preserving\nconsensus integrity. Repeated simulations showed stable reward distributions\nand system performance, affirming PoTS's robustness. Furthermore, the\ncorrelation between performance and reward peaked at 0.90 for team size 16,\nreflecting an optimal balance between fairness and meritocracy. Overall, PoTS\noffers a cooperative, energy-efficient alternative to PoW, mitigating\ncentralization risks and promoting equitable participation. These findings\nvalidate PoTS as a sustainable and fair consensus mechanism suited for future\nblockchain systems.\n","authors":["Naoki Yonezawa"],"pdf_url":"https://arxiv.org/pdf/2503.19289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19270v1","updated":"2025-03-25T02:05:38Z","published":"2025-03-25T02:05:38Z","title":"LOCO: Rethinking Objects for Network Memory","summary":"  In this work, we explore an object-based programming model for filling the\nspace between shared memory and distributed systems programming. We argue that\nthe natural representation for resources distributed across a memory network\n(e.g. RDMA or CXL) is the traditional shared memory object. This concurrent\nobject (which we call a \"channel\" object) exports traditional methods, but,\nespecially in an incoherent or uncacheable memory network, stores its state in\na distributed fashion across all participating nodes. In a sense, the channel\nobject's state is stored \"across the network\".\n  Based on this philosophy, we introduce the Library of Channel Objects (LOCO),\na library for building multi-node objects on RDMA. Channel objects are\ncomposable and designed for both the strong locality effects and the weak\nconsistency of RDMA. Unlike prior work, channel objects do not hide memory\ncomplexity, instead relying on the programmer to use NUMA-like techniques to\nexplicitly manage each object. As a consequence, our channel objects have\nperformance similar to custom RDMA systems (e.g. distributed maps), but with a\nfar simpler programming model. Our distributed map channel has better read and\ncomparable write performance to a state-of-the-art custom RDMA solution, using\nwell-encapsulated and reusable primitives.\n","authors":["George Hodgkins","Mark Madler","Joseph Izraelevitz"],"pdf_url":"https://arxiv.org/pdf/2503.19270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04976v3","updated":"2025-03-25T01:15:15Z","published":"2024-03-08T01:16:26Z","title":"Towards Datacenter Environmental Sustainability Using Carbon\n  Depreciation Models","summary":"  Recently, the growing need for increasingly capable computing resources to be\navailable on-demand has led to the prosperity of data centers. These data\ncenters have led to several challenges and opportunities to address the\nenvironmental impacts from this computing resource. Conventional thinking has\nbeen concerned with minimizing energy usage of data centers to address\nsustainability. However, due to energy efficiency trends and renewable energy\nintegration, recent evidence has demonstrated that embodied carbon is\nincreasingly important and calls for improvements in data center provisioning\nstrategies. In this paper we propose to adopt carbon depreciation models to\nbetter encourage the longer lifetime of hardware in the data center. Carbon\ndepreciation models apply a higher proportion of embodied carbon to newly\nprovisioned servers. This promotes provisioning fewer new servers to service\njobs only with strict quality-of-service (QoS) constraints and extending\nlifetime of existing servers whose embodied carbon has already been mostly\nrecovered. Along with carbon depreciation, we make the case that both embodied\nand operational carbon from server idle time must also be recovered during\nactive jobs. This promotes provisioning strategies that maintain high rates of\nutilization. We show that prior carbon accounting strategies are\ncounterproductive for sustainability with a greedy job scheduler that attempts\nto minimize carbon under QoS constraints as they price jobs as 25% cheaper on\nnew versus old hardware. Our approach uses a greedy scheduler that prefers\nolder hardware due to non-linear carbon depreciation promoting sustainable\nprovisioning. Our approach reduces carbon by between 28--57% depending on\nassumptions for server lifetimes.\n","authors":["Shixin Ji","Zhuoping Yang","Alex K. Jones","Peipei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.04976v3.pdf","comment":"12 pages, 12 figures"}]},"2025-03-24T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2503.19063v1","updated":"2025-03-24T18:47:18Z","published":"2025-03-24T18:47:18Z","title":"COoL-TEE: Client-TEE Collaboration for Resilient Distributed Search","summary":"  Current marketplaces rely on search mechanisms with distributed systems but\ncentralized governance, making them vulnerable to attacks, failures, censorship\nand biases. While search mechanisms with more decentralized governance (e.g.,\nDeSearch) have been recently proposed, these are still exposed to information\nhead-start attacks (IHS) despite the use of Trusted Execution Environments\n(TEEs). These attacks allow malicious users to gain a head-start over other\nusers for the discovery of new assets in the market, which give them an unfair\nadvantage in asset acquisition. We propose COoL-TEE, a TEE-based provider\nselection mechanism for distributed search, running in single- or\nmulti-datacenter environments, that is resilient to information head-start\nattacks. COoL-TEE relies on a Client-TEE collaboration, which enables clients\nto distinguish between slow providers and malicious ones. Performance\nevaluations in single- and multi-datacenter environments show that, using\nCOoL-TEE, malicious users respectively gain only up to 2% and 7% of assets more\nthan without IHS, while they can claim 20% or more on top of their fair share\nin the same conditions with DeSearch.\n","authors":["Matthieu Bettinger","Etienne Rivière","Sonia Ben Mokhtar","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2503.19063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19055v1","updated":"2025-03-24T18:25:05Z","published":"2025-03-24T18:25:05Z","title":"Reliability is Blind: Collective Incentives for Decentralized Computing\n  Marketplaces without Individual Behavior Information","summary":"  In decentralized cloud computing marketplaces, ensuring fair and efficient\ninteractions among asset providers and end-users is crucial. A key concern is\nmeeting agreed-upon service-level objectives like the service's reliability. In\nthis decentralized context, traditional mechanisms often fail to address the\ncomplexity of task failures, due to limited available and trustworthy insights\ninto these independent actors' individual behavior. This paper proposes a\ncollective incentive mechanism that blindly punishes all involved parties when\na task fails. Based on ruin theory, we show that Collective Incentives improve\nbehavior in the marketplace by creating a disincentive for faults and\nmisbehavior even when the parties at fault are unknown, in turn leading to a\nmore robust marketplace. Simulations for small and large pools of marketplace\nassets show that Collective Incentives enable to meet or exceed a reliability\ntarget, i.e., the success-rate of tasks run using marketplace assets, by\neventually discarding failure-prone assets while preserving reliable ones.\n","authors":["Henry Mont","Matthieu Bettinger","Sonia Ben Mokhtar","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2503.19055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19050v1","updated":"2025-03-24T18:21:08Z","published":"2025-03-24T18:21:08Z","title":"Mist: Efficient Distributed Training of Large Language Models via\n  Memory-Parallelism Co-Optimization","summary":"  Various parallelism, such as data, tensor, and pipeline parallelism, along\nwith memory optimizations like activation checkpointing, redundancy\nelimination, and offloading, have been proposed to accelerate distributed\ntraining for Large Language Models. To find the best combination of these\ntechniques, automatic distributed training systems are proposed. However,\nexisting systems only tune a subset of optimizations, due to the lack of\noverlap awareness, inability to navigate the vast search space, and ignoring\nthe inter-microbatch imbalance, leading to sub-optimal performance. To address\nthese shortcomings, we propose Mist, a memory, overlap, and imbalance-aware\nautomatic distributed training system that comprehensively co-optimizes all\nmemory footprint reduction techniques alongside parallelism. Mist is based on\nthree key ideas: (1) fine-grained overlap-centric scheduling, orchestrating\noptimizations in an overlapped manner, (2) symbolic-based performance analysis\nthat predicts runtime and memory usage using symbolic expressions for fast\ntuning, and (3) imbalance-aware hierarchical tuning, decoupling the process\ninto an inter-stage imbalance and overlap aware Mixed Integer Linear\nProgramming problem and an intra-stage Dual-Objective Constrained Optimization\nproblem, and connecting them through Pareto frontier sampling. Our evaluation\nresults show that Mist achieves an average of 1.28$\\times$ (up to 1.73$\\times$)\nand 1.27$\\times$ (up to 2.04$\\times$) speedup compared to state-of-the-art\nmanual system Megatron-LM and state-of-the-art automatic system Aceso,\nrespectively.\n","authors":["Zhanda Zhu","Christina Giannoula","Muralidhar Andoorveedu","Qidong Su","Karttikeya Mangalam","Bojian Zheng","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2503.19050v1.pdf","comment":"Accepted by EuroSys 2025"},{"id":"http://arxiv.org/abs/2411.06364v2","updated":"2025-03-24T18:16:58Z","published":"2024-11-10T05:12:51Z","title":"EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving","summary":"  As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.\n","authors":["Haiying Shen","Tanmoy Sen"],"pdf_url":"https://arxiv.org/pdf/2411.06364v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2503.13075v2","updated":"2025-03-24T16:20:43Z","published":"2025-03-17T11:28:48Z","title":"ILVES: Accurate and efficient bond length and angle constraints in\n  molecular dynamics","summary":"  Force field-based molecular dynamics simulations are customarily carried out\nby constraining internal degrees of freedom. The de facto state-of-the-art\nalgorithms for this purpose, SHAKE, LINCS and P-LINCS, converge slowly,\nimpeding high-accuracy calculations and limiting the realism of simulations.\nFurthermore, LINCS and P-LINCS cannot handle general angular constraints, which\nrestricts increasing the time step.\n  In this paper, we introduce ILVES, a set of parallel algorithms that converge\nso rapidly that it is now practical to solve bond length and associated angular\nconstraint equations as accurately as the hardware will allow. We have\nintegrated our work into Gromacs and our analysis demonstrates that, in most\ncases, our software is superior to the state-of-the-art. We anticipate that\nILVES will allow for an increase in the time step, thus accelerating\ncontemporary calculations by a factor of at least 2. This will allow the\nscientific community to increase the range of phenomena that can therefore be\nsimulated.\n","authors":["Lorién López-Villellas","Carl Christian Kjelgaard Mikkelsen","Juan José Galano-Frutos","Santiago Marco-Sola","Jesús Alastruey-Benedé","Pablo Ibáñez","Miquel Moretó","Maria Cristina De Rosa","Pablo García-Risueño"],"pdf_url":"https://arxiv.org/pdf/2503.13075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08042v2","updated":"2025-03-24T15:53:11Z","published":"2025-02-12T00:58:31Z","title":"Parallel $k$-Core Decomposition: Theory and Practice","summary":"  This paper proposes efficient solutions for $k$-core decomposition with high\nparallelism. The problem of $k$-core decomposition is fundamental in graph\nanalysis and has applications across various domains. However, existing\nalgorithms face significant challenges in achieving work-efficiency in theory\nand/or high parallelism in practice, and suffer from various performance\nbottlenecks.\n  We present a simple, work-efficient parallel framework for $k$-core\ndecomposition that is easy to implement and adaptable to various strategies for\nimproving work-efficiency. We introduce two techniques to enhance parallelism:\na sampling scheme to reduce contention on high-degree vertices, and vertical\ngranularity control (VGC) to mitigate scheduling overhead for low-degree\nvertices. Furthermore, we design a hierarchical bucket structure to optimize\nperformance for graphs with high coreness values.\n  We evaluate our algorithm on a diverse set of real-world and synthetic\ngraphs. Compared to state-of-the-art parallel algorithms, including ParK, PKC,\nand Julienne, our approach demonstrates superior performance on 23 out of 25\ngraphs when tested on a 96-core machine. Our algorithm shows speedups of up to\n315$\\times$ over ParK, 33.4$\\times$ over PKC, and 52.5$\\times$ over Julienne.\n","authors":["Youzhe Liu","Xiaojun Dong","Yan Gu","Yihan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.08042v2.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2503.18474v1","updated":"2025-03-24T09:20:21Z","published":"2025-03-24T09:20:21Z","title":"Õptimal Fault-Tolerant Labeling for Reachability and Approximate\n  Distances in Directed Planar Graphs","summary":"  We present a labeling scheme that assigns labels of size $\\tilde O(1)$ to the\nvertices of a directed weighted planar graph $G$, such that for any fixed\n$\\varepsilon>0$ from the labels of any three vertices $s$, $t$ and $f$ one can\ndetermine in $\\tilde O(1)$ time a $(1+\\varepsilon)$-approximation of the\n$s$-to-$t$ distance in the graph $G\\setminus\\{f\\}$. For approximate distance\nqueries, prior to our work, no efficient solution existed, not even in the\ncentralized oracle setting. Even for the easier case of reachability, $\\tilde\nO(1)$ queries were known only with a centralized oracle of size $\\tilde O(n)$\n[SODA 21].\n","authors":["Itai Boneh","Shiri Chechik","Shay Golan","Shay Mozes","Oren Weimann"],"pdf_url":"https://arxiv.org/pdf/2503.18474v1.pdf","comment":"To appear in STOC 2025"},{"id":"http://arxiv.org/abs/2503.18427v1","updated":"2025-03-24T08:12:40Z","published":"2025-03-24T08:12:40Z","title":"AES-SpMM: Balancing Accuracy and Speed by Adaptive Edge Sampling\n  Strategy to Accelerate SpMM in GNNs","summary":"  Coordinating the design of sampling and sparse-dense matrix multiplication\n(SpMM) is crucial for accelerating graph neural networks (GNNs). However, due\nto irrational sampling strategies, existing methods face a trade-off between\naccuracy and speed. Moreover, as computational optimizations progress, data\nloading has gradually become the primary bottleneck in GNN inference. To\naddress these issues, we propose AES-SpMM, an adaptive edge sampling SpMM\nkernel. It considers the relationship between the number of non-zero elements\nin each matrix row and the shared memory width. The edge sampling scheme is\nadaptively selected according to the different situations of each row. AES-SpMM\nreduces the graph size through adaptive edge sampling to fit the GPU's shared\nmemory, lowering the computational cost and enhancing data locality, thus\nbalancing the accuracy and speed of GNN inference. Additionally, we introduce a\nquantization-based AES-SpMM, which applies quantization and dequantization to\nfeature data in GNNs. This approach significantly reduces data loading time\nwhile keeping accuracy loss negligible. We evaluated AES-SpMM with common GNN\nmodels and datasets. The results show that AES-SpMM outperforms both the\ncuSPARSE SpMM kernel and GE-SpMM by up to 25.87 times and 23.01 times,\nrespectively, with less than 1% accuracy loss. Compared to ES-SpMM, it reduces\naccuracy loss by 3.4% on average , achieving a 1.31 times speedup. Compared to\nAES-SpMM, quantization-based AES-SpMM has a maximum accuracy loss of 0.3% and\nfeature data loading time overhead is reduced by 50.91%-70.51%.\n","authors":["Yingchen Song","Yaobin Wang","Yi Luo","Huan Wu","Pingping Tang"],"pdf_url":"https://arxiv.org/pdf/2503.18427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18424v1","updated":"2025-03-24T08:08:21Z","published":"2025-03-24T08:08:21Z","title":"ED-DAO: Energy Donation Algorithms based on Decentralized Autonomous\n  Organization","summary":"  Energy is a fundamental component of modern life, driving nearly all aspects\nof daily activities. As such, the inability to access energy when needed is a\nsignificant issue that requires innovative solutions. In this paper, we propose\nED-DAO, a novel fully transparent and community-driven decentralized autonomous\norganization (DAO) designed to facilitate energy donations. We analyze the\nenergy donation process by exploring various approaches and categorizing them\nbased on both the source of donated energy and funding origins. We propose a\nnovel Hybrid Energy Donation (HED) algorithm, which enables contributions from\nboth external and internal donors. External donations are payments sourced from\nentities such as charities and organizations, where energy is sourced from the\nutility grid and prosumers. Internal donations, on the other hand, come from\npeer contributors with surplus energy. HED prioritizes donations in the\nfollowing sequence: peer-sourced energy (P2D), utilitygrid-sourced energy\n(UG2D), and direct energy donations by peers (P2PD). By merging these donation\napproaches, the HED algorithm increases the volume of donated energy, providing\na more effective means to address energy poverty. Experiments were conducted on\na dataset to evaluate the effectiveness of the proposed method. The results\nshowed that HED increased the total donated energy by at least 0.43% (64\nmegawatts) compared to the other algorithms (UG2D, P2D, and P2PD).\n","authors":["Abdulrezzak Zekiye","Ouns Bouachir","Öznur Özkasap","Moayad Aloqaily"],"pdf_url":"https://arxiv.org/pdf/2503.18424v1.pdf","comment":"6 pages, 5 figures, and 4 tables. Accepted for publication in IEEE\n  International Conference on Communications (IEEE ICC 2025)"},{"id":"http://arxiv.org/abs/2503.11901v2","updated":"2025-03-24T03:52:43Z","published":"2025-03-14T22:14:18Z","title":"Characterizing GPU Resilience and Impact on AI/HPC Systems","summary":"  In this study, we characterize GPU failures in Delta, the current large-scale\nAI system with over 600 petaflops of peak compute throughput. The system\ncomprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA\nA40, A100, and H100 GPUs. The study uses two and a half years of data on GPU\nerrors. We evaluate the resilience of GPU hardware components to determine the\nvulnerability of different GPU components to failure and their impact on the\nGPU and node availability. We measure the key propagation paths in GPU\nhardware, GPU interconnect (NVLink), and GPU memory. Finally, we evaluate the\nimpact of the observed GPU errors on user jobs. Our key findings are: (i)\nContrary to common beliefs, GPU memory is over 30x more reliable than GPU\nhardware in terms of MTBE (mean time between errors). (ii) The newly introduced\nGSP (GPU System Processor) is the most vulnerable GPU hardware component. (iii)\nNVLink errors did not always lead to user job failure, and we attribute it to\nthe underlying error detection and retry mechanisms employed. (iv) We show\nmultiple examples of hardware errors originating from one of the key GPU\nhardware components, leading to application failure. (v) We project the impact\nof GPU node availability on larger scales with emulation and find that\nsignificant overprovisioning between 5-20% would be necessary to handle GPU\nfailures. If GPU availability were improved to 99.9%, the overprovisioning\nwould be reduced by 4x.\n","authors":["Shengkun Cui","Archit Patke","Ziheng Chen","Aditya Ranjan","Hung Nguyen","Phuong Cao","Saurabh Jha","Brett Bode","Gregory Bauer","Chandra Narayanaswami","Daby Sow","Catello Di Martino","Zbigniew T. Kalbarczyk","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2503.11901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18292v1","updated":"2025-03-24T02:28:04Z","published":"2025-03-24T02:28:04Z","title":"Jenga: Effective Memory Management for Serving LLM with Heterogeneity","summary":"  Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).\n","authors":["Chen Zhang","Kuntai Du","Shu Liu","Woosuk Kwon","Xiangxi Mo","Yufeng Wang","Xiaoxuan Liu","Kaichao You","Zhuohan Li","Mingsheng Long","Jidong Zhai","Joseph Gonzalez","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2503.18292v1.pdf","comment":"16 pages, 19 figures"},{"id":"http://arxiv.org/abs/2405.13584v2","updated":"2025-03-24T01:54:06Z","published":"2024-05-22T12:27:24Z","title":"Emulating Full Participation: An Effective and Fair Client Selection\n  Strategy for Federated Learning","summary":"  In federated learning, client selection is a critical problem that\nsignificantly impacts both model performance and fairness. Prior studies\ntypically treat these two objectives separately, or balance them using simple\nweighting schemes. However, we observe that commonly used metrics for model\nperformance and fairness often conflict with each other, and a straightforward\nweighted combination is insufficient to capture their complex interactions. To\naddress this, we first propose two guiding principles that directly tackle the\ninherent conflict between the two metrics while reinforcing each other. Based\non these principles, we formulate the client selection problem as a long-term\noptimization task, leveraging the Lyapunov function and the submodular nature\nof the problem to solve it effectively. Experiments show that the proposed\nmethod improves both model performance and fairness, guiding the system to\nconverge comparably to full client participation. This improvement can be\nattributed to the fact that both model performance and fairness benefit from\nthe diversity of the selected clients' data distributions. Our approach\nadaptively enhances this diversity by selecting clients based on their data\ndistributions, thereby improving both model performance and fairness.\n","authors":["Qingming Li","Juzheng Miao","Puning Zhao","Li Zhou","H. Vicky Zhao","Shouling Ji","Bowen Zhou","Furui Liu"],"pdf_url":"https://arxiv.org/pdf/2405.13584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18265v1","updated":"2025-03-24T01:15:43Z","published":"2025-03-24T01:15:43Z","title":"Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence","summary":"  Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.\n","authors":["Akaash Vishal Hazarika","Mahak Shah","Swapnil Patil","Pradyumna Shukla"],"pdf_url":"https://arxiv.org/pdf/2503.18265v1.pdf","comment":"International Conference on AI and Financial Innovation AIFI-2025"},{"id":"http://arxiv.org/abs/2503.18260v1","updated":"2025-03-24T01:01:19Z","published":"2025-03-24T01:01:19Z","title":"Bridging Emotions and Architecture: Sentiment Analysis in Modern\n  Distributed Systems","summary":"  Sentiment analysis is a field within NLP that has gained importance because\nit is applied in various areas such as; social media surveillance, customer\nfeedback evaluation and market research. At the same time, distributed systems\nallow for effective processing of large amounts of data. Therefore, this paper\nexamines how sentiment analysis converges with distributed systems by\nconcentrating on different approaches, challenges and future investigations.\nFurthermore, we do an extensive experiment where we train sentiment analysis\nmodels using both single node configuration and distributed architecture to\nbring out the benefits and shortcomings of each method in terms of performance\nand accuracy.\n","authors":["Mahak Shah","Akaash Vishal Hazarika","Meetu Malhotra","Sachin C. Patil","Joshit Mohanty"],"pdf_url":"https://arxiv.org/pdf/2503.18260v1.pdf","comment":"IEEE 3rd International Conference on Advancements in Smart, Secure\n  and Intelligent Computing (ASSIC)"}]},"2025-03-23T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2503.18206v1","updated":"2025-03-23T21:13:52Z","published":"2025-03-23T21:13:52Z","title":"Predictive Performance of Photonic SRAM-based In-Memory Computing for\n  Tensor Decomposition","summary":"  Photonics-based in-memory computing systems have demonstrated a significant\nspeedup over traditional transistor-based systems because of their ultra-fast\noperating frequencies and high data bandwidths. Photonic static random access\nmemory (pSRAM) is a crucial component for achieving the objective of ultra-fast\nphotonic in-memory computing systems. In this work, we model and evaluate the\nperformance of a novel photonic SRAM array architecture in development.\nAdditionally, we examine hyperspectral operation through wavelength division\nmultiplexing (WDM) to enhance the throughput of the pSRAM array. We map\nMatricized Tensor Times Khatri-Rao Product (MTTKRP), a computational kernel\ncommonly used in tensor decomposition, to the proposed pSRAM array\narchitecture. We also develop a predictive performance model to estimate the\nsustained performance of different configurations of the pSRAM array. Using the\npredictive performance model, we demonstrate that the pSRAM array achieves 17\nPetaOps while performing MTTKRP in a practical hardware configuration.\n","authors":["Sasindu Wijeratne","Sugeet Sunder","Md Abdullah-Al Kaiser","Akhilesh Jaiswal","Clynn Mathew","Ajey P. Jacob","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2503.18206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18198v1","updated":"2025-03-23T20:46:03Z","published":"2025-03-23T20:46:03Z","title":"Accelerating Sparse MTTKRP for Small Tensor Decomposition on GPU","summary":"  Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the\nbottleneck kernel of sparse tensor decomposition. In tensor decomposition,\nspMTTKRP is performed iteratively along all the modes of an input tensor. In\nthis work, we propose a mode-specific tensor layout on GPU that uses multiple\ntensor copies, where each copy is optimized for a specific mode. The proposed\ntensor layout increases the data locality of external memory accesses and\neliminates the intermediate values communicated between the GPU thread blocks\nand the GPU global memory. We also propose a tensor partitioning scheme to\noptimally distribute the total computations among GPU streaming multiprocessors\nbased on the sparsity and the dimensions of the input tensor. Our approach\nachieves a geometric mean speedup of 2.4x, 7.9x, and 8.9x in total execution\ntime compared with the state-of-the-art GPU baselines.\n","authors":["Sasindu Wijeratne","Rajgopal Kannan","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2503.18198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18140v1","updated":"2025-03-23T17:11:48Z","published":"2025-03-23T17:11:48Z","title":"INDIGO: Page Migration for Hardware Memory Disaggregation Across a\n  Network","summary":"  Hardware memory disaggregation (HMD) is an emerging technology that enables\naccess to remote memory, thereby creating expansive memory pools and reducing\nmemory underutilization in datacenters. However, a significant challenge arises\nwhen accessing remote memory over a network: increased contention that can lead\nto severe application performance degradation. To reduce the performance\npenalty of using remote memory, the operating system uses page migration to\npromote frequently accessed pages closer to the processor. However, previously\nproposed page migration mechanisms do not achieve the best performance in HMD\nsystems because of obliviousness to variable page transfer costs that occur due\nto network contention. To address these limitations, we present INDIGO: a\nnetwork-aware page migration framework that uses novel page telemetry and a\nlearning-based approach for network adaptation. We implemented INDIGO in the\nLinux kernel and evaluated it with common cloud and HPC applications on a real\ndisaggregated memory system prototype. Our evaluation shows that INDIGO offers\nup to 50-70% improvement in application performance compared to other\nstate-of-the-art page migration policies and reduces network traffic up to 2x.\n","authors":["Archit Patke","Christian Pinto","Saurabh Jha","Haoran Qiu","Zbigniew Kalbarczyk","Ravishankar Iyer"],"pdf_url":"https://arxiv.org/pdf/2503.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18093v1","updated":"2025-03-23T14:45:03Z","published":"2025-03-23T14:45:03Z","title":"Reliable Replication Protocols on SmartNICs","summary":"  Today's datacenter applications rely on datastores that are required to\nprovide high availability, consistency, and performance. To achieve high\navailability, these datastores replicate data across several nodes. Such\nreplication is managed through a reliable protocol designed to keep the\nreplicas consistent using a consistency model, even in the presence of faults.\nFor several applications, strong consistency models are favored over weaker\nconsistency models, as the former guarantee a more intuitive behavior for\nclients. Furthermore, to meet the demands of high online traffic, datastores\nmust offer high throughput and low latency.\n  However, delivering both strong consistency and high performance\nsimultaneously can be challenging. Reliable replication protocols typically\nrequire multiple rounds of communication over the network stack, which\nintroduces latency and increases the load on network resources. Moreover, these\nprotocols consume considerable CPU resources, which impacts the overall\nperformance of applications, especially in high-throughput environments.\n  In this work, we aim to design a hardware-accelerated system for replication\nprotocols to address these challenges. We approach offloading the replication\nprotocol onto SmartNICs, which are specialized network interface cards that can\nbe programmed to implement custom logic directly on the NIC. By doing so, we\naim to enhance performance while preserving strong consistency, all while\nsaving valuable CPU cycles that can be used for applications' logic.\n","authors":["M. R. Siavash Katebzadeh","Antonios Katsarakis","Boris Grot"],"pdf_url":"https://arxiv.org/pdf/2503.18093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07849v5","updated":"2025-03-23T13:43:19Z","published":"2023-01-19T02:11:47Z","title":"Efficient Computation in Congested Anonymous Dynamic Networks","summary":"  An anonymous dynamic network is a network of indistinguishable processes\nwhose communication links may appear or disappear unpredictably over time.\nPrevious research has shown that deterministically computing an arbitrary\nfunction of a multiset of input values given to these processes takes only a\nlinear number of communication rounds (Di Luna-Viglietta, FOCS 2022).\n  However, fast algorithms for anonymous dynamic networks rely on the\nconstruction and transmission of large data structures called \"history trees\",\nwhose size is polynomial in the number of processes. This approach is\nunfeasible if the network is congested, and only messages of logarithmic size\ncan be sent through its links. Observe that sending a large message piece by\npiece over several rounds is not in itself a solution, due to the anonymity of\nthe processes combined with the dynamic nature of the network. Moreover, it is\nknown that certain basic tasks such as all-to-all token dissemination (by means\nof single-token forwarding) require $\\Omega(n^2/\\log n)$ rounds in congested\nnetworks (Dutta et al., SODA 2013).\n  In this work, we develop a series of practical and efficient techniques that\nmake it possible to use history trees in congested anonymous dynamic networks.\nAmong other applications, we show how to compute arbitrary functions in such\nnetworks in $O(n^3)$ communication rounds, greatly improving upon previous\nstate-of-the-art algorithms for congested networks.\n","authors":["Giuseppe A. Di Luna","Giovanni Viglietta"],"pdf_url":"https://arxiv.org/pdf/2301.07849v5.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.17924v1","updated":"2025-03-23T03:40:45Z","published":"2025-03-23T03:40:45Z","title":"WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model\n  Training","summary":"  In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for\nlarge language model training. We first thoroughly analyze the workload\nimbalance issue in LLM training and identify two primary sources of imbalance\nat the pipeline parallelism and context parallelism levels. Then, to address\nthe imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a\nworkload-aware variable-length document packing method to balance the\ncomputation and communication workload across micro-batches. Additionally, at\nthe context parallelism level, WLB-LLM introduces a novel fine-grained\nper-document sharding strategy, ensuring each worker within a context\nparallelism group has an identical workload. Comprehensive experiments under\ndifferent model scales demonstrate that WLB-LLM significantly mitigates the\nworkload imbalance during 4D parallelism LLM training and achieves an average\nspeedup of 1.23x when applying WLB-LLM in our internal LLM training framework.\n","authors":["Zheng Wang","Anna Cai","Xinfeng Xie","Zaifeng Pan","Yue Guan","Weiwei Chu","Jie Wang","Shikai Li","Jianyu Huang","Chris Cai","Yuchen Hao","Yufei Ding"],"pdf_url":"https://arxiv.org/pdf/2503.17924v1.pdf","comment":"12 pages, 16 figures"}]},"2025-03-22T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2409.00822v3","updated":"2025-03-22T21:35:56Z","published":"2024-09-01T19:43:40Z","title":"RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network\n  Acceleration on GPUs","summary":"  Top-k selection algorithms are fundamental in a wide range of applications,\nincluding high-performance computing, information retrieval, big data\nprocessing, and neural network model training. In this paper, we present\nRTop-K, a highly efficient parallel row-wise top-k selection algorithm\nspecifically designed for GPUs. RTop-K leverages a binary search-based approach\nto optimize row-wise top-k selection, providing a scalable and accelerated\nsolution. We conduct a detailed analysis of early stopping in our algorithm,\nshowing that it effectively maintains the testing accuracy of neural network\nmodels while substantially improving performance. Our GPU implementation of\nRTop-K demonstrates superior performance over state-of-the-art row-wise top-k\nGPU implementations, achieving an average speed-up of up to 11.49$\\times$ with\nearly stopping and 7.29$\\times$ without early stopping. Moreover, RTop-K\naccelerates the overall training workflow of MaxK-GNNs, delivering speed-ups\nranging from 11.97% to 33.29% across different models and datasets.\n","authors":["Xi Xie","Yuebo Luo","Hongwu Peng","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2409.00822v3.pdf","comment":"ICLR 2025 Conference"},{"id":"http://arxiv.org/abs/2503.17826v1","updated":"2025-03-22T17:47:47Z","published":"2025-03-22T17:47:47Z","title":"CRDT-Based Game State Synchronization in Peer-to-Peer VR","summary":"  Virtual presence demands ultra-low latency, a factor that centralized\narchitectures, by their nature, cannot minimize. Local peer-to-peer\narchitectures offer a compelling alternative, but also pose unique challenges\nin terms of network infrastructure. This paper introduces a prototype\nleveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time\ncollaboration in a shared virtual environment. Using this prototype, we\ninvestigate latency, synchronization, and the challenges of decentralized\ncoordination in dynamic non-Byzantine contexts. We aim to question prevailing\nassumptions about decentralized architectures and explore the practical\npotential of P2P in advancing virtual presence. This work challenges the\nconstraints of mediated networks and highlights the potential of decentralized\narchitectures to redefine collaboration and interaction in digital spaces.\n","authors":["Abel Dantas","Carlos Baquero"],"pdf_url":"https://arxiv.org/pdf/2503.17826v1.pdf","comment":"Total PDF pages: 11 Figures: 11"},{"id":"http://arxiv.org/abs/2502.09303v2","updated":"2025-03-22T13:48:11Z","published":"2025-02-13T13:16:10Z","title":"Towards Seamless Hierarchical Federated Learning under Intermittent\n  Client Participation: A Stagewise Decision-Making Methodology","summary":"  Federated Learning (FL) offers a pioneering distributed learning paradigm\nthat enables devices/clients to build a shared global model. This global model\nis obtained through frequent model transmissions between clients and a central\nserver, which may cause high latency, energy consumption, and congestion over\nbackhaul links. To overcome these drawbacks, Hierarchical Federated Learning\n(HFL) has emerged, which organizes clients into multiple clusters and utilizes\nedge nodes (e.g., edge servers) for intermediate model aggregations between\nclients and the central server. Current research on HFL mainly focus on\nenhancing model accuracy, latency, and energy consumption in scenarios with a\nstable/fixed set of clients. However, addressing the dynamic availability of\nclients -- a critical aspect of real-world scenarios -- remains underexplored.\nThis study delves into optimizing client selection and client-to-edge\nassociations in HFL under intermittent client participation so as to minimize\noverall system costs (i.e., delay and energy), while achieving fast model\nconvergence. We unveil that achieving this goal involves solving a complex\nNP-hard problem. To tackle this, we propose a stagewise methodology that splits\nthe solution into two stages, referred to as Plan A and Plan B. Plan A focuses\non identifying long-term clients with high chance of participation in\nsubsequent model training rounds. Plan B serves as a backup, selecting\nalternative clients when long-term clients are unavailable during model\ntraining rounds. This stagewise methodology offers a fresh perspective on\nclient selection that can enhance both HFL and conventional FL via enabling\nlow-overhead decision-making processes. Through evaluations on MNIST and\nCIFAR-10 datasets, we show that our methodology outperforms existing benchmarks\nin terms of model accuracy and system costs.\n","authors":["Minghong Wu","Minghui Liwang","Yuhan Su","Li Li","Seyyedali Hosseinalipour","Xianbin Wang","Huaiyu Dai","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2502.09303v2.pdf","comment":"20 pages, 8 figures,5 tables"},{"id":"http://arxiv.org/abs/2503.17743v1","updated":"2025-03-22T12:02:43Z","published":"2025-03-22T12:02:43Z","title":"Neutron particle transport 3D method of characteristic Multi GPU\n  platform Parallel Computing","summary":"  Three-dimensional neutron transport calculations using the Method of\nCharacteristics (MOC) are highly regarded for their exceptional computational\nefficiency, precision, and stability. Nevertheless, when dealing with\nextensive-scale computations, the computational demands are substantial,\nleading to prolonged computation times. To address this challenge while\nconsidering GPU memory limitations, this study transplants the real-time\ngeneration and characteristic line computation techniques onto the GPU\nplatform. Empirical evidence emphasizes that the GPU-optimized approach\nmaintains a heightened level of precision in computation results and produces a\nsignificant acceleration effect. Furthermore, to fully harness the\ncomputational capabilities of GPUs, a dual approach involving characteristic\nline preloading and load balancing mechanisms is adopted, further enhancing\ncomputational efficiency. The resulting increase in computational efficiency,\ncompared to traditional methods, reaches an impressive 300 to 400-fold\nimprovement.\n","authors":["Faguo Zhou","Shunde Li","Rong Xue","Lingkun Bu","Ningming Nie","Peng Shi","Jue Wang","Yun Hu","Zongguo Wang","Yangang Wang","Qinmeng Yang","Miao Yu"],"pdf_url":"https://arxiv.org/pdf/2503.17743v1.pdf","comment":"14 pages, 7 figures. Submitted to a peer-reviewed journal"},{"id":"http://arxiv.org/abs/2503.17707v1","updated":"2025-03-22T09:25:03Z","published":"2025-03-22T09:25:03Z","title":"PipeBoost: Resilient Pipelined Architecture for Fast Serverless LLM\n  Scaling","summary":"  This paper presents PipeBoost, a low-latency LLM serving system for multi-GPU\n(serverless) clusters, which can rapidly launch inference services in response\nto bursty requests without preemptively over-provisioning GPUs. Many LLM\ninference tasks rely on the same base model (e.g., LoRA). To leverage this,\nPipeBoost introduces fault-tolerant pipeline parallelism across both model\nloading and inference stages. This approach maximizes aggregate PCIe bandwidth\nand parallel computation across GPUs, enabling faster generation of the first\ntoken. PipeBoost also introduces recovery techniques that enable uninterrupted\ninference services by utilizing the shared advantages of multiple GPUs.\nExperimental results show that, compared to state-of-the-art low-latency LLM\nserving systems, PipeBoost reduces inference latency by 31% to 49.8%. For\ncertain models (e.g., OPT-1.3B), PipeBoost achieves cold-start latencies in the\nrange of a few hundred microseconds.\n","authors":["Chongpeng Liu","Xiaojian Liao","Hancheng Liu","Limin Xiao","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2503.17707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17697v1","updated":"2025-03-22T08:39:01Z","published":"2025-03-22T08:39:01Z","title":"Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for\n  Autonomous Driving","summary":"  To accommodate constantly changing road conditions, real-time model training\nis essential for autonomous driving (AD). Federated learning (FL) serves as a\npromising paradigm to enable autonomous vehicles to train models\ncollaboratively with their onboard computing resources. However, existing\nvehicle selection schemes for FL all assume predetermined and\nlocation-independent vehicles' datasets, neglecting the fact that vehicles\ncollect training data along their routes, thereby resulting in suboptimal\nvehicle selection. To improve the perception quality in AD for a region, we\npropose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring\ntrajectory-dependent vehicular training data collection. To this end, we first\nderive the convergence bound of FL by considering the impact of both vehicles'\nuncertain trajectories and uploading probabilities, from which we discover that\nminimizing the training loss is equivalent to minimizing a weighted sum of\nlocal and global earth mover's distance (EMD) between vehicles' collected data\ndistribution and global data distribution. Based on this observation, we\nformulate the trajectory-dependent vehicle selection and data collection\nproblem for FL in AD. Given that the problem is NP-hard, we develop an\nefficient algorithm to find the solution with an approximation guarantee.\nExtensive simulation results have demonstrated the effectiveness of our\napproach in improving object detection performance compared with existing\nbenchmarks.\n","authors":["Yanan Ma","Senkang Hu","Zhengru Fang","Yun Ji","Yiqin Deng","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2503.17697v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.17691v1","updated":"2025-03-22T08:25:01Z","published":"2025-03-22T08:25:01Z","title":"Using a Market Economy to Provision Compute Resources Across Planet-wide\n  Clusters","summary":"  We present a practical, market-based solution to the resource provisioning\nproblem in a set of heterogeneous resource clusters. We focus on provisioning\nrather than immediate scheduling decisions to allow users to change long-term\njob specifications based on market feedback. Users enter bids to purchase\nquotas, or bundles of resources for long-term use. These requests are mapped\ninto a simulated clock auction which determines uniform, fair resource prices\nthat balance supply and demand. The reserve prices for resources sold by the\noperator in this auction are set based on current utilization, thus guiding the\nusers as they set their bids towards under-utilized resources. By running these\nauctions at regular time intervals, prices fluctuate like those in a real-world\neconomy and provide motivation for users to engineer systems that can best take\nadvantage of available resources.\n  These ideas were implemented in an experimental resource market at Google.\nOur preliminary results demonstrate an efficient transition of users from more\ncongested resource pools to less congested resources. The disparate engineering\ncosts for users to reconfigure their jobs to run on less expensive resource\npools was evidenced by the large price premiums some users were willing to pay\nfor more expensive resources. The final resource allocations illustrated how\nthis framework can lead to significant, beneficial changes in user behavior,\nreducing the excessive shortages and surpluses of more traditional allocation\nmethods.\n","authors":["Murray Stokely","Jim Winget","Ed Keyes","Carrie Grimes","Benjamin Yolken"],"pdf_url":"https://arxiv.org/pdf/2503.17691v1.pdf","comment":"Published in 2009 IEEE International Symposium on Parallel &\n  Distributed Processing"},{"id":"http://arxiv.org/abs/2503.17652v1","updated":"2025-03-22T05:04:44Z","published":"2025-03-22T05:04:44Z","title":"Time- and Space-Optimal Silent Self-Stabilizing Exact Majority in\n  Population Protocols","summary":"  We address the self-stabilizing exact majority problem in the population\nprotocol model, introduced by Angluin, Aspnes, Diamadi, Fischer, and Peralta\n(2004). In this model, there are $n$ state machines, called agents, which form\na network. At each time step, only two agents interact with each other, and\nupdate their states. In the self-stabilizing exact majority problem, each agent\nhas a fixed opinion, $\\mathtt{A}$ or $\\mathtt{B}$, and stabilizes to a safe\nconfiguration in which all agents output the majority opinion from any initial\nconfiguration.\n  In this paper, we show the impossibility of solving the self-stabilizing\nexact majority problem without knowledge of $n$ in any protocol. We propose a\nsilent self-stabilizing exact majority protocol, which stabilizes within $O(n)$\nparallel time in expectation and within $O(n \\log n)$ parallel time with high\nprobability, using $O(n)$ states, with knowledge of $n$. Here, a silent\nprotocol means that, after stabilization, the state of each agent does not\nchange. We establish lower bounds, proving that any silent protocol requires\n$\\Omega(n)$ states, $\\Omega(n)$ parallel time in expectation, and $\\Omega(n\n\\log n)$ parallel time with high probability to reach a safe configuration.\nThus, the proposed protocol is time- and space-optimal.\n","authors":["Haruki Kanaya","Ryota Eguchi","Taisho Sasada","Fukuhito Ooshita","Michiko Inoue"],"pdf_url":"https://arxiv.org/pdf/2503.17652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00210v2","updated":"2025-03-22T02:32:17Z","published":"2024-12-31T01:24:52Z","title":"Debunking the CUDA Myth Towards GPU-based AI Systems","summary":"  This paper presents a comprehensive evaluation of Intel Gaudi NPUs as an\nalternative to NVIDIA GPUs, which is currently the de facto standard in AI\nsystem design. First, we create a suite of microbenchmarks to compare Intel\nGaudi-2 with NVIDIA A100, showing that Gaudi-2 achieves competitive performance\nnot only in primitive AI compute, memory, and communication operations but also\nin executing several important AI workloads end-to-end. We then assess Gaudi\nNPU's programmability by discussing several software-level optimization\nstrategies to employ for implementing critical FBGEMM operators and vLLM,\nevaluating their efficiency against GPU-optimized counterparts. Results\nindicate that Gaudi-2 achieves energy efficiency comparable to A100, though\nthere are notable areas for improvement in terms of software maturity. Overall,\nwe conclude that, with effective integration into high-level AI frameworks,\nGaudi NPUs could challenge NVIDIA GPU's dominance in the AI server market,\nthough further improvements are necessary to fully compete with NVIDIA's robust\nsoftware ecosystem.\n","authors":["Yunjae Lee","Juntaek Lim","Jehyeon Bang","Eunyeong Cho","Huijong Jeong","Taesu Kim","Hyungjun Kim","Joonhyung Lee","Jinseop Im","Ranggi Hwang","Se Jung Kwon","Dongsoo Lee","Minsoo Rhu"],"pdf_url":"https://arxiv.org/pdf/2501.00210v2.pdf","comment":"Accepted for publication at the 52nd IEEE/ACM International Symposium\n  on Computer Architecture (ISCA-52), 2025"},{"id":"http://arxiv.org/abs/2503.17603v1","updated":"2025-03-22T01:17:56Z","published":"2025-03-22T01:17:56Z","title":"A Generative Caching System for Large Language Models","summary":"  Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.\n","authors":["Arun Iyengar","Ashish Kundu","Ramana Kompella","Sai Nandan Mamidi"],"pdf_url":"https://arxiv.org/pdf/2503.17603v1.pdf","comment":null}]},"2025-03-21T00:00:00Z":{"Container scheduling":[{"id":"http://arxiv.org/abs/2503.17528v1","updated":"2025-03-21T20:21:22Z","published":"2025-03-21T20:21:22Z","title":"Serinv: A Scalable Library for the Selected Inversion of\n  Block-Tridiagonal with Arrowhead Matrices","summary":"  The inversion of structured sparse matrices is a key but computationally and\nmemory-intensive operation in many scientific applications. There are cases,\nhowever, where only particular entries of the full inverse are required. This\nhas motivated the development of so-called selected-inversion algorithms,\ncapable of computing only specific elements of the full inverse. Currently,\nmost of them are either shared-memory codes or limited to CPU implementations.\nHere, we introduce Serinv, a scalable library providing distributed, GPU-based\nalgorithms for the selected inversion and Cholesky decomposition of\npositive-definite, block-tridiagonal arrowhead matrices. This matrix class is\nhighly relevant in statistical climate modeling and materials science\napplications. The performance of Serinv is demonstrated on synthetic and real\ndatasets from statistical air temperature prediction models. In our numerical\ntests, Serinv achieves 32.3% strong and 47.2% weak scaling efficiency and up to\ntwo orders of magnitude speedup over the sparse direct solvers PARDISO and\nMUMPS on 16 GPUs.\n","authors":["Vincent Maillou","Lisa Gaedke-Merzhaeuser","Alexandros Nikolaos Ziogas","Olaf Schenk","Mathieu Luisier"],"pdf_url":"https://arxiv.org/pdf/2503.17528v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.14649v2","updated":"2025-03-21T17:51:53Z","published":"2025-03-18T18:58:13Z","title":"RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving","summary":"  Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.\n","authors":["Wenqi Jiang","Suvinay Subramanian","Cat Graves","Gustavo Alonso","Amir Yazdanbakhsh","Vidushi Dadu"],"pdf_url":"https://arxiv.org/pdf/2503.14649v2.pdf","comment":"16 pages, 19 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.20212v2","updated":"2025-03-21T17:40:01Z","published":"2024-07-29T17:42:25Z","title":"Distributed Quantum Approximate Optimization Algorithm on a\n  Quantum-Centric Supercomputing Architecture","summary":"  Quantum approximate optimization algorithm (QAOA) has shown promise in\nsolving combinatorial optimization problems by providing quantum speedup on\nnear-term gate-based quantum computing systems. However, QAOA faces challenges\nfor high-dimensional problems due to the large number of qubits required and\nthe complexity of deep circuits, limiting its scalability for real-world\napplications. In this study, we present a distributed QAOA (DQAOA), which\nleverages distributed computing strategies to decompose a large computational\nworkload into smaller tasks that require fewer qubits and shallower circuits\nthan necessitated to solve the original problem. These sub-problems are\nprocessed using a combination of high-performance and quantum computing\nresources. The global solution is iteratively updated by aggregating\nsub-solutions, allowing convergence toward the optimal solution. We demonstrate\nthat DQAOA can handle considerably large-scale optimization problems (e.g.,\n1,000-bit problem) achieving a high approximation ratio ($\\sim$99%) and short\ntime-to-solution ($\\sim$276 s), outperforming existing strategies. Furthermore,\nwe realize DQAOA on a quantum-centric supercomputing architecture, paving the\nway for practical applications of gate-based quantum computers in real-world\noptimization tasks. To extend DQAOA's applicability to materials science, we\nfurther develop an active learning algorithm integrated with our DQAOA\n(AL-DQAOA), which involves machine learning, DQAOA, and active data production\nin an iterative loop. We successfully optimize photonic structures using\nAL-DQAOA, indicating that solving real-world optimization problems using\ngate-based quantum computing is feasible. We expect the proposed DQAOA to be\napplicable to a wide range of optimization problems and AL-DQAOA to find\nbroader applications in material design.\n","authors":["Seongmin Kim","Vincent R. Pascuzzi","Zhihao Xu","Tengfei Luo","Eungkyu Lee","In-Saeng Suh"],"pdf_url":"https://arxiv.org/pdf/2407.20212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00937v2","updated":"2025-03-21T16:53:47Z","published":"2025-02-02T22:10:40Z","title":"ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving","summary":"  Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces.\n","authors":["Haoran Qiu","Anish Biswas","Zihan Zhao","Jayashree Mohan","Alind Khare","Esha Choukse","Íñigo Goiri","Zeyu Zhang","Haiying Shen","Chetan Bansal","Ramachandran Ramjee","Rodrigo Fonseca"],"pdf_url":"https://arxiv.org/pdf/2502.00937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17283v1","updated":"2025-03-21T16:30:22Z","published":"2025-03-21T16:30:22Z","title":"Energy Efficiency trends in HPC: what high-energy and astrophysicists\n  need to know","summary":"  The growing energy demands of HPC systems have made energy efficiency a\ncritical concern for system developers and operators. However, HPC users are\ngenerally less aware of how these energy concerns influence the design,\ndeployment, and operation of supercomputers even though they experience the\nconsequences. This paper examines the implications of HPC's energy consumption,\nproviding an overview of current trends aimed at improving energy efficiency.\nWe describe how hardware innovations such as energy-efficient processors, novel\nsystem architectures, power management techniques, and advanced scheduling\npolicies do have a direct impact on how applications need to be programmed and\nexecuted on HPC systems. For application developers, understanding how these\nnew systems work and how to analyse and report the performances of their own\nsoftware is critical in the dialog with HPC system designers and\nadministrators. The paper aims to raise awareness about energy efficiency among\nusers, particularly in the high energy physics and astrophysics domains,\noffering practical advice on how to analyse and optimise applications to reduce\ntheir energy consumption without compromising on performance.\n","authors":["Estela Suarez","Jorge Amaya","Martin Frank","Oliver Freyermuth","Maria Girone","Bartosz Kostrzewa","Susanne Pfalzner"],"pdf_url":"https://arxiv.org/pdf/2503.17283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17231v1","updated":"2025-03-21T15:33:09Z","published":"2025-03-21T15:33:09Z","title":"LoGoFair: Post-Processing for Local and Global Fairness in Federated\n  Learning","summary":"  Federated learning (FL) has garnered considerable interest for its capability\nto learn from decentralized data sources. Given the increasing application of\nFL in decision-making scenarios, addressing fairness issues across different\nsensitive groups (e.g., female, male) in FL is crucial. Current research often\nfocuses on facilitating fairness at each client's data (local fairness) or\nwithin the entire dataset across all clients (global fairness). However,\nexisting approaches that focus exclusively on either local or global fairness\nfail to address two key challenges: (\\textbf{CH1}) Under statistical\nheterogeneity, global fairness does not imply local fairness, and vice versa.\n(\\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the\naforementioned challenges, this paper proposes a novel post-processing\nframework for achieving both Local and Global Fairness in the FL context,\nnamely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal\nclassifier under local and global fairness constraints, which strikes the\noptimal accuracy-fairness balance in the probabilistic sense. To address CH2,\nLoGoFair employs a model-agnostic federated post-processing procedure that\nenables clients to collaboratively optimize global fairness while ensuring\nlocal fairness, thereby achieving the optimal fair classifier within FL.\nExperimental results on three real-world datasets further illustrate the\neffectiveness of the proposed LoGoFair framework.\n","authors":["Li Zhang","Chaochao Chen","Zhongxuan Han","Qiyong Zhong","Xiaolin Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.17231v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2501.11006v2","updated":"2025-03-21T15:07:55Z","published":"2025-01-19T10:44:03Z","title":"GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code\n  Generation","summary":"  Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy.\n","authors":["Shashikant Ilager","Lukas Florian Briem","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2501.11006v2.pdf","comment":"Under submission in ACM/IEEE conference, 11 pages"},{"id":"http://arxiv.org/abs/2503.17173v1","updated":"2025-03-21T14:19:45Z","published":"2025-03-21T14:19:45Z","title":"Robustness of deep learning classification to adversarial input on GPUs:\n  asynchronous parallel accumulation is a source of vulnerability","summary":"  The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments.\n","authors":["Sanjif Shanmugavelu","Mathieu Taillefumier","Christopher Culver","Vijay Ganesh","Oscar Hernandez","Ada Sedova"],"pdf_url":"https://arxiv.org/pdf/2503.17173v1.pdf","comment":"Under review at EuroPar 2025"},{"id":"http://arxiv.org/abs/2502.15285v3","updated":"2025-03-21T11:01:05Z","published":"2025-02-21T08:23:32Z","title":"Offload Rethinking by Cloud Assistance for Efficient Environmental Sound\n  Recognition on LPWANs","summary":"  Learning-based environmental sound recognition has emerged as a crucial\nmethod for ultra-low-power environmental monitoring in biological research and\ncity-scale sensing systems. These systems usually operate under limited\nresources and are often powered by harvested energy in remote areas. Recent\nefforts in on-device sound recognition suffer from low accuracy due to resource\nconstraints, whereas cloud offloading strategies are hindered by high\ncommunication costs. In this work, we introduce ORCA, a novel\nresource-efficient cloud-assisted environmental sound recognition system on\nbatteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),\ntargeting wide-area audio sensing applications. We propose a cloud assistance\nstrategy that remedies the low accuracy of on-device inference while minimizing\nthe communication costs for cloud offloading. By leveraging a\nself-attention-based cloud sub-spectral feature selection method to facilitate\nefficient on-device inference, ORCA resolves three key challenges for\nresource-constrained cloud offloading over LPWANs: 1) high communication costs\nand low data rates, 2) dynamic wireless channel conditions, and 3) unreliable\noffloading. We implement ORCA on an energy-harvesting batteryless\nmicrocontroller and evaluate it in a real world urban sound testbed. Our\nresults show that ORCA outperforms state-of-the-art methods by up to $80\n\\times$ in energy savings and $220 \\times$ in latency reduction while\nmaintaining comparable accuracy.\n","authors":["Le Zhang","Quanling Zhao","Run Wang","Shirley Bian","Onat Gungor","Flavio Ponzina","Tajana Rosing"],"pdf_url":"https://arxiv.org/pdf/2502.15285v3.pdf","comment":"Accepted by The 23rd ACM Conference on Embedded Networked Sensor\n  Systems (SenSys '25)"},{"id":"http://arxiv.org/abs/2503.16893v1","updated":"2025-03-21T06:56:35Z","published":"2025-03-21T06:56:35Z","title":"Improving the End-to-End Efficiency of Offline Inference for Multi-LLM\n  Applications Based on Sampling and Simulation","summary":"  As large language models (LLMs) have shown great success in many tasks, they\nare used in various applications. While a lot of works have focused on the\nefficiency of single-LLM application (e.g., offloading, request scheduling,\nparallelism strategy selection), multi-LLM applications receive less attention,\nparticularly in offline inference scenarios. In this work, we aim to improve\nthe offline end-to-end inference efficiency of multi-LLM applications in the\nsingle-node multi-GPU environment. The problem involves two key decisions: (1)\ndetermining which LLMs to run concurrently each time (we may not run all the\nmodels at the same time), and (2) selecting a parallelism strategy to use for\neach LLM. This problem is NP-hard. Naive solutions may not work well because\nthe running time for a model to complete a set of requests depends on the\nrequest workload and the selected parallelism strategy, and they lack an\naccurate model of the running time. As the LLM output lengths are unknown\nbefore running, to estimate the model running time, we propose a\nsampling-then-simulation method which first estimates the output lengths by\nsampling from an empirical cumulative function we obtained from a large dataset\nin advance, and then simulates the LLM inference process accordingly. Based on\nthe simulation, we estimate the per-iteration latencys to get the total\nlatency. A greedy method is proposed to optimize the scheduling of the LLMs in\nthe application across the GPUs. We then propose a framework SamuLLM which\ncontains two phases: planning, which calls the greedy method for an application\nand running, which runs the application and dynamically adjust the model\nscheduling based on the runtime information. Experiments on 3 applications and\na mixed application show that SamuLLM can achieve 1.0-2.4$\\times$ end-to-end\nspeedups compared to the competitors.\n","authors":["Jingzhi Fang","Yanyan Shen","Yue Wang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2503.16893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16875v1","updated":"2025-03-21T06:22:42Z","published":"2025-03-21T06:22:42Z","title":"Federated Cross-Domain Click-Through Rate Prediction With Large Language\n  Model Augmentation","summary":"  Accurately predicting click-through rates (CTR) under stringent privacy\nconstraints poses profound challenges, particularly when user-item interactions\nare sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)\nmethods frequently assume homogeneous feature spaces and rely on centralized\ndata sharing, neglecting complex inter-domain discrepancies and the subtle\ntrade-offs imposed by privacy-preserving protocols. Here, we present Federated\nCross-Domain CTR Prediction with Large Language Model Augmentation\n(FedCCTR-LM), a federated framework engineered to address these limitations by\nsynchronizing data augmentation, representation disentanglement, and adaptive\nprivacy protection. Our approach integrates three core innovations. First, the\nPrivacy-Preserving Augmentation Network (PrivAugNet) employs large language\nmodels to enrich user and item representations and expand interaction\nsequences, mitigating data sparsity and feature incompleteness. Second, the\nIndependent Domain-Specific Transformer with Contrastive Learning (IDST-CL)\nmodule disentangles domain-specific and shared user preferences, employing\nintra-domain representation alignment (IDRA) and crossdomain representation\ndisentanglement (CDRD) to refine the learned embeddings and enhance knowledge\ntransfer across domains. Finally, the Adaptive Local Differential Privacy\n(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal\nbalance between rigorous privacy guarantees and predictive accuracy. Empirical\nevaluations on four real-world datasets demonstrate that FedCCTR-LM\nsubstantially outperforms existing baselines, offering robust,\nprivacy-preserving, and generalizable cross-domain CTR prediction in\nheterogeneous, federated environments.\n","authors":["Jiangcheng Qin","Xueyuan Zhang","Baisong Liu","Jiangbo Qian","Yangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16029v2","updated":"2025-03-21T03:03:15Z","published":"2025-03-20T10:52:50Z","title":"A Controllable and Realistic Framework for Evaluating Microservice\n  Scheduling in Cloud-Edge Continuum","summary":"  The transition from traditional architectures to containerized microservices\nwithin the cloud-edge computing continuum introduces significant challenges,\nparticularly in the efficient scheduling of microservices under dynamic\nconditions. Complex and fluctuating call-graph dependencies, varying cross-node\ncommunication latencies, and unpredictable bandwidth conditions substantially\nimpact the performance and reliability of deployed microservices. Consequently,\naccurately evaluating scheduling policies in such dynamic environments remains\nessential yet challenging due to the lack of realistic and controllable\nevaluation frameworks.\n  In this paper, we propose iDynamics, a novel evaluation framework designed\nexplicitly to address these challenges. iDynamics provides comprehensive and\ncontrollable evaluation capabilities by emulating realistic dynamics, including\nconfigurable call-graph topologies, cross-node communication delays, and\nbandwidth variability. The framework is composed of modular components, such as\nthe Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy\nExtender, enabling fine-grained environmental control and facilitating\nsystematic comparisons of different scheduling strategies. Extensive\nexperiments on a real cloud-edge testbed demonstrate that iDynamics effectively\ncaptures diverse dynamic scenarios encountered in microservice deployments,\noffering a robust solution for evaluating and optimizing policy performance\nunder realistic and controllable conditions.\n","authors":["Ming Chen","Muhammed Tawfiqul Islam","Maria Rodriguez Read","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2503.16029v2.pdf","comment":"14 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.16815v1","updated":"2025-03-21T02:59:25Z","published":"2025-03-21T02:59:25Z","title":"DeFT: Mitigating Data Dependencies for Flexible Communication Scheduling\n  in Distributed Training","summary":"  Communication scheduling aims to reduce communication bottlenecks in data\nparallel training (DP) by maximizing the overlap between computation and\ncommunication. However, existing schemes fall short due to three main issues:\n(1) hard data dependencies break some overlapping between communication and\ncomputation; (2) high coverage rates impair further improvement on performance;\n(3) imbalanced communication/computation times of tensors caused by\npartitioning/fusion strategies cause more bubbles. To address these drawbacks,\nwe propose a new communication scheduling scheme DeFT, whose key insight is to\nmitigate data dependencies and support flexible scheduling in distributed\ntraining. DeFT uncovers new overlapping chances in training by transforming the\nscheduling problem into multiple knapsack problems. Specifically, DeFT\neliminates hard dependencies with delayed updates, reducing the coverage rate\nby adjusting update frequency and utilizing heterogeneous communication links,\nmerging the computation times of backward or forward as the knapsack capacity\nto avoid the negative impact of unbalanced tensors. Additionally, DeFT\npreserves training accuracy by adjusting its scheduling strategy via\nconvergence loss quantification. Extensive experiments with 16 A100 GPUs showed\nthat DeFT achieved speedups of 29% to 115% on three representative benchmarks\ncompared to US-Byte and Bytescheduler with no loss of accuracy.\n","authors":["Lin Meng","Yuzhong Sun"],"pdf_url":"https://arxiv.org/pdf/2503.16815v1.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.16794v1","updated":"2025-03-21T02:06:25Z","published":"2025-03-21T02:06:25Z","title":"Local Ratio based Real-time Job Offloading and Resource Allocation in\n  Mobile Edge Computing","summary":"  Mobile Edge Computing (MEC) has emerged as a promising paradigm enabling\nvehicles to handle computation-intensive and time-sensitive applications for\nintelligent transportation. Due to the limited resources in MEC, effective\nresource management is crucial for improving system performance. While existing\nstudies mostly focus on the job offloading problem and assume that job resource\ndemands are fixed and given apriori, the joint consideration of job offloading\n(selecting the edge server for each job) and resource allocation (determining\nthe bandwidth and computation resources for offloading and processing) remains\nunderexplored. This paper addresses the joint problem for deadline-constrained\njobs in MEC with both communication and computation resource constraints,\naiming to maximize the total utility gained from jobs. To tackle this problem,\nwe propose an approximation algorithm, $\\mathtt{IDAssign}$, with an\napproximation bound of $\\frac{1}{6}$, and experimentally evaluate the\nperformance of $\\mathtt{IDAssign}$ by comparing it to state-of-the-art\nheuristics using a real-world taxi trace and object detection applications.\n","authors":["Chuanchao Gao","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2503.16794v1.pdf","comment":"accepted by The 4th Real-time And intelliGent Edge computing\n  workshop, hold on May 6th, 2025 in Irvine, CA, USA"},{"id":"http://arxiv.org/abs/2503.16783v1","updated":"2025-03-21T01:39:29Z","published":"2025-03-21T01:39:29Z","title":"CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based\n  Proof-of-Stake Blockchains","summary":"  We present a formal analysis of quorum-based State Machine Replication (SMR)\nprotocols in Proof-of-Stake (PoS) systems under a hybrid threat model\ncomprising honest, Byzantine, and rational validators. Our analysis of\ntraditional quorum-based protocols establishes two fundamental impossibility\nresults: (1) in partially synchronous networks, no quorum-based protocol can\nachieve SMR when rational and Byzantine validators comprise more than $1/3$ of\nparticipants, and (2) in synchronous networks, SMR remains impossible when\nrational and Byzantine validators comprise $2/3$ or more of participants.\n  To overcome these limitations, we propose two complementary solutions in our\nhybrid model. First, we introduce a protocol that enforces a bound on the\nvolume of the total transacted amount that is finalized within any time window\n$\\Delta$ and prove that this bound is necessary for secure SMR protocols in our\nmodel. Second, we present the \\emph{strongest chain rule}, which enables\nefficient finalization of transactions when the majority of honest participants\nprovably support the SMR execution. Through empirical analysis of Ethereum and\nCosmos networks, we demonstrate that validator participation consistently\nexceeds the required ${5}/{6}$ threshold, establishing the practical\nfeasibility of our solution in production PoS systems.\n","authors":["Zeta Avarikioti","Eleftherios Kokoris Kogias","Ray Neiheiser","Christos Stefo"],"pdf_url":"https://arxiv.org/pdf/2503.16783v1.pdf","comment":null}]}}