<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-27T00:00:00Z">2025-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust DNN Partitioning and Resource Allocation Under Uncertain
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaojun Nan, Yunchu Han, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In edge intelligence systems, deep neural network (DNN) partitioning and data
offloading can provide real-time task inference for resource-constrained mobile
devices. However, the inference time of DNNs is typically uncertain and cannot
be precisely determined in advance, presenting significant challenges in
ensuring timely task processing within deadlines. To address the uncertain
inference time, we propose a robust optimization scheme to minimize the total
energy consumption of mobile devices while meeting task probabilistic
deadlines. The scheme only requires the mean and variance information of the
inference time, without any prediction methods or distribution functions. The
problem is formulated as a mixed-integer nonlinear programming (MINLP) that
involves jointly optimizing the DNN model partitioning and the allocation of
local CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first
decompose the original problem into two subproblems: resource allocation and
DNN model partitioning. Subsequently, the two subproblems with probability
constraints are equivalently transformed into deterministic optimization
problems using the chance-constrained programming (CCP) method. Finally, the
convex optimization technique and the penalty convex-concave procedure (PCCP)
technique are employed to obtain the optimal solution of the resource
allocation subproblem and a stationary point of the DNN model partitioning
subproblem, respectively. The proposed algorithm leverages real-world data from
popular hardware platforms and is evaluated on widely used DNN models.
Extensive simulations show that our proposed algorithm effectively addresses
the inference time uncertainty with probabilistic deadline guarantees while
minimizing the energy consumption of mobile devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCEP: An Ontology-Based Complex Event Processing Framework for
  Healthcare Decision Support in Big Data Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritesh Chandra, Sonali Agarwal, Shashi Shekhar Kumar, Navjot Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential expansion of real-time data streams across multiple domains
needs the development of effective event detection, correlation, and
decision-making systems. However, classic Complex Event Processing (CEP)
systems struggle with semantic heterogeneity, data interoperability, and
knowledge driven event reasoning in Big Data environments. To solve these
challenges, this research work presents an Ontology based Complex Event
Processing (OCEP) framework, which utilizes semantic reasoning and Big Data
Analytics to improve event driven decision support. The proposed OCEP
architecture utilizes ontologies to support reasoning to event streams. It
ensures compatibility with different data sources and lets you find the events
based on the context. The Resource Description Framework (RDF) organizes event
data, and SPARQL query enables rapid event reasoning and retrieval. The
approach is implemented within the Hadoop environment, which consists of Hadoop
Distributed File System (HDFS) for scalable storage and Apache Kafka for
real-time CEP based event execution. We perform a real-time healthcare analysis
and case study to validate the model, utilizing IoT sensor data for illness
monitoring and emergency responses. This OCEP framework successfully integrates
several event streams, leading to improved early disease detection and aiding
doctors in decision-making. The result shows that OCEP predicts event detection
with an accuracy of 85%. This research work utilizes an OCEP to solve the
problems with semantic interoperability and correlation of complex events in
Big Data analytics. The proposed architecture presents an intelligent, scalable
and knowledge driven event processing framework for healthcare based decision
support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level
  Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Qu, Weihao Zhang, Junfeng Lin, Songchen Ma, Hongyi Li, Luping Shi, Chengzhong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To efficiently support large-scale NNs, multi-level hardware, leveraging
advanced integration and interconnection technologies, has emerged as a
promising solution to counter the slowdown of Moore's law. However, the vast
design space of such hardware, coupled with the complexity of their spatial
hierarchies and organizations, introduces significant challenges for design
space exploration (DSE). Existing DSE tools, which rely on predefined hardware
templates to explore parameters for specific architectures, fall short in
exploring diverse organizations, spatial hierarchies, and architectural
polymorphisms inherent in multi-level hardware. To address these limitations,
we present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure
for domain-specific DSE of multi-level hardware. MLDSE introduces three key
innovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces
a hardware intermediate representation (IR) that can recursively model diverse
multi-level hardware with composable elements at various granularities. 2)
Mapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping
primitives, facilitating the mapping strategy exploration on multi-level
hardware, especially synchronization and cross-level communication; 3)
Simulation: MLDSE supports universal simulator generation based on task-level
event-driven simulation mechanism. It features a hardware-consistent scheduling
algorithm that can handle general task-level resource contention. Through
experiments on LLM workloads, we demonstrate MLDSE's unique capability to
perform three-tier DSE spanning architecture, hardware parameter, and mapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous BFT Consensus Made Wireless 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Liu, Minghui Xu, Tianyi Sun, Xiuzhen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for
their robustness in unpredictable environments without relying on timing
assumptions, are becoming increasingly vital for wireless applications. While
these protocols have proven effective in wired networks, their adaptation to
wireless environments presents significant challenges. Asynchronous BFT
consensus, characterized by its N parallel consensus components (e.g.,
asynchronous Byzantine agreement, reliable broadcast), suffers from high
message complexity, leading to network congestion and inefficiency, especially
in resource-constrained wireless networks. Asynchronous Byzantine agreement
(ABA) protocols, a foundational component of asynchronous BFT, require careful
balancing of message complexity and cryptographic overhead to achieve efficient
implementation in wireless settings. Additionally, the absence of dedicated
testbeds for asynchronous wireless BFT consensus protocols hinders development
and performance evaluation. To address these challenges, we propose a consensus
batching protocol (ConsensusBatcher), which supports both vertical and
horizontal batching of multiple parallel consensus components. We leverage
ConsensusBatcher to adapt three asynchronous BFT consensus protocols
(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained
wireless networks. To evaluate the performance of ConsensusBatcher-enabled
consensus protocols in wireless environments, we develop and open-source a
testbed for deployment and performance assessment of these protocols. Using
this testbed, we demonstrate that ConsensusBatcher-based consensus reduces
latency by 48% to 59% and increases throughput by 48% to 62% compared to
baseline consensus protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICDCS 2025, 11 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PilotANN: Memory-Bounded GPU Acceleration for Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Gui, Peiqi Yin, Xiao Yan, Chaorui Zhang, Weixi Zhang, James Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) has become fundamental to modern
deep learning applications, having gained particular prominence through its
integration into recent generative models that work with increasingly complex
datasets and higher vector dimensions. Existing CPU-only solutions, even the
most efficient graph-based ones, struggle to meet these growing computational
demands, while GPU-only solutions face memory constraints. As a solution, we
propose PilotANN, a hybrid CPU-GPU system for graph-based ANNS that utilizes
both CPU's abundant RAM and GPU's parallel processing capabilities. Our
approach decomposes the graph traversal process of top-$k$ search into three
stages: GPU-accelerated subgraph traversal using SVD-reduced vectors, CPU
refinement and precise search using complete vectors. Furthermore, we introduce
fast entry selection to improve search starting points while maximizing GPU
utilization. Experimental results demonstrate that PilotANN achieves $3.9 - 5.4
\times$ speedup in throughput on 100-million scale datasets, and is able to
handle datasets up to $12 \times$ larger than the GPU memory. We offer a
complete open-source implementation at https://github.com/ytgui/PilotANN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous
  Processor Co-Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunquan Gao, Zhiguo Zhang, Praveen Kumar Donta, Chinmaya Kumar Dehury, Xiujun Wang, Dusit Niyato, Qiyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are increasingly deployed across diverse
industries, driving demand for mobile device support. However, existing mobile
inference frameworks often rely on a single processor per model, limiting
hardware utilization and causing suboptimal performance and energy efficiency.
Expanding DNN accessibility on mobile platforms requires adaptive,
resource-efficient solutions to meet rising computational needs without
compromising functionality. Parallel inference of multiple DNNs on
heterogeneous processors remains challenging. Some works partition DNN
operations into subgraphs for parallel execution across processors, but these
often create excessive subgraphs based only on hardware compatibility,
increasing scheduling complexity and memory overhead.
  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)
strategy for optimizing multi-DNN inference on mobile heterogeneous processors.
ADMS constructs an optimal subgraph partitioning strategy offline, balancing
hardware operation support and scheduling granularity, and uses a
processor-state-aware algorithm to dynamically adjust workloads based on
real-time conditions. This ensures efficient workload distribution and
maximizes processor utilization. Experiments show ADMS reduces multi-DNN
inference latency by 4.04 times compared to vanilla frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloud Resource Allocation with Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Boghani, Emin Kirimlioglu, Amrita Moturi, Hao-Ting Tso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a convex optimization framework for overcoming the limitations of
Kubernetes Cluster Autoscaler by intelligently allocating diverse cloud
resources while minimizing costs and fragmentation. Current Kubernetes scaling
mechanisms are restricted to homogeneous scaling of existing node types,
limiting cost-performance optimization possibilities. Our matrix-based model
captures resource demands, costs, and capacity constraints in a unified
mathematical framework. A key contribution is our logarithmic approximation to
the indicator function, which enables dynamic node type selection while
maintaining problem convexity. Our approach balances cost optimization with
operational complexity through interior-point methods. Experiments with
real-world Kubernetes workloads demonstrate reduced costs and improved resource
utilization compared to conventional Cluster Autoscaler strategies that can
only scale up or down existing node pools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Selective Homomorphic Encryption Approach for Faster
  Privacy-Preserving Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Korkmaz, Praveen Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a machine learning method that supports training models
on decentralized devices or servers, where each holds its local data, removing
the need for data exchange. This approach is especially useful in healthcare,
as it enables training on sensitive data without needing to share them. The
nature of federated learning necessitates robust security precautions due to
data leakage concerns during communication. To address this issue, we propose a
new approach that employs selective encryption, homomorphic encryption,
differential privacy, and bit-wise scrambling to minimize data leakage while
achieving good execution performance. Our technique , FAS (fast and secure
federated learning) is used to train deep learning models on medical imaging
data. We implemented our technique using the Flower framework and compared with
a state-of-the-art federated learning approach that also uses selective
homomorphic encryption. Our experiments were run in a cluster of eleven
physical machines to create a real-world federated learning scenario on
different datasets. We observed that our approach is up to 90\% faster than
applying fully homomorphic encryption on the model weights. In addition, we can
avoid the pretraining step that is required by our competitor and can save up
to 46% in terms of total execution time. While our approach was faster, it
obtained similar security results as the competitor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning during Learning: An Efficient Federated Machine Unlearning
  Method <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Federated Learning (FL) has garnered significant attention
as a distributed machine learning paradigm. To facilitate the implementation of
the right to be forgotten, the concept of federated machine unlearning (FMU)
has also emerged. However, current FMU approaches often involve additional
time-consuming steps and may not offer comprehensive unlearning capabilities,
which renders them less practical in real FL scenarios. In this paper, we
introduce FedAU, an innovative and efficient FMU framework aimed at overcoming
these limitations. Specifically, FedAU incorporates a lightweight auxiliary
unlearning module into the learning process and employs a straightforward
linear operation to facilitate unlearning. This approach eliminates the
requirement for extra time-consuming steps, rendering it well-suited for FL.
Furthermore, FedAU exhibits remarkable versatility. It not only enables
multiple clients to carry out unlearning tasks concurrently but also supports
unlearning at various levels of granularity, including individual data samples,
specific classes, and even at the client level. We conducted extensive
experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the
performance of FedAU. The results demonstrate that FedAU effectively achieves
the desired unlearning effect while maintaining model accuracy. Our code is
availiable at https://github.com/Liar-Mask/FedAU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TileLink: Generating Efficient Compute-Communication Overlapping Kernels
  using Tile-Centric Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large deep learning models have achieved state-of-the-art performance in a
wide range of tasks. These models often necessitate distributed systems for
efficient training and inference. The fundamental building blocks for
distributed model execution are intra-layer parallel operators. The most
effective approach to enhancing the performance of intra-layer parallel
operators involves overlapping computation with communication. The overlapping
can be achieved through either operator decomposition or kernel fusion. While
decomposing operators is straightforward to implement, it often results in
suboptimal performance. On the other hand, fusing communication kernels with
compute kernels demands significant expertise and is error-prone.
  In this paper, we propose TileLink to enable efficient compilation and
generation of overlapped compute-communication kernels. TileLink is composed of
frontend and backend. In the frontend, TileLink decouples the design space of
communication and computation, linking these two parts via tile-centric
primitives. In the backend, TileLink translates these primitives into low-level
communication instructions, integrating the communication and computation
components to achieve overlapped execution. In experiments, TileLink achieves
from $1.17\times$ to $20.76\times$ speedup to non-overlapping baseline and
achieves performance comparable to state-of-the-art overlapping libraries on
GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Renaming in distributed certification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Bousquet, Louis Esperet, Laurent Feuilloley, Sébastien Zeitoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local certification is the area of distributed network computing asking the
following question: How to certify to the nodes of a network that a global
property holds, if they are limited to a local verification?
  In this area, it is often essential to have identifiers, that is, unique
integers assigned to the nodes. In this short paper, we show how to reduce the
range of the identifiers, in three different settings. More precisely, we show
how to rename identifiers in the classical local certification setting, when we
can (resp.\ cannot) choose the new identifiers, and we show how a global
certificate can help to encode very compactly a new identifier assignment that
is not injective in general, but still useful in applications.
  We conclude with a number of applications of these results: For every $\ell$,
there are local certification schemes for the properties of having clique
number at most $\ell$, having diameter at most $\ell$, and having independence
number at most~2, with certificates of size $O(n)$. We also show that there is
a global certification scheme for bipartiteness with certificates of size
$O(n)$. All these results are optimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure: v2: added a number of applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco De Sclavis, Giuseppe Galano, Aldo Glielmo, Matteo Nardelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Large Model Training through Overlapped Activation
  Recomputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Chen, Wenjie Zhang, Shuibing He, Weijian Chen, Siling Yang, Kexin Huang, Yanlong Yin, Xuan Zhan, Yingjie Gu, Zhuwei Peng, Yi Zheng, Zhefeng Wang, Gang Chen Yingjie Gu, Zhuwei Peng, Kexin Huang, Xuan Zhan, Weijian Chen, Yi Zheng, Zhefeng Wang, Yanlong Yin, Gang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large model training often uses recomputation to alleviate memory pressure
and pipelines to exploit the parallelism of data, tensors, and devices.
However, existing recomputation approaches may incur high overhead when
training real-world models, as they are executed on demand in the critical
training path. In this paper, we present Lynx, a new recomputation framework to
reduce overhead by overlapping recomputation with communication in training
pipelines. To reduce the large search space for recomputation strategies, we
propose a heuristic-based recomputation scheduling algorithm, which is based on
the observation that there are identical structures in large DNN models so that
we can apply the same scheduling policy to all such structures. Additionally,
we propose a recomputation-aware model partitioning method to balance each
stage's execution time for improved training throughput. Our comprehensive
evaluation using GPT models with 1.3B-23B parameters shows that Lynx
outperforms existing recomputation approaches by up to 1.37x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> of Disaggregated Memory: Cross-layer Technique Insights for
  Next-Generation Datacenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wang, Chao Li, Taolei Wang, Jinyang Guo, Hanzhang Yang, Yiming Zhuansun, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing scale of data requires efficient memory subsystems with large
memory capacity and high memory performance. Disaggregated architecture has
become a promising solution for today's cloud and edge computing for its
scalability and elasticity. As a critical part of disaggregation, disaggregated
memory faces many design challenges in many dimensions, including hardware
scalability, architecture structure, software system design, application
programmability, resource allocation, power management, etc. These challenges
inspire a number of novel solutions at different system levels to improve
system efficiency. In this paper, we provide a comprehensive review of
disaggregated memory, including the methodology and technologies of
disaggregated memory system foundation, optimization, and management. We study
the technical essentials of disaggregated memory systems and analyze them from
the hardware, architecture, system, and application levels. Then, we compare
the design details of typical cross-layer designs on disaggregated memory.
Finally, we discuss the challenges and opportunities of future disaggregated
memory works that serve better for next-generation elastic and efficient
datacenters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of the Impact of Federated Learning on Machine
  Learning Model Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Yang, Zhuoran Wang, Benson Chou, Sophie Xu, Hao Wang, Jingxian Wang, Qizhen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables distributed ML model training on private user
data at the global scale. Despite the potential of FL demonstrated in many
domains, an in-depth view of its impact on model accuracy remains unclear. In
this paper, we investigate, systematically, how this learning paradigm can
affect the accuracy of state-of-the-art ML models for a variety of ML tasks. We
present an empirical study that involves various data types: text, image,
audio, and video, and FL configuration knobs: data distribution, FL scale,
client sampling, and local and global computations. Our experiments are
conducted in a unified FL framework to achieve high fidelity, with substantial
human efforts and resource investments. Based on the results, we perform a
quantitative analysis of the impact of FL, and highlight challenging scenarios
where applying FL degrades the accuracy of the model drastically and identify
cases where the impact is negligible. The detailed and extensive findings can
benefit practical deployments and future development of FL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Some new methods <span class="chip" style="font-size: 60%">244</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,
generating motion conditioned on both content and style from multiple
modalities. Unlike existing approaches that either focus on generating diverse
motion content or transferring style from sequences, StyleMotif seamlessly
synthesizes motion across a wide range of content while incorporating stylistic
cues from multi-modal inputs, including motion, text, image, video, and audio.
To achieve this, we introduce a style-content cross fusion mechanism and align
a style encoder with a pre-trained multi-modal model, ensuring that the
generated motion accurately captures the reference style while preserving
realism. Extensive experiments demonstrate that our framework surpasses
existing methods in stylized motion generation and exhibits emergent
capabilities for multi-modal motion stylization, enabling more nuanced motion
synthesis. Source code and pre-trained models will be released upon acceptance.
Project Page: https://stylemotif.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://stylemotif.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-SCore: A Stable Registration-based Framework for 3D Shape
  Correspondence <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing character shape correspondence is a critical and fundamental
task in computer vision and graphics, with diverse applications including
re-topology, attribute transfer, and shape interpolation. Current dominant
functional map methods, while effective in controlled scenarios, struggle in
real situations with more complex challenges such as non-isometric shape
discrepancies. In response, we revisit registration-for-correspondence methods
and tap their potential for more stable shape correspondence estimation. To
overcome their common issues including unstable deformations and the necessity
for careful pre-alignment or high-quality initial 3D correspondences, we
introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape
Correspondence. We first re-purpose a foundation model for 2D character
correspondence that ensures reliable and stable 2D mappings. Crucially, we
propose a novel Semantic Flow Guided Registration approach that leverages 2D
correspondence to guide mesh deformations. Our framework significantly
surpasses existing methods in challenging scenarios, and brings possibilities
for a wide array of real applications, as demonstrated in our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025. Homepage:
  https://haolinliu97.github.io/Stable-Score/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single
  Video <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Yifan Yao, Albert J. Zhai, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a unified approach to understanding dynamic scenes from
casual videos. Large pretrained vision foundation models, such as
vision-language, video depth prediction, motion tracking, and segmentation
models, offer promising capabilities. However, training a single model for
comprehensive 4D understanding remains challenging. We introduce Uni4D, a
multi-stage optimization framework that harnesses multiple pretrained models to
advance dynamic 3D modeling, including static/dynamic reconstruction, camera
pose estimation, and dense 3D motion tracking. Our results show
state-of-the-art performance in dynamic 4D modeling with superior visual
quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the
effectiveness of repurposing visual foundation models for 4D understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Project page (with code):
  https://davidyao99.github.io/uni4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we aim to compress the vision tokens of a Large Vision Language
Model (LVLM) into a representation that is simultaneously suitable for (a)
generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is
storage-efficient. We propose a novel compression approach, called Fwd2Bot,
that uses the LVLM itself to compress the visual information in a task-agnostic
manner. At the core of Fwd2bot there exists a "double-forward pass" training
strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates
a bottleneck by condensing the visual information into a small number of
summary tokens. Then, using the same LLM, the second forward pass processes the
language instruction(s) alongside the summary tokens, used as a direct
replacement for the image ones. The training signal is provided by two losses:
an autoregressive one applied after the second pass that provides a direct
optimization objective for compression, and a contrastive loss, applied after
the first pass, that further boosts the representation strength, especially for
discriminative tasks. The training is further enhanced by stage-specific
adapters. We accompany the proposed method by an in-depth ablation study.
Overall, Fwd2Bot results in highly-informative compressed representations
suitable for both generative and discriminative tasks. For generative tasks, we
offer a 2x higher compression rate without compromising the generative
capabilities, setting a new state-of-the-art result. For discriminative tasks,
we set a new state-of-the-art on image retrieval and compositionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTRL-O: Language-Controllable Object-Centric Visual Representation
  Learning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Didolkar, Andrii Zadaianchuk, Rabiul Awal, Maximilian Seitzer, Efstratios Gavves, Aishwarya Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric representation learning aims to decompose visual scenes into
fixed-size vectors called "slots" or "object files", where each slot captures a
distinct object. Current state-of-the-art object-centric models have shown
remarkable success in object discovery in diverse domains, including complex
real-world scenes. However, these models suffer from a key limitation: they
lack controllability. Specifically, current object-centric models learn
representations based on their preconceived understanding of objects, without
allowing user input to guide which objects are represented. Introducing
controllability into object-centric models could unlock a range of useful
capabilities, such as the ability to extract instance-specific representations
from a scene. In this work, we propose a novel approach for user-directed
control over slot representations by conditioning slots on language
descriptions. The proposed ConTRoLlable Object-centric representation learning
approach, which we term CTRL-O, achieves targeted object-language binding in
complex real-world scenes without requiring mask supervision. Next, we apply
these controllable slot representations on two downstream vision language
tasks: text-to-image generation and visual question answering. The proposed
approach enables instance-specific text-to-image generation and also achieves
strong performance on visual question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release
  Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the reliability and effectiveness of software release decisions is
critical, particularly in safety-critical domains like automotive systems.
Precise analysis of release validation data, often presented in tabular form,
plays a pivotal role in this process. However, traditional methods that rely on
manual analysis of extensive test datasets and validation metrics are prone to
delays and high costs. Large Language Models (LLMs) offer a promising
alternative but face challenges in analytical reasoning, contextual
understanding, handling out-of-scope queries, and processing structured test
data consistently; limitations that hinder their direct application in
safety-critical scenarios. This paper introduces GateLens, an LLM-based tool
for analyzing tabular data in the automotive domain. GateLens translates
natural language queries into Relational Algebra (RA) expressions and then
generates optimized Python code. It outperforms the baseline system on
benchmarking datasets, achieving higher F1 scores and handling complex and
ambiguous queries with greater robustness. Ablation studies confirm the
critical role of the RA module, with performance dropping sharply when omitted.
Industrial evaluations reveal that GateLens reduces analysis time by over 80%
while maintaining high accuracy and reliability. As demonstrated by presented
results, GateLens achieved high performance without relying on few-shot
examples, showcasing strong generalization across various query types from
diverse company roles. Insights from deploying GateLens with a partner
automotive company offer practical guidance for integrating AI into critical
workflows such as release validation. Results show that by automating test
result analysis, GateLens enables faster, more informed, and dependable release
decisions, and can thus advance software scalability and reliability in
automotive systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large
  Reasoning Models with Iterative Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely
primarily on parametric knowledge, limiting factual accuracy. While recent
works equip reinforcement learning (RL)-based LRMs with retrieval capabilities,
they suffer from overthinking and lack robustness in reasoning, reducing their
effectiveness in question answering (QA) tasks. To address this, we propose
ReaRAG, a factuality-enhanced reasoning model that explores diverse queries
without excessive iterations. Our solution includes a novel data construction
framework with an upper bound on the reasoning chain length. Specifically, we
first leverage an LRM to generate deliberate thinking, then select an action
from a predefined action space (Search and Finish). For Search action, a query
is executed against the RAG engine, where the result is returned as observation
to guide reasoning steps later. This process iterates until a Finish action is
chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach
outperforms existing baselines on multi-hop QA. Further analysis highlights its
strong reflective ability to recognize errors and refine its reasoning
trajectory. Our study enhances LRMs' factuality while effectively integrating
robust reasoning for Retrieval-Augmented Generation (RAG).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collab: Controlled Decoding using Mixture of Agents for LLM Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment of Large Language models (LLMs) is crucial for safe and trustworthy
deployment in applications. Reinforcement learning from human feedback (RLHF)
has emerged as an effective technique to align LLMs to human preferences and
broader utilities, but it requires updating billions of model parameters, which
is computationally expensive. Controlled Decoding, by contrast, provides a
mechanism for aligning a model at inference time without retraining. However,
single-agent decoding approaches often struggle to adapt to diverse tasks due
to the complexity and variability inherent in these tasks. To strengthen the
test-time performance w.r.t the target task, we propose a mixture of
agent-based decoding strategies leveraging the existing off-the-shelf aligned
LLM policies. Treating each prior policy as an agent in the spirit of mixture
of agent collaboration, we develop a decoding method that allows for
inference-time alignment through a token-level selection strategy among
multiple agents. For each token, the most suitable LLM is dynamically chosen
from a pool of models based on a long-term utility metric. This
policy-switching mechanism ensures optimal model selection at each step,
enabling efficient collaboration and alignment among LLMs during decoding.
Theoretical analysis of our proposed algorithm establishes optimal performance
with respect to the target task represented via a target reward for the given
off-the-shelf models. We conduct comprehensive empirical evaluations with
open-source aligned models on diverse tasks and preferences, which demonstrates
the merits of this approach over single-agent decoding baselines. Notably,
Collab surpasses the current SoTA decoding strategy, achieving an improvement
of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier dimensions favor frequent tokens in language model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iuri Macocco, Nora Graichen, Gemma Boleda, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study last-layer outlier dimensions, i.e.dimensions that display extreme
activations for the majority of inputs. We show that outlier dimensions arise
in many different modern language models, and trace their function back to the
heuristic of constantly predicting frequent words. We further show how a model
can block this heuristic when it is not contextually appropriate, by assigning
a counterbalancing weight mass to the remaining dimensions, and we investigate
which model parameters boost outlier dimensions and when they arise during
training. We conclude that outlier dimensions are a specialized mechanism
discovered by many distinct models to implement a useful token prediction
heuristic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elementwise Layer Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Stollenwerk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer
Normalization. Although the method is empirically well-motivated and appealing
from a practical point of view, it lacks a theoretical foundation. In this
work, we derive DyT mathematically and show that a well-defined approximation
is needed to do so. By dropping said approximation, an alternative element-wise
transformation is obtained, which we call Elementwise Layer Normalization
(ELN). We demonstrate that ELN resembles Layer Normalization more accurately
than DyT does.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyue Xie, George Z. Wei, Avik Kuthiala, Ce Zheng, Ananya Bal, Mosam Dabhi, Liting Wen, Taru Rustagi, Ethan Lai, Sushil Khyalia, Rohan Choudhury, Morteza Ziyadi, Xu Zhang, Hao Yang, László A. Jeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier models have either been language-only or have primarily focused on
vision and language modalities. Although recent advancements in models with
vision and audio understanding capabilities have shown substantial progress,
the field lacks a standardized evaluation framework for thoroughly assessing
their cross-modality perception performance. We introduce MAVERIX~(Multimodal
Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and
2,556 questions explicitly designed to evaluate multimodal models through tasks
that necessitate close integration of video and audio information. MAVERIX
uniquely provides models with audiovisual tasks, closely mimicking the
multimodal perceptual experiences available to humans during inference and
decision-making processes. To our knowledge, MAVERIX is the first benchmark
aimed explicitly at assessing comprehensive audiovisual integration.
Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show
performance approaching human levels (around 70% accuracy), while human experts
reach near-ceiling performance (95.1%). With standardized evaluation protocols,
a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a
challenging testbed for advancing audiovisual multimodal intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model
  for High-Fidelity Histology Nuclei Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of cell nuclei in histopathology images is essential
for numerous biomedical research and clinical applications. However, existing
cell nucleus segmentation methods only consider a single dataset (i.e., primary
domain), while neglecting to leverage supplementary data from diverse sources
(i.e., auxiliary domains) to reduce overfitting and enhance the performance.
Although incorporating multiple datasets could alleviate overfitting, it often
exacerbates performance drops caused by domain shifts. In this work, we
introduce Adversarial Multi-domain Alignment of Segment Anything Model
(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these
obstacles through two key innovations. First, we propose a Conditional Gradient
Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features
from diverse domains to promote domain-invariant representation learning while
preserving crucial discriminative features for the primary dataset. Second, we
address SAM's inherent low-resolution output by designing a High-Resolution
Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in
order to capture intricate nuclei boundaries in high-resolution histology
images. To the best of our knowledge, this is the first attempt to adapt SAM
for multi-dataset learning with application to histology nuclei segmentation.
We validate our method on several publicly available datasets, demonstrating
consistent and significant improvements over state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Rendering Distillation: Adapting Stable Diffusion for
  Instant Text-to-Mesh Generation without 3D Data <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Ma, Xinyue Liang, Rongyuan Wu, Xiangyu Zhu, Zhen Lei, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is highly desirable to obtain a model that can generate high-quality 3D
meshes from text prompts in just seconds. While recent attempts have adapted
pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into
generators of 3D representations (e.g., Triplane), they often suffer from poor
quality due to the lack of sufficient high-quality 3D training data. Aiming at
overcoming the data shortage, we propose a novel training scheme, termed as
Progressive Rendering Distillation (PRD), eliminating the need for 3D
ground-truths by distilling multi-view diffusion models and adapting SD into a
native 3D generator. In each iteration of training, PRD uses the U-Net to
progressively denoise the latent from random noise for a few steps, and in each
step it decodes the denoised latent into 3D output. Multi-view diffusion
models, including MVDream and RichDreamer, are used in joint with SD to distill
text-consistent textures and geometries into the 3D outputs through score
distillation. Since PRD supports training without 3D ground-truths, we can
easily scale up the training data and improve generation quality for
challenging text prompts with creative concepts. Meanwhile, PRD can accelerate
the inference speed of the generation model in just a few steps. With PRD, we
train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\%$
trainable parameters to adapt SD for Triplane generation. TriplaneTurbo
outperforms previous text-to-3D generators in both efficiency and quality.
Specifically, it can produce high-quality 3D meshes in 1.2 seconds and
generalize well for challenging text input. The code is available at
https://github.com/theEricMa/TriplaneTurbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025.
  Code:https://github.com/theEricMa/TriplaneTurbo.
  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku
  with Self-Play and Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have shown significant
advancements in natural language processing (NLP), with strong capa-bilities in
generation, comprehension, and rea-soning. These models have found applications
in education, intelligent decision-making, and gaming. However, effectively
utilizing LLMs for strategic planning and decision-making in the game of Gomoku
remains a challenge. This study aims to develop a Gomoku AI system based on
LLMs, simulating the human learning process of playing chess. The system is
de-signed to understand and apply Gomoku strat-egies and logic to make rational
decisions. The research methods include enabling the model to "read the board,"
"understand the rules," "select strategies," and "evaluate positions," while
en-hancing its abilities through self-play and rein-forcement learning. The
results demonstrate that this approach significantly improves the se-lection of
move positions, resolves the issue of generating illegal positions, and reduces
pro-cess time through parallel position evaluation. After extensive self-play
training, the model's Gomoku-playing capabilities have been notably enhanced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent IoT Attack Detection Design via ODLLM with Feature
  Ranking-based Knowledge Base 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satvik Verma, Qun Wang, E. Wes Bethel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of Internet of Things (IoT) devices has introduced
significant cybersecurity challenges, particularly with the increasing
frequency and sophistication of Distributed Denial of Service (DDoS) attacks.
Traditional machine learning (ML) techniques often fall short in detecting such
attacks due to the complexity of blended and evolving patterns. To address
this, we propose a novel framework leveraging On-Device Large Language Models
(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for
intelligent IoT network attack detection. By implementing feature ranking
techniques and constructing both long and short KBs tailored to model
capacities, the proposed framework ensures efficient and accurate detection of
DDoS attacks while overcoming computational and privacy limitations. Simulation
results demonstrate that the optimized framework achieves superior accuracy
across diverse attack types, especially when using compact models in edge
computing environments. This work provides a scalable and secure solution for
real-time IoT security, advancing the applicability of edge intelligence in
cybersecurity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMI-LINGUA: Expert Annotated Large-Scale <span class="highlight-title">Dataset</span> for Multitask NLP in
  Hindi-English Code-Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajvee Sheth, Himanshu Beniwal, Mayank Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of digital communication has driven the widespread use of
code-mixing, particularly Hindi-English, in multilingual communities. Existing
datasets often focus on romanized text, have limited scope, or rely on
synthetic data, which fails to capture realworld language nuances. Human
annotations are crucial for assessing the naturalness and acceptability of
code-mixed text. To address these challenges, We introduce COMI-LINGUA, the
largest manually annotated dataset for code-mixed text, comprising 100,970
instances evaluated by three expert annotators in both Devanagari and Roman
scripts. The dataset supports five fundamental NLP tasks: Language
Identification, Matrix Language Identification, Part-of-Speech Tagging, Named
Entity Recognition, and Translation. We evaluate LLMs on these tasks using
COMILINGUA, revealing limitations in current multilingual modeling strategies
and emphasizing the need for improved code-mixed text processing capabilities.
COMI-LINGUA is publically availabe at:
https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitive Science-Inspired Evaluation of Core Capabilities for Object
  Understanding in AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José Hernández-Orallo, Lucy Cheke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core components of our world models is 'intuitive physics' - an
understanding of objects, space, and causality. This capability enables us to
predict events, plan action and navigate environments, all of which rely on a
composite sense of objecthood. Despite its importance, there is no single,
unified account of objecthood, though multiple theoretical frameworks provide
insights. In the first part of this paper, we present a comprehensive overview
of the main theoretical frameworks in objecthood research - Gestalt psychology,
enactive cognition, and developmental psychology - and identify the core
capabilities each framework attributes to object understanding, as well as what
functional roles they play in shaping world models in biological agents. Given
the foundational role of objecthood in world modelling, understanding
objecthood is also essential in AI. In the second part of the paper, we
evaluate how current AI paradigms approach and test objecthood capabilities
compared to those in cognitive science. We define an AI paradigm as a
combination of how objecthood is conceptualised, the methods used for studying
objecthood, the data utilised, and the evaluation techniques. We find that,
whilst benchmarks can detect that AI systems model isolated aspects of
objecthood, the benchmarks cannot detect when AI systems lack functional
integration across these capabilities, not solving the objecthood challenge
fully. Finally, we explore novel evaluation approaches that align with the
integrated vision of objecthood outlined in this paper. These methods are
promising candidates for advancing from isolated object capabilities toward
general-purpose AI with genuine object understanding in real-world contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Assembly Learning with Heterogeneous Layer Weight Merging <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Kai Zhang, Jin Wang, Xu-Xiang Zhong, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging acquires general capabilities without extra data or training by
combining multiple models' parameters. Previous approaches achieve linear mode
connectivity by aligning parameters into the same loss basin using permutation
invariance. In this paper, we introduce Model Assembly Learning (MAL), a novel
paradigm for model merging that iteratively integrates parameters from diverse
models in an open-ended model zoo to enhance the base model's capabilities.
Unlike previous works that require identical architectures, MAL allows the
merging of heterogeneous architectures and selective parameters across layers.
Specifically, the base model can incorporate parameters from different layers
of multiple pre-trained models. We systematically investigate the conditions
and fundamental settings of heterogeneous parameter merging, addressing all
possible mismatches in layer widths between the base and target models.
Furthermore, we establish key laws and provide practical guidelines for
effectively implementing MAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Workshop on Neural Network Weights as a New Data Modality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Potential of Past Research: Using Generative AI to
  Reconstruct Healthcare Simulation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Monks, Alison Harper, Amy Heather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete-event simulation (DES) is widely used in healthcare Operations
Research, but the models themselves are rarely shared. This limits their
potential for reuse and long-term impact in the modelling and healthcare
communities. This study explores the feasibility of using generative artificial
intelligence (AI) to recreate published models using Free and Open Source
Software (FOSS), based on the descriptions provided in an academic journal.
Using a structured methodology, we successfully generated, tested and
internally reproduced two DES models, including user interfaces. The reported
results were replicated for one model, but not the other, likely due to missing
information on distributions. These models are substantially more complex than
AI-generated DES models published to date. Given the challenges we faced in
prompt engineering, code generation, and model testing, we conclude that our
iterative approach to model development, systematic comparison and testing, and
the expertise of our team were necessary to the success of our recreated
simulation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fully Automated Decision-Making Systems for Greenhouse Control:
  Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongshuai Liu, Taeyeong Choi, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has been successful in building control policies to drive a
complex system to desired states in various applications (e.g. games, robotics,
etc.). To be specific, a number of parameters of policy can be automatically
optimized from the observations of environment to be able to generate a
sequence of decisions leading to the best performance. In this survey paper, we
particularly explore such policy-learning techniques for another unique,
practical use-case scenario--farming, in which critical decisions (e.g., water
supply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,
damage to plants) while maximizing the revenue (e.g., healthy crops) in the
end. We first provide a broad overview of latest studies on it to identify not
only domain-specific challenges but opportunities with potential solutions,
some of which are suggested as promising directions for future research. Also,
we then introduce our successful approach to being ranked second among 46 teams
at the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to
discuss the lessons learned about important considerations for design to create
autonomous farm-management systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in
  Morocco 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassir Lairgi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate determination of the beginning of each Hijri month is essential
for religious, cultural, and administrative purposes. Manazel (The code and
datasets are available at https://github.com/lairgiyassir/manazel) addresses
this challenge in Morocco by leveraging 13 years of crescent visibility data to
refine the ODEH criterion, a widely used standard for lunar crescent visibility
prediction. The study integrates two key features, the Arc of Vision (ARCV) and
the total width of the crescent (W), to enhance the accuracy of lunar
visibility assessments. A machine learning approach utilizing the Logistic
Regression algorithm is employed to classify crescent visibility conditions,
achieving a predictive accuracy of 98.83%. This data-driven methodology offers
a robust and reliable framework for determining the start of the Hijri month,
comparing different data classification tools, and improving the consistency of
lunar calendar calculations in Morocco. The findings demonstrate the
effectiveness of machine learning in astronomical applications and highlight
the potential for further enhancements in the modeling of crescent visibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities
in LLMs through reinforcement learning (RL) with rule-based rewards. Building
on this idea, we are the first to explore how rule-based RL can enhance the
reasoning capabilities of multimodal large language models (MLLMs) for graphic
user interface (GUI) action prediction tasks. To this end, we curate a small
yet high-quality dataset of 136 challenging tasks, encompassing five common
action types on mobile devices. We also introduce a unified rule-based action
reward, enabling model optimization via policy-based algorithms such as Group
Relative Policy Optimization (GRPO). Experimental results demonstrate that our
proposed data-efficient model, UI-R1-3B, achieves substantial improvements on
both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID
benchmark AndroidControl, the action type accuracy improves by 15%, while
grounding accuracy increases by 10.3%, compared with the base model (i.e.
Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model
surpasses the base model by 6.0% and achieves competitive performance with
larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning
(SFT) on 76K data. These results underscore the potential of rule-based
reinforcement learning to advance GUI understanding and control, paving the way
for future research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Measure Based Generalizable Approach to Understandability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful agent-human partnerships require that any agent generated
information is understandable to the human, and that the human can easily steer
the agent towards a goal. Such effective communication requires the agent to
develop a finer-level notion of what is understandable to the human.
State-of-the-art agents, including LLMs, lack this detailed notion of
understandability because they only capture average human sensibilities from
the training data, and therefore afford limited steerability (e.g., requiring
non-trivial prompt engineering).
  In this paper, instead of only relying on data, we argue for developing
generalizable, domain-agnostic measures of understandability that can be used
as directives for these agents. Existing research on understandability measures
is fragmented, we survey various such efforts across domains, and lay a
cognitive-science-rooted groundwork for more coherent and domain-agnostic
research investigations in future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenEdit: Compounding Operators and Continuous Improvement to Tackle
  Text-to-SQL in the Enterprise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karime Maamari, Connor Landy, Amine Mhedhbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Text-to-SQL, driven by large language models, are
democratizing data access. Despite these advancements, enterprise deployments
remain challenging due to the need to capture business-specific knowledge,
handle complex queries, and meet expectations of continuous improvements. To
address these issues, we designed and implemented GenEdit: our Text-to-SQL
generation system that improves with user feedback. GenEdit builds and
maintains a company-specific knowledge set, employs a pipeline of operators
decomposing SQL generation, and uses feedback to update its knowledge set to
improve future SQL generations.
  We describe GenEdit's architecture made of two core modules: (i) decomposed
SQL generation; and (ii) knowledge set edits based on user feedback. For
generation, GenEdit leverages compounding operators to improve knowledge
retrieval and to create a plan as chain-of-thought steps that guides
generation. GenEdit first retrieves relevant examples in an initial retrieval
stage where original SQL queries are decomposed into sub-statements, clauses or
sub-queries. It then also retrieves instructions and schema elements. Using the
retrieved contextual information, GenEdit then generates step-by-step plan in
natural language on how to produce the query. Finally, GenEdit uses the plan to
generate SQL, minimizing the need for model reasoning, which enhances complex
SQL generation. If necessary, GenEdit regenerates the query based on syntactic
and semantic errors. The knowledge set edits are recommended through an
interactive copilot, allowing users to iterate on their feedback and to
regenerate SQL queries as needed. Each generation uses staged edits which
update the generation prompt. Once the feedback is submitted, it gets merged
after passing regression testing and obtaining an approval, improving future
generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>, Divide, and Conquer: Bypassing Large Language Model Safety
  Filters via Segmented and Distributed <span class="highlight-title">Prompt</span> Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Wahréus, Ahmed Hussain, Panos Papadimitratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have transformed task automation and content
generation across various domains while incorporating safety filters to prevent
misuse. We introduce a novel jailbreaking framework that employs distributed
prompt processing combined with iterative refinements to bypass these safety
measures, particularly in generating malicious code. Our architecture consists
of four key modules: prompt segmentation, parallel processing, response
aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts
across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate
(SR) in generating malicious code. Notably, our comparative analysis reveals
that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared
to our LLM jury system (73.2%), with manual verification confirming that
single-judge assessments often accept incomplete implementations. Moreover, we
demonstrate that our distributed architecture improves SRs by 12% over the
non-distributed approach in an ablation study, highlighting both the
effectiveness of distributed prompt processing and the importance of robust
evaluation methodologies in assessing jailbreak attempts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages; 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical Iterative Denoising: A Discrete Generative Model Applied to
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoann Boget, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete Diffusion and Flow Matching models have significantly advanced
generative modeling for discrete structures, including graphs. However, the
time dependencies in the noising process of these models lead to error
accumulation and propagation during the backward process. This issue,
particularly pronounced in mask diffusion, is a known limitation in sequence
modeling and, as we demonstrate, also impacts discrete diffusion models for
graphs.
  To address this problem, we propose a novel framework called Iterative
Denoising, which simplifies discrete diffusion and circumvents the issue by
assuming conditional independence across time. Additionally, we enhance our
model by incorporating a Critic, which during generation selectively retains or
corrupts elements in an instance based on their likelihood under the data
distribution. Our empirical evaluations demonstrate that the proposed method
significantly outperforms existing discrete diffusion baselines in graph
generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate camera calibration is a fundamental task for 3D perception,
especially when dealing with real-world, in-the-wild environments where complex
optical distortions are common. Existing methods often rely on pre-rectified
images or calibration patterns, which limits their applicability and
flexibility. In this work, we introduce a novel framework that addresses these
challenges by jointly modeling camera intrinsic and extrinsic parameters using
a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus
from semantic to geometric features, enabling more accurate modeling of local
distortions. We propose AlignDiff, a diffusion model conditioned on geometric
priors, enabling the simultaneous estimation of camera distortions and scene
geometry. To enhance distortion prediction, we incorporate edge-aware
attention, focusing the model on geometric features around image edges, rather
than semantic content. Furthermore, to enhance generalizability to real-world
captures, we incorporate a large database of ray-traced lenses containing over
three thousand samples. This database characterizes the distortion inherent in
a diverse variety of lens forms. Our experiments demonstrate that the proposed
method significantly reduces the angular error of estimated ray bundles by ~8.2
degrees and overall calibration accuracy, outperforming existing approaches on
challenging, real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magnitude-Phase Dual-Path Speech Enhancement Network based on
  <span class="highlight-title">Self-Supervised</span> Embedding and Perceptual Contrast Stretch Boosting <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alimjan Mattursun, Liejun Wang, Yinfeng Yu, Chunyang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech self-supervised learning (SSL) has made great progress in various
speech processing tasks, but there is still room for improvement in speech
enhancement (SE). This paper presents BSP-MPNet, a dual-path framework that
combines self-supervised features with magnitude-phase information for SE. The
approach starts by applying the perceptual contrast stretching (PCS) algorithm
to enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC)
encoder then extracts coarse features from the enhanced spectrum. Next, a
feature-separating self-supervised learning (FS-SSL) model generates
self-supervised embeddings for the magnitude and phase components separately.
These embeddings are fused to create cross-domain feature representations.
Finally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine
the features, apply them to the mask, and reconstruct the speech signal. We
evaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental
results show that BSP-MPNet outperforms existing methods under various noise
conditions, providing new directions for self-supervised speech enhancement
research. The implementation of the BSP-MPNet code is available
online\footnote[2]{https://github.com/AlimMat/BSP-MPNet. \label{s1}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper (6 pages). Accepted for publication by ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Local Perspective-based Model for Overlapping Community Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaofeng Zhou, Rui-Feng Wang, Kangning Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection, which identifies densely connected node clusters with
sparse between-group links, is vital for analyzing network structure and
function in real-world systems. Most existing community detection methods based
on GCNs primarily focus on node-level information while overlooking
community-level features, leading to performance limitations on large-scale
networks. To address this issue, we propose LQ-GCN, an overlapping community
detection model from a local community perspective. LQ-GCN employs a
Bernoulli-Poisson model to construct a community affiliation matrix and form an
end-to-end detection framework. By adopting local modularity as the objective
function, the model incorporates local community information to enhance the
quality and accuracy of clustering results. Additionally, the conventional GCNs
architecture is optimized to improve the model capability in identifying
overlapping communities in large-scale networks. Experimental results
demonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual
Information (NMI) and a 26.3% improvement in Recall compared to baseline models
across multiple real-world benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ debug-gym: A Text-Based Environment for Interactive Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, Marc-Alexandre Côté
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly relied upon for coding tasks,
yet in most scenarios it is assumed that all relevant information can be either
accessed in context or matches their training data. We posit that LLMs can
benefit from the ability to interactively explore a codebase to gather the
information relevant to their task. To achieve this, we present a textual
environment, namely debug-gym, for developing LLM-based agents in an
interactive coding setting. Our environment is lightweight and provides a
preset of useful tools, such as a Python debugger (pdb), designed to facilitate
an LLM-based agent's interactive debugging. Beyond coding and debugging tasks,
this approach can be generalized to other tasks that would benefit from
information-seeking behavior by an LLM agent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWI: Speaking with Intent in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Yin, EunJeong Hwang, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent, typically clearly formulated and planned, functions as a cognitive
framework for reasoning and problem-solving. This paper introduces the concept
of Speaking with Intent (SWI) in large language models (LLMs), where the
explicitly generated intent encapsulates the model's underlying intention and
provides high-level planning to guide subsequent analysis and communication. By
emulating deliberate and purposeful thoughts in the human mind, SWI is
hypothesized to enhance the reasoning capabilities and generation quality of
LLMs. Extensive experiments on mathematical reasoning benchmarks consistently
demonstrate the superiority of Speaking with Intent over Baseline (i.e.,
generation without explicit intent). Moreover, SWI outperforms answer-trigger
prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive
performance with the strong method ARR (Analyzing, Retrieving, and Reasoning).
Additionally, the effectiveness and generalizability of SWI are solidified on
reasoning-intensive question answering (QA) and text summarization benchmarks,
where SWI brings consistent improvement to the Baseline generation. In text
summarization, SWI-generated summaries exhibit greater accuracy, conciseness,
and factual correctness, with fewer hallucinations. Furthermore, human
evaluations verify the coherence, effectiveness, and interpretability of the
intent produced by SWI. This proof-of-concept study creates a novel avenue for
enhancing LLMs' reasoning abilities with cognitive notions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages. Code: https://github.com/YuweiYin/SWI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized
  Text-Guided Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achint Soni, Meet Soni, Sirisha Rambhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided image editing aims to modify specific regions of an image
according to natural language instructions while maintaining the general
structure and the background fidelity. Existing methods utilize masks derived
from cross-attention maps generated from diffusion models to identify the
target regions for modification. However, since cross-attention mechanisms
focus on semantic relevance, they struggle to maintain the image integrity. As
a result, these methods often lack spatial consistency, leading to editing
artifacts and distortions. In this work, we address these limitations and
introduce LOCATEdit, which enhances cross-attention maps through a graph-based
approach utilizing self-attention-derived patch relationships to maintain
smooth, coherent attention across image regions, ensuring that alterations are
limited to the designated items while retaining the surrounding structure.
\method consistently and substantially outperforms existing baselines on
PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on
various editing tasks. Code can be found on
https://github.com/LOCATEdit/LOCATEdit/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resource Transliteration for Roman-Urdu and Urdu Using
  <span class="highlight-title">Transformer</span>-Based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umer Butt, Stalin Veranasi, Günter Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Information Retrieval (IR) field increasingly recognizes the
importance of inclusivity, addressing the needs of low-resource languages
remains a significant challenge. Transliteration between Urdu and its Romanized
form, Roman Urdu, remains underexplored despite the widespread use of both
scripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset
showed promising results but suffered from poor domain adaptability and limited
evaluation. We propose a transformer-based approach using the m2m100
multilingual translation model, enhanced with masked language modeling (MLM)
pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse
Dakshina dataset. To address previous evaluation flaws, we introduce rigorous
dataset splits and assess performance using BLEU, character-level BLEU, and
CHRF. Our model achieves strong transliteration performance, with Char-BLEU
scores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These
results outperform both RNN baselines and GPT-4o Mini and demonstrate the
effectiveness of multilingual transfer learning for low-resource
transliteration tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MONO2REST: Identifying and Exposing Microservices: a Reusable
  RESTification Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthéo Lecrivain, Hanifa Barry, Dalila Tamzalit, Houari Sahraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The microservices architectural style has become the de facto standard for
large-scale cloud applications, offering numerous benefits in scalability,
maintainability, and deployment flexibility. Many organizations are pursuing
the migration of legacy monolithic systems to a microservices architecture.
However, this process is challenging, risky, time-intensive, and
prone-to-failure while several organizations lack necessary financial
resources, time, or expertise to set up this migration process. So, rather than
trying to migrate a legacy system where migration is risky or not feasible, we
suggest exposing it as a microservice application without without having to
migrate it. In this paper, we present a reusable, automated, two-phase approach
that combines evolutionary algorithms with machine learning techniques. In the
first phase, we identify microservices at the method level using a
multi-objective genetic algorithm that considers both structural and semantic
dependencies between methods. In the second phase, we generate REST APIs for
each identified microservice using a classification algorithm to assign HTTP
methods and endpoints. We evaluated our approach with a case study on the
Spring PetClinic application, which has both monolithic and microservices
implementations that serve as ground truth for comparison. Results demonstrate
that our approach successfully aligns identified microservices with those in
the reference microservices implementation, highlighting its effectiveness in
service identification and API generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative Evaluation of Quantum/Classical Neural Network Using a Game
  Solver Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suzukaze Kamei, Hideaki Kawaguchi, Shin Nishio, Tatakahiko Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To evaluate the performance of quantum computing systems relative to
classical counterparts and explore the potential for quantum advantage, we
propose a game-solving benchmark based on Elo ratings in the game of
tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum
convolutional neural networks (QCNNs), and hybrid classical-quantum models by
assessing their performance against a random-move agent in automated matches.
Additionally, we implement a QCNN integrated with quantum communication and
evaluate its performance to quantify the overhead introduced by noisy quantum
channels. Our results show that the classical-quantum hybrid model achieves Elo
ratings comparable to those of classical CNNs, while the standalone QCNN
underperforms under current hardware constraints. The communication overhead
was found to be modest. These findings demonstrate the viability of using
game-based benchmarks for evaluating quantum computing systems and suggest that
quantum communication can be incorporated with limited impact on performance,
providing a foundation for future hybrid quantum applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyword-Oriented Multimodal Modeling for Euphemism Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Euphemism identification deciphers the true meaning of euphemisms, such as
linking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts,
aiding content moderation and combating underground markets. While existing
methods are primarily text-based, the rise of social media highlights the need
for multimodal analysis, incorporating text, images, and audio. However, the
lack of multimodal datasets for euphemisms limits further research. To address
this, we regard euphemisms and their corresponding target keywords as keywords
and first introduce a keyword-oriented multimodal corpus of euphemisms
(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including
text, images, and speech. We further propose a keyword-oriented multimodal
euphemism identification method (KOM-EI), which uses cross-modal feature
alignment and dynamic fusion modules to explicitly utilize the visual and audio
features of the keywords for efficient euphemism identification. Extensive
experiments demonstrate that KOM-EI outperforms state-of-the-art models and
large language models, and show the importance of our multimodal datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Resampling with Bootstrap for Noisy Multi-Objective
  Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Budszuhn, Mark Joachim Krallmann, Daniel Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of noisy multi-objective optimization lies in the constant
trade-off between exploring new decision points and improving the precision of
known points through resampling. This decision should take into account both
the variability of the objective functions and the current estimate of a point
in relation to the Pareto front. Since the amount and distribution of noise are
generally unknown, it is desirable for a decision function to be highly
adaptive to the properties of the optimization problem. This paper presents a
resampling decision function that incorporates the stochastic nature of the
optimization problem by using bootstrapping and the probability of dominance.
The distribution-free estimation of the probability of dominance is achieved
using bootstrap estimates of the means. To make the procedure applicable even
with very few observations, we transfer the distribution observed at other
decision points. The efficiency of this resampling approach is demonstrated by
applying it in the NSGA-II algorithm with a sequential resampling procedure
under multiple noise variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Procedural Content Generation Benchmark: An Open-source Testbed for
  Generative Challenges in Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Procedural Content Generation Benchmark for
evaluating generative algorithms on different game content creation tasks. The
benchmark comes with 12 game-related problems with multiple variants on each
problem. Problems vary from creating levels of different kinds to creating rule
sets for simple arcade games. Each problem has its own content representation,
control parameters, and evaluation metrics for quality, diversity, and
controllability. This benchmark is intended as a first step towards a
standardized way of comparing generative algorithms. We use the benchmark to
score three baseline algorithms: a random generator, an evolution strategy, and
a genetic algorithm. Results show that some problems are easier to solve than
others, as well as the impact the chosen objective has on quality, diversity,
and controllability of the generated artifacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 2 tables, published at FDG2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retinal Fundus Multi-Disease Image Classification using Hybrid
  CNN-<span class="highlight-title">Transformer</span>-Ensemble Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deependra Singh, Saksham Agarwal, Subhankar Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research is motivated by the urgent global issue of a large population
affected by retinal diseases, which are evenly distributed but underserved by
specialized medical expertise, particularly in non-urban areas. Our primary
objective is to bridge this healthcare gap by developing a comprehensive
diagnostic system capable of accurately predicting retinal diseases solely from
fundus images. However, we faced significant challenges due to limited, diverse
datasets and imbalanced class distributions. To overcome these issues, we have
devised innovative strategies. Our research introduces novel approaches,
utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),
Transformer encoders, and ensemble architectures sequentially and in parallel
to classify retinal fundus images into 20 disease labels. Our overarching goal
is to assess these advanced models' potential in practical applications, with a
strong focus on enhancing retinal disease diagnosis accuracy across a broader
spectrum of conditions. Importantly, our efforts have surpassed baseline model
results, with the C-Tran ensemble model emerging as the leader, achieving a
remarkable model score of 0.9166, surpassing the baseline score of 0.9.
Additionally, experiments with the IEViT model showcased equally promising
outcomes with improved computational efficiency. We've also demonstrated the
effectiveness of dynamic patch extraction and the integration of domain
knowledge in computer vision tasks. In summary, our research strives to
contribute significantly to retinal disease diagnosis, addressing the critical
need for accessible healthcare solutions in underserved regions while aiming
for comprehensive and accurate disease prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 7 tables. Conference paper presented at the
  International Health Informatics Conference (IHIC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial
  <span class="highlight-title">Prompt</span> Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Marinelli, Josef Pichlmeier, Tamas Bisztray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a metric called Number of Thoughts (NofT) to
determine the difficulty of tasks pre-prompting and support Large Language
Models (LLMs) in production contexts. By setting thresholds based on the number
of thoughts, this metric can discern the difficulty of prompts and support more
effective prompt routing. A 2% decrease in latency is achieved when routing
prompts from the MathInstruct dataset through quantized, distilled versions of
Deepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this
metric can be used to detect adversarial prompts used in prompt injection
attacks with high efficacy. The Number of Thoughts can inform a classifier that
achieves 95% accuracy in adversarial prompt detection. Our experiments ad
datasets used are available on our GitHub page:
https://github.com/rymarinelli/Number_Of_Thoughts/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Latent Information in Transaction Hashes: Hypergraph Learning
  for Ethereum Ponzi Scheme Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Wu, Yixin Yang, Chengxiang Jin, Silu Mu, Xiaolei Qian, Jiajun Zhou, Shanqing Yu, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption of Ethereum, financial frauds such as Ponzi
schemes have become increasingly rampant in the blockchain ecosystem, posing
significant threats to the security of account assets. Existing Ethereum fraud
detection methods typically model account transactions as graphs, but this
approach primarily focuses on binary transactional relationships between
accounts, failing to adequately capture the complex multi-party interaction
patterns inherent in Ethereum. To address this, we propose a hypergraph
modeling method for the Ponzi scheme detection method in Ethereum, called
HyperDet. Specifically, we treat transaction hashes as hyperedges that connect
all the relevant accounts involved in a transaction. Additionally, we design a
two-step hypergraph sampling strategy to significantly reduce computational
complexity. Furthermore, we introduce a dual-channel detection module,
including the hypergraph detection channel and the hyper-homo graph detection
channel, to be compatible with existing detection methods. Experimental results
show that, compared to traditional homogeneous graph-based methods, the
hyper-homo graph detection channel achieves significant performance
improvements, demonstrating the superiority of hypergraph in Ponzi scheme
detection. This research offers innovations for modeling complex relationships
in blockchain data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-to-Vision: Multi-graph Understanding and Reasoning using
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhou Li, Haiyun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured
learning, have long faced dual challenges of exponentially escalating
computational complexity and inadequate cross-scenario generalization
capability. With the rapid advancement of multimodal learning, Vision-Language
Models (VLMs) have demonstrated exceptional cross-modal relational reasoning
capabilities and generalization capacities, thereby opening up novel pathways
for overcoming the inherent limitations of conventional graph learning
paradigms. However, current research predominantly concentrates on
investigating the single-graph reasoning capabilities of VLMs, which
fundamentally fails to address the critical requirement for coordinated
reasoning across multiple heterogeneous graph data in real-world application
scenarios. To address these limitations, we propose the first multi-graph joint
reasoning benchmark for VLMs. Our benchmark encompasses four graph categories:
knowledge graphs, flowcharts, mind maps, and route maps,with each graph group
accompanied by three progressively challenging instruction-response pairs.
Leveraging this benchmark, we conducted comprehensive capability assessments of
state-of-the-art VLMs and performed fine-tuning on open-source models. This
study not only addresses the underexplored evaluation gap in multi-graph
reasoning for VLMs but also empirically validates their generalization
superiority in graph-structured learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuroplasticity in Artificial Intelligence -- An <span class="highlight-title">Overview</span> and
  Inspirations on Drop In \& Out Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupei Li, Manuel Milling, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has achieved new levels of performance and
spread in public usage with the rise of deep neural networks (DNNs). Initially
inspired by human neurons and their connections, NNs have become the foundation
of AI models for many advanced architectures. However, some of the most
integral processes in the human brain, particularly neurogenesis and
neuroplasticity in addition to the more spread neuroapoptosis have largely been
ignored in DNN architecture design. Instead, contemporary AI development
predominantly focuses on constructing advanced frameworks, such as large
language models, which retain a static structure of neural connections during
training and inference. In this light, we explore how neurogenesis,
neuroapoptosis, and neuroplasticity can inspire future AI advances.
Specifically, we examine analogous activities in artificial NNs, introducing
the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and
structural pruning for neuroapoptosis. We additionally suggest neuroplasticity
combining the two for future large NNs in ``life-long learning'' settings
following the biological inspiration. We conclude by advocating for greater
research efforts in this interdisciplinary domain and identifying promising
directions for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning
  and Collaborative Reasoning at the Network Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Ni, Haofeng Sun, Huiqing Ao, Hui Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large artificial intelligence (AI) models exhibit remarkable capabilities in
various application scenarios, but deploying them at the network edge poses
significant challenges due to issues such as data privacy, computational
resources, and latency. In this paper, we explore federated fine-tuning and
collaborative reasoning techniques to facilitate the implementation of large AI
models in resource-constrained wireless networks. Firstly, promising
applications of large AI models within specific domains are discussed.
Subsequently, federated fine-tuning methods are proposed to adapt large AI
models to specific tasks or environments at the network edge, effectively
addressing the challenges associated with communication overhead and enhancing
communication efficiency. These methodologies follow clustered, hierarchical,
and asynchronous paradigms to effectively tackle privacy issues and eliminate
data silos. Furthermore, to enhance operational efficiency and reduce latency,
efficient frameworks for model collaborative reasoning are developed, which
include decentralized horizontal collaboration, cloud-edge-end vertical
collaboration, and multi-access collaboration. Next, simulation results
demonstrate the effectiveness of our proposed methods in reducing the
fine-tuning loss of large AI models across various downstream tasks. Finally,
several open challenges and research opportunities are outlined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Roles of Large Language Models in Reshaping Transportation
  Systems: A <span class="highlight-title">Survey</span>, Framework, and Roadmap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Nie, Jian Sun, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern transportation systems face pressing challenges due to increasing
demand, dynamic environments, and heterogeneous information integration. The
rapid evolution of Large Language Models (LLMs) offers transformative potential
to address these challenges. Extensive knowledge and high-level capabilities
derived from pretraining evolve the default role of LLMs as text generators to
become versatile, knowledge-driven task solvers for intelligent transportation
systems. This survey first presents LLM4TR, a novel conceptual framework that
systematically categorizes the roles of LLMs in transportation into four
synergetic dimensions: information processors, knowledge encoders, component
generators, and decision facilitators. Through a unified taxonomy, we
systematically elucidate how LLMs bridge fragmented data pipelines, enhance
predictive analytics, simulate human-like reasoning, and enable closed-loop
interactions across sensing, learning, modeling, and managing tasks in
transportation systems. For each role, our review spans diverse applications,
from traffic prediction and autonomous driving to safety analytics and urban
mobility optimization, highlighting how emergent capabilities of LLMs such as
in-context learning and step-by-step reasoning can enhance the operation and
management of transportation systems. We further curate practical guidance,
including available resources and computational guidelines, to support
real-world deployment. By identifying challenges in existing LLM-based
solutions, this survey charts a roadmap for advancing LLM-driven transportation
research, positioning LLMs as central actors in the next generation of
cyber-physical-social mobility ecosystems. Online resources can be found in the
project page: https://github.com/tongnie/awesome-llm4tr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for
  Skill Learning <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Keller, Daniel Tanneberg, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning is a popular method for teaching robots new behaviors.
However, most existing methods focus on teaching short, isolated skills rather
than long, multi-step tasks. To bridge this gap, imitation learning algorithms
must not only learn individual skills but also an abstract understanding of how
to sequence these skills to perform extended tasks effectively. This paper
addresses this challenge by proposing a neuro-symbolic imitation learning
framework. Using task demonstrations, the system first learns a symbolic
representation that abstracts the low-level state-action space. The learned
representation decomposes a task into easier subtasks and allows the system to
leverage symbolic planning to generate abstract plans. Subsequently, the system
utilizes this task decomposition to learn a set of neural skills capable of
refining abstract plans into actionable robot commands. Experimental results in
three simulated robotic environments demonstrate that, compared to baselines,
our neuro-symbolic approach increases data efficiency, improves generalization
capabilities, and facilitates interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An evaluation of LLMs and Google Translate for translation of selected
  Indian languages via sentiment and semantic analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohitash Chandra, Aryan Chaudhary, Yeshwanth Rayavarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models (LLMs) have been prominent for language translation,
including low-resource languages. There has been limited study about the
assessment of the quality of translations generated by LLMs, including Gemini,
GPT and Google Translate. In this study, we address this limitation by using
semantic and sentiment analysis of selected LLMs for Indian languages,
including Sanskrit, Telugu and Hindi. We select prominent texts that have been
well translated by experts and use LLMs to generate their translations to
English, and then we provide a comparison with selected expert (human)
translations. Our findings suggest that while LLMs have made significant
progress in translation accuracy, challenges remain in preserving sentiment and
semantic integrity, especially in figurative and philosophical contexts. The
sentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving
the sentiments for the Bhagavad Gita (Sanskrit-English) translations when
compared to Google Translate. We observed a similar trend for the case of Tamas
(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs
similarly to GPT-3.5 in the translation in terms of sentiments for the three
languages. We found that LLMs are generally better at translation for capturing
sentiments when compared to Google Translate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion
  Battery RUL Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoa Tran, Bao Huynh, Tri Le, Lam Pham, Vy-Rin Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery
(LIB) health management systems is crucial for ensuring reliability and safety.
Current methods typically assume that training and testing data share the same
distribution, overlooking the benefits of incorporating diverse data sources to
enhance model performance. To address this limitation, we introduce a
data-independent RUL prediction framework along with its domain adaptation (DA)
approach, which leverages heterogeneous data sources for improved target
predictions. Our approach integrates comprehensive data preprocessing,
including feature extraction, denoising, and normalization, with a
data-independent prediction model that combines Long Short-Term Memory (LSTM),
Multihead Attention, and a Neural Ordinary Differential Equation (NODE) block,
termed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained
using a novel technique inspired by the Domain-Adversarial Neural Network
(DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy
(MMD) to learn domain-invariant features from labeled cycling data in the
source and target domains. Experimental results demonstrate that our approach
outperforms state-of-the-art techniques, providing reliable RUL predictions for
real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Duality of Interpretability and Explainability in
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moncef Garouani, Josiane Mothe, Ayah Barhrhouj, Julien Aligon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of machine learning (ML) has led to the widespread
adoption of complex "black box" models, such as deep neural networks and
ensemble methods. These models exhibit exceptional predictive performance,
making them invaluable for critical decision-making across diverse domains
within society. However, their inherently opaque nature raises concerns about
transparency and interpretability, making them untrustworthy decision support
systems. To alleviate such a barrier to high-stakes adoption, research
community focus has been on developing methods to explain black box models as a
means to address the challenges they pose. Efforts are focused on explaining
these models instead of developing ones that are inherently interpretable.
Designing inherently interpretable models from the outset, however, can pave
the path towards responsible and beneficial applications in the field of ML. In
this position paper, we clarify the chasm between explaining black boxes and
adopting inherently interpretable models. We emphasize the imperative need for
model interpretability and, following the purpose of attaining better (i.e.,
more effective or efficient w.r.t. predictive performance) and trustworthy
predictors, provide an experimental evaluation of latest hybrid learning
methods that integrates symbolic knowledge into neural network predictors. We
demonstrate how interpretable hybrid models could potentially supplant black
box ones in different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using large language models to produce literature <span class="highlight-title">review</span>s: Usages and
  systematic biases of microphysics parametrizations in 2699 publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhang Zhang, Shengnan Fu, David M. Schultz, Zhonghua Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models afford opportunities for using computers for intensive
tasks, realizing research opportunities that have not been considered before.
One such opportunity could be a systematic interrogation of the scientific
literature. Here, we show how a large language model can be used to construct a
literature review of 2699 publications associated with microphysics
parametrizations in the Weather and Research Forecasting (WRF) model, with the
goal of learning how they were used and their systematic biases, when
simulating precipitation. The database was constructed of publications
identified from Web of Science and Scopus searches. The large language model
GPT-4 Turbo was used to extract information about model configurations and
performance from the text of 2699 publications. Our results reveal the
landscape of how nine of the most popular microphysics parameterizations have
been used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus
Ensemble, Morrison, Thompson, and WRF Double-Moment. More studies used
one-moment parameterizations before 2020 and two-moment parameterizations after
2020. Seven out of nine parameterizations tended to overestimate precipitation.
However, systematic biases of parameterizations differed in various regions.
Except simulations using the Lin, Ferrier, and Goddard parameterizations that
tended to underestimate precipitation over almost all locations, the remaining
six parameterizations tended to overestimate, particularly over China,
southeast Asia, western United States, and central Africa. This method could be
used by other researchers to help understand how the increasingly massive body
of scientific literature can be harnessed through the power of artificial
intelligence to solve their research problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Learning Inspired Crossover Operator and Strategy Enhancements
  for Evolutionary Multitasking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Wang, Xiang Feng, Huiqun Yu, Edmund M-K Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evolutionary multitasking, strategies such as crossover operators and
skill factor assignment are critical for effective knowledge transfer. Existing
improvements to crossover operators primarily focus on low-dimensional variable
combinations, such as arithmetic crossover or partially mapped crossover, which
are insufficient for modeling complex high-dimensional interactions.Moreover,
static or semi-dynamic crossover strategies fail to adapt to the dynamic
dependencies among tasks. In addition, current Multifactorial Evolutionary
Algorithm frameworks often rely on fixed skill factor assignment strategies,
lacking flexibility. To address these limitations, this paper proposes the
Multifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based
on residual learning. The method employs a Very Deep Super-Resolution (VDSR)
model to generate high-dimensional residual representations of individuals,
enhancing the modeling of complex relationships within dimensions. A
ResNet-based mechanism dynamically assigns skill factors to improve task
adaptability, while a random mapping mechanism efficiently performs crossover
operations and mitigates the risk of negative transfer. Theoretical analysis
and experimental results show that MFEA-RL outperforms state-of-the-art
multitasking algorithms. It excels in both convergence and adaptability on
standard evolutionary multitasking benchmarks, including CEC2017-MTSO and
WCCI2020-MTSO. Additionally, its effectiveness is validated through a
real-world application scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chyau Yang, Tian-Sheuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a 71.2-$\mu$W speech recognition accelerator designed
for edge devices' real-time applications, emphasizing an ultra low power
design. Achieved through algorithm and hardware co-optimizations, we propose a
compact recurrent spiking neural network with two recurrent layers, one fully
connected layer, and a low time step (1 or 2). The 2.79-MB model undergoes
pruning and 4-bit fixed-point quantization, shrinking it by 96.42\% to 0.1 MB.
On the hardware front, we take advantage of \textit{mixed-level pruning},
\textit{zero-skipping} and \textit{merged spike} techniques, reducing
complexity by 90.49\% to 13.86 MMAC/S. The \textit{parallel time-step
execution} addresses inter-time-step data dependencies and enables weight
buffer power savings through weight sharing. Capitalizing on the sparse spike
activity, an input broadcasting scheme eliminates zero computations, further
saving power. Implemented on the TSMC 28-nm process, the design operates in
real time at 100 kHz, consuming 71.2 $\mu$W, surpassing state-of-the-art
designs. At 500 MHz, it has 28.41 TOPS/W and 1903.11 GOPS/mm$^2$ in energy and
area efficiency, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ci-Hao Wu, Tian-Sheuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based speech enhancement models yield impressive results.
However, their heterogeneous and complex structure restricts model compression
potential, resulting in greater complexity and reduced hardware efficiency.
Additionally, these models are not tailored for streaming and low-power
applications. Addressing these challenges, this paper proposes a low-power
streaming speech enhancement accelerator through model and hardware
optimization. The proposed high performance model is optimized for hardware
execution with the co-design of model compression and target application, which
reduces 93.9\% of model size by the proposed domain-aware and streaming-aware
pruning techniques. The required latency is further reduced with batch
normalization-based transformers. Additionally, we employed softmax-free
attention, complemented by an extra batch normalization, facilitating simpler
hardware design. The tailored hardware accommodates these diverse computing
patterns by breaking them down into element-wise multiplication and
accumulation (MAC). This is achieved through a 1-D processing array, utilizing
configurable SRAM addressing, thereby minimizing hardware complexities and
simplifying zero skipping. Using the TSMC 40nm CMOS process, the final
implementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only
8.08 mW for real-time inference at a 62.5MHz frequency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFeed: Multi-dimensional Summarization Refinement with Reflective
  Reasoning on Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, Hwanjun Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization refinement faces challenges when extending to multi-dimension.
In this paper, we introduce ReFeed, a powerful summarization refinement
pipeline that enhances multiple dimensions through reflective reasoning on
feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based
dataset optimized for training a lightweight model with reflective reasoning.
Our experiments reveal how the number of dimensions, feedback exposure, and
reasoning policy influence refinement performance, highlighting reflective
reasoning and simultaneously addressing multiple feedback is crucial to
mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy
feedback and feedback order. Lastly, our finding emphasizes that creating data
with a proper goal and guideline constitutes a fundamental pillar of effective
reasoning. The dataset and model will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured
  Knowledge Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAG
structures knowledge as graphs to leverage the relations among entities.
However, previous GraphRAG methods are limited by binary relations: one edge in
the graph only connects two entities, which cannot well model the n-ary
relations among more than two entities that widely exist in reality. To address
this limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG method
that represents n-ary relational facts via hyperedges, modeling the complicated
n-ary relations in the real world. To retrieve and generate over hypergraphs,
we introduce a complete pipeline with a hypergraph construction method, a
hypergraph retrieval strategy, and a hypergraph-guided generation mechanism.
Experiments across medicine, agriculture, computer science, and law demonstrate
that HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy and
generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for
  Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) facilitates image retrieval through a
multimodal query consisting of a reference image and modification text. The
reference image defines the retrieval context, while the modification text
specifies desired alterations. However, existing CIR datasets predominantly
employ coarse-grained modification text (CoarseMT), which inadequately captures
fine-grained retrieval intents. This limitation introduces two key challenges:
(1) ignoring detailed differences leads to imprecise positive samples, and (2)
greater ambiguity arises when retrieving visually similar images. These issues
degrade retrieval accuracy, necessitating manual result filtering or repeated
queries. To address these limitations, we develop a robust fine-grained CIR
data annotation pipeline that minimizes imprecise positive samples and enhances
CIR systems' ability to discern modification intents accurately. Using this
pipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained
CIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,
the first CIR framework explicitly designed to parse the modification text.
FineCIR effectively captures fine-grained modification semantics and aligns
them with ambiguous visual entities, enhancing retrieval precision. Extensive
experiments demonstrate that FineCIR consistently outperforms state-of-the-art
CIR baselines on both fine-grained and traditional CIR benchmark datasets. Our
FineCIR code and fine-grained CIR datasets are available at
https://github.com/SDU-L/FineCIR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternVL-X: Advancing and Accelerating InternVL Series with Efficient
  Visual Token Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most multimodal large language models (MLLMs) treat visual tokens as "a
sequence of text", integrating them with text tokens into a large language
model (LLM). However, a great quantity of visual tokens significantly increases
the demand for computational resources and time. In this paper, we propose
InternVL-X, which outperforms the InternVL model in both performance and
efficiency by incorporating three visual token compression methods. First, we
propose a novel vision-language projector, PVTC. This component integrates
adjacent visual embeddings to form a local query and utilizes the transformed
CLS token as a global query, then performs point-to-region cross-attention
through these local and global queries to more effectively convert visual
features. Second, we present a layer-wise visual token compression module,
LVTC, which compresses tokens in the LLM shallow layers and then expands them
through upsampling and residual connections in the deeper layers. This
significantly enhances the model computational efficiency. Futhermore, we
propose an efficient high resolution slicing method, RVTC, which dynamically
adjusts the number of visual tokens based on image area or length filtering.
RVTC greatly enhances training efficiency with only a slight reduction in
performance. By utilizing 20% or fewer visual tokens, InternVL-X achieves
state-of-the-art performance on 7 public MLLM benchmarks, and improves the
average metric by 2.34% across 12 tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep
  Models with Limited Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, Issa Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks are among the most effective, practical, and stealthy
attacks in deep learning. In this paper, we consider a practical scenario where
a developer obtains a deep model from a third party and uses it as part of a
safety-critical system. The developer wants to inspect the model for potential
backdoors prior to system deployment. We find that most existing detection
techniques make assumptions that are not applicable to this scenario. In this
paper, we present a novel framework for detecting backdoors under realistic
restrictions. We generate candidate triggers by deductively searching over the
space of possible triggers. We construct and optimize a smoothed version of
Attack Success Rate as our search objective. Starting from a broad class of
template attacks and just using the forward pass of a deep model, we reverse
engineer the backdoor attack. We conduct extensive evaluation on a wide range
of attacks, models, and datasets, with our technique performing almost
perfectly across these settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate
  Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyue Tu, Siqi Wu, Li Li, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoder-based structures have dominated recent learned image compression
methods. However, the inherent information loss associated with autoencoders
limits their rate-distortion performance at high bit rates and restricts their
flexibility of rate adaptation. In this paper, we present a variable-rate image
compression model based on invertible transform to overcome these limitations.
Specifically, we design a lightweight multi-scale invertible neural network,
which bijectively maps the input image into multi-scale latent representations.
To improve the compression efficiency, a multi-scale spatial-channel context
model with extended gain units is devised to estimate the entropy of the latent
representation from high to low levels. Experimental results demonstrate that
the proposed method achieves state-of-the-art performance compared to existing
variable-rate methods, and remains competitive with recent multi-model
approaches. Notably, our method is the first learned image compression solution
that outperforms VVC across a very wide range of bit rates using a single
model, especially at high bit rates.The source code is available at
\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large language models has garnered widespread attention for
model merging techniques, especially training-free methods which combine model
capabilities within the parameter space. However, two challenges remain: (1)
uniform treatment of all parameters leads to performance degradation; (2)
search-based algorithms are often inefficient. In this paper, we present an
innovative framework termed Reinforced Model Merging (RMM), which encompasses
an environment and agent tailored for merging tasks. These components interact
to execute layer-wise merging actions, aiming to search the optimal merging
architecture. Notably, RMM operates without any gradient computations on the
original models, rendering it feasible for edge devices. Furthermore, by
utilizing data subsets during the evaluation process, we addressed the
bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100
times. Extensive experiments demonstrate that RMM achieves state-of-the-art
performance across various vision and NLP datasets and effectively overcomes
the limitations of the existing baseline methods. Our code is available at
https://github.com/WuDiHJQ/Reinforced-Model-Merging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn by Reasoning: Analogical Weight Generation for Few-Shot
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot class-incremental Learning (FSCIL) enables models to learn new
classes from limited data while retaining performance on previously learned
classes. Traditional FSCIL methods often require fine-tuning parameters with
limited new class data and suffer from a separation between learning new
classes and utilizing old knowledge. Inspired by the analogical learning
mechanisms of the human brain, we propose a novel analogical generative method.
Our approach includes the Brain-Inspired Analogical Generator (BiAG), which
derives new class weights from existing classes without parameter fine-tuning
during incremental stages. BiAG consists of three components: Weight
Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module
(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory
for semantic conversion, WSA supplements new class weights, and WPAA computes
analogies to generate new class weights. Experiments on miniImageNet, CUB-200,
and CIFAR-100 datasets demonstrate that our method achieves higher final and
average accuracy compared to SOTA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OminiAdapt: Learning Cross-Task Invariance for Robust and
  Environment-Aware Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Wang, Weiyun Yi, Xinhao Kong, Wanting Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of embodied intelligence, leveraging large-scale
human data for high-level imitation learning on humanoid robots has become a
focal point of interest in both academia and industry. However, applying
humanoid robots to precision operation domains remains challenging due to the
complexities they face in perception and control processes, the long-standing
physical differences in morphology and actuation mechanisms between humanoid
robots and humans, and the lack of task-relevant features obtained from
egocentric vision. To address the issue of covariate shift in imitation
learning, this paper proposes an imitation learning algorithm tailored for
humanoid robots. By focusing on the primary task objectives, filtering out
background information, and incorporating channel feature fusion with spatial
attention mechanisms, the proposed algorithm suppresses environmental
disturbances and utilizes a dynamic weight update strategy to significantly
improve the success rate of humanoid robots in accomplishing target tasks.
Experimental results demonstrate that the proposed method exhibits robustness
and scalability across various typical task scenarios, providing new ideas and
approaches for autonomous learning and control in humanoid robots. The project
will be open-sourced on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-to-Music Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokai Wang, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, Victor Shea-Jay Huang, Yue Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-to-music Generation, including video-to-music and image-to-music
tasks, is a significant branch of multimodal artificial intelligence
demonstrating vast application prospects in fields such as film scoring, short
video creation, and dance music synthesis. However, compared to the rapid
development of modalities like text and images, research in vision-to-music is
still in its preliminary stage due to its complex internal structure and the
difficulty of modeling dynamic relationships with video. Existing surveys focus
on general music generation without comprehensive discussion on
vision-to-music. In this paper, we systematically review the research progress
in the field of vision-to-music generation. We first analyze the technical
characteristics and core challenges for three input types: general videos,
human movement videos, and images, as well as two output types of symbolic
music and audio music. We then summarize the existing methodologies on
vision-to-music generation from the architecture perspective. A detailed review
of common datasets and evaluation metrics is provided. Finally, we discuss
current challenges and promising directions for future research. We hope our
survey can inspire further innovation in vision-to-music generation and the
broader field of multimodal generation in academic research and industrial
applications. To follow latest works and foster further innovation in this
field, we are continuously maintaining a GitHub repository at
https://github.com/wzk1015/Awesome-Vision-to-Music-Generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Splitting Conformal Prediction for Multi-Step Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingdi Yu, Zhiwei Cao, Ruihang Wang, Zhen Yang, Lijun Deng, Min Hu, Yong Luo, Xin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting is crucial for applications like resource scheduling
and risk management, where multi-step predictions provide a comprehensive view
of future trends. Uncertainty Quantification (UQ) is a mainstream approach for
addressing forecasting uncertainties, with Conformal Prediction (CP) gaining
attention due to its model-agnostic nature and statistical guarantees. However,
most variants of CP are designed for single-step predictions and face
challenges in multi-step scenarios, such as reliance on real-time data and
limited scalability. This highlights the need for CP methods specifically
tailored to multi-step forecasting. We propose the Dual-Splitting Conformal
Prediction (DSCP) method, a novel CP approach designed to capture inherent
dependencies within time-series data for multi-step forecasting. Experimental
results on real-world datasets from four different domains demonstrate that the
proposed DSCP significantly outperforms existing CP variants in terms of the
Winkler Score, achieving a performance improvement of up to 23.59% compared to
state-of-the-art methods. Furthermore, we deployed the DSCP approach for
renewable energy generation and IT load forecasting in power management of a
real-world trajectory-based application, achieving an 11.25% reduction in
carbon emissions through predictive optimization of data center operations and
controls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, 3 tables. Submitted to Applied Soft Computing.
  With Editor This is the first public release of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResearchBench: Benchmarking LLMs in Scientific Discovery via
  Inspiration-Based Task Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated potential in assisting
scientific research, yet their ability to discover high-quality research
hypotheses remains unexamined due to the lack of a dedicated benchmark. To
address this gap, we introduce the first large-scale benchmark for evaluating
LLMs with a near-sufficient set of sub-tasks of scientific discovery:
inspiration retrieval, hypothesis composition, and hypothesis ranking. We
develop an automated framework that extracts critical components - research
questions, background surveys, inspirations, and hypotheses - from scientific
papers across 12 disciplines, with expert validation confirming its accuracy.
To prevent data contamination, we focus exclusively on papers published in
2024, ensuring minimal overlap with LLM pretraining data. Our evaluation
reveals that LLMs perform well in retrieving inspirations, an
out-of-distribution task, suggesting their ability to surface novel knowledge
associations. This positions LLMs as "research hypothesis mines", capable of
facilitating automated scientific discovery by generating innovative hypotheses
at scale with minimal human intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving $(α, f)$-Byzantine Resilience in Federated Learning via
  layerwise aggregation and cosine distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario García-Márquez, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of artificial intelligence systems has amplified
societal concerns regarding their usage, necessitating regulatory frameworks
that encompass data privacy. Federated Learning (FL) is posed as potential
solution to data privacy challenges in distributed machine learning by enabling
collaborative model training {without data sharing}. However, FL systems remain
vulnerable to Byzantine attacks, where malicious nodes contribute corrupted
model updates. While Byzantine Resilient operators have emerged as a widely
adopted robust aggregation algorithm to mitigate these attacks, its efficacy
diminishes significantly in high-dimensional parameter spaces, sometimes
leading to poor performing models. This paper introduces Layerwise Cosine
Aggregation, a novel aggregation scheme designed to enhance robustness of these
rules in such high-dimensional settings while preserving computational
efficiency. A theoretical analysis is presented, demonstrating the superior
robustness of the proposed Layerwise Cosine Aggregation compared to original
robust aggregation operators. Empirical evaluation across diverse image
classification datasets, under varying data distributions and Byzantine attack
scenarios, consistently demonstrates the improved performance of Layerwise
Cosine Aggregation, achieving up to a 16% increase in model accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Knowledge-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in
  Healthcare Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyeYoung Lee, Pavel Tsoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate patient mortality prediction enables effective risk stratification,
leading to personalized treatment plans and improved patient outcomes. However,
predicting mortality in healthcare remains a significant challenge, with
existing studies often focusing on specific diseases or limited predictor sets.
This study evaluates machine learning models for all-cause in-hospital
mortality prediction using the MIMIC-III database, employing a comprehensive
feature engineering approach. Guided by clinical expertise and literature, we
extracted key features such as vital signs (e.g., heart rate, blood pressure),
laboratory results (e.g., creatinine, glucose), and demographic information.
The Random Forest model achieved the highest performance with an AUC of 0.94,
significantly outperforming other machine learning and deep learning
approaches. This demonstrates Random Forest's robustness in handling
high-dimensional, noisy clinical data and its potential for developing
effective clinical decision support tools. Our findings highlight the
importance of careful feature engineering for accurate mortality prediction. We
conclude by discussing implications for clinical adoption and propose future
directions, including enhancing model robustness and tailoring prediction
models for specific diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karanbir Singh, William Ngu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in retrieving accessible information have evolved faster in the
last few years compared to the decades since the internet's creation. Search
engines, like Google, have been the number one way to find relevant data. They
have always relied on the user's abilities to find the best information in its
billions of links and sources at everybody's fingertips. The advent of large
language models (LLMs) has completely transformed the field of information
retrieval. The LLMs excel not only at retrieving relevant knowledge but also at
summarizing it effectively, making information more accessible and consumable
for users. On top of it, the rise of AI Agents has introduced another aspect to
information retrieval i.e. dynamic information retrieval which enables the
integration of real-time data such as weather forecasts, and financial data
with the knowledge base to curate context-aware knowledge. However, despite
these advancements the agents remain susceptible to issues of bias and
fairness, challenges deeply rooted within the knowledge base and training of
LLMs. This study introduces a novel approach to bias-aware knowledge retrieval
by leveraging agentic framework and the innovative use of bias detectors as
tools to identify and highlight inherent biases in the retrieved content. By
empowering users with transparency and awareness, this approach aims to foster
more equitable information systems and promote the development of responsible
AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graphs as World Models for Semantic Material-Aware Obstacle
  Handling in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Bheemaiah, Seungyong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inability of autonomous vehicles (AVs) to infer the material properties
of obstacles limits their decision-making capacity. While AVs rely on sensor
systems such as cameras, LiDAR, and radar to detect obstacles, this study
suggests combining sensors with a knowledge graph (KG)-based world model to
improve AVs' comprehension of physical material qualities. Beyond sensor data,
AVs can infer qualities such as malleability, density, and elasticity using a
semantic KG that depicts the relationships between obstacles and their
attributes. Using the CARLA autonomous driving simulator, we evaluated AV
performance with and without KG integration. The findings demonstrate that the
KG-based method improves obstacle management, which allows AVs to use material
qualities to make better decisions about when to change lanes or apply
emergency braking. For example, the KG-integrated AV changed lanes for hard
impediments like traffic cones and successfully avoided collisions with
flexible items such as plastic bags by passing over them. Compared to the
control system, the KG framework demonstrated improved responsiveness to
obstacles by resolving conflicting sensor data, causing emergency stops for
13.3% more cases. In addition, our method exhibits a 6.6% higher success rate
in lane-changing maneuvers in experimental scenarios, particularly for larger,
high-impact obstacles. While we focus particularly on autonomous driving, our
work demonstrates the potential of KG-based world models to improve
decision-making in embodied AI systems and scale to other domains, including
robotics, healthcare, and environmental simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenFusion: Closing the Loop between Reconstruction and Generation via
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D reconstruction and generation have demonstrated impressive novel
view synthesis results, achieving high fidelity and efficiency. However, a
notable conditioning gap can be observed between these two fields, e.g.,
scalable 3D scene reconstruction often requires densely captured views, whereas
3D generation typically relies on a single or no input view, which
significantly limits their applications. We found that the source of this
phenomenon lies in the misalignment between 3D constraints and generative
priors. To address this problem, we propose a reconstruction-driven video
diffusion model that learns to condition video frames on artifact-prone RGB-D
renderings. Moreover, we propose a cyclical fusion pipeline that iteratively
adds restoration frames from the generative model to the training set, enabling
progressive expansion and addressing the viewpoint saturation limitations seen
in previous reconstruction and generation pipelines. Our evaluation, including
view synthesis from sparse view and masked input, validates the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Large Language Models For Monte Carlo Simulation of Chemical
  Reaction Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, Vaghawan Ojha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical reaction network is an important method for modeling and exploring
complex biological processes, bio-chemical interactions and the behavior of
different dynamics in system biology. But, formulating such reaction kinetics
takes considerable time. In this paper, we leverage the efficiency of modern
large language models to automate the stochastic monte carlo simulation of
chemical reaction networks and enable the simulation through the reaction
description provided in the form of natural languages. We also integrate this
process into widely used simulation tool Copasi to further give the edge and
ease to the modelers and researchers. In this work, we show the efficacy and
limitations of the modern large language models to parse and create reaction
kinetics for modelling complex chemical reaction processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on MadeAI 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Wear and Tear: Exploiting Natural Damage for Generating
  Physical-World Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of adversarial examples in the physical world poses significant
challenges to the deployment of Deep Neural Networks in safety-critical
applications such as autonomous driving. Most existing methods for crafting
physical-world adversarial examples are ad-hoc, relying on temporary
modifications like shadows, laser beams, or stickers that are tailored to
specific scenarios. In this paper, we introduce a new class of physical-world
adversarial examples, AdvWT, which draws inspiration from the naturally
occurring phenomenon of `wear and tear', an inherent property of physical
objects. Unlike manually crafted perturbations, `wear and tear' emerges
organically over time due to environmental degradation, as seen in the gradual
deterioration of outdoor signboards. To achieve this, AdvWT follows a two-step
approach. First, a GAN-based, unsupervised image-to-image translation network
is employed to model these naturally occurring damages, particularly in the
context of outdoor signboards. The translation network encodes the
characteristics of damaged signs into a latent `damage style code'. In the
second step, we introduce adversarial perturbations into the style code,
strategically optimizing its transformation process. This manipulation subtly
alters the damage style representation, guiding the network to generate
adversarial images where the appearance of damages remains perceptually
realistic, while simultaneously ensuring their effectiveness in misleading
neural networks. Through comprehensive experiments on two traffic sign
datasets, we show that AdvWT effectively misleads DNNs in both digital and
physical domains. AdvWT achieves an effective attack success rate, greater
robustness, and a more natural appearance compared to existing physical-world
adversarial examples. Additionally, integrating AdvWT into training enhances a
model's generalizability to real-world damaged signs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Optimization for Privacy-Utility Balance in
  Differentially Private Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishka Ranaweera, David Smith, Pubudu N. Pathirana, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it a promising approach
for privacy-preserving machine learning. However, ensuring differential privacy
(DP) in FL presents challenges due to the trade-off between model utility and
privacy protection. Clipping gradients before aggregation is a common strategy
to limit privacy loss, but selecting an optimal clipping norm is non-trivial,
as excessively high values compromise privacy, while overly restrictive
clipping degrades model performance. In this work, we propose an adaptive
clipping mechanism that dynamically adjusts the clipping norm using a
multi-objective optimization framework. By integrating privacy and utility
considerations into the optimization objective, our approach balances privacy
preservation with model accuracy. We theoretically analyze the convergence
properties of our method and demonstrate its effectiveness through extensive
experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show
that adaptive clipping consistently outperforms fixed-clipping baselines,
achieving improved accuracy under the same privacy constraints. This work
highlights the potential of dynamic clipping strategies to enhance
privacy-utility trade-offs in differentially private federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning with Differential Privacy: An Utility-Enhanced
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishka Ranaweera, Dinh C. Nguyen, Pubudu N. Pathirana, David Smith, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has emerged as an attractive approach to protect data
privacy by eliminating the need for sharing clients' data while reducing
communication costs compared with centralized machine learning algorithms.
However, recent studies have shown that federated learning alone does not
guarantee privacy, as private data may still be inferred from the uploaded
parameters to the central server. In order to successfully avoid data leakage,
adopting differential privacy (DP) in the local optimization process or in the
local update aggregation process has emerged as two feasible ways for achieving
sample-level or user-level privacy guarantees respectively, in federated
learning models. However, compared to their non-private equivalents, these
approaches suffer from a poor utility. To improve the privacy-utility
trade-off, we present a modification to these vanilla differentially private
algorithms based on a Haar wavelet transformation step and a novel noise
injection scheme that significantly lowers the asymptotic bound of the noise
variance. We also present a holistic convergence analysis of our proposed
algorithm, showing that our method yields better convergence performance than
the vanilla DP algorithms. Numerical experiments on real-world datasets
demonstrate that our method outperforms existing approaches in model utility
while maintaining the same privacy guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in Low-Level Features for Cross-Domain Few-Shot
  Segmentation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Yixiong Zou, Yuhua Li, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the
pixel-level segmentation capabilities learned from large-scale source-domain
datasets to downstream target-domain datasets, with only a few annotated images
per class. In this paper, we focus on a well-observed but unresolved phenomenon
in CDFSS: for target domains, particularly those distant from the source
domain, segmentation performance peaks at the very early epochs, and declines
sharply as the source-domain training proceeds. We delve into this phenomenon
for an interpretation: low-level features are vulnerable to domain shifts,
leading to sharper loss landscapes during the source-domain training, which is
the devil of CDFSS. Based on this phenomenon and interpretation, we further
propose a method that includes two plug-and-play modules: one to flatten the
loss landscapes for low-level features during source-domain training as a novel
sharpness-aware minimization method, and the other to directly supplement
target-domain information to the model during target-domain testing by
low-level-based calibration. Extensive experiments on four target datasets
validate our rationale and demonstrate that our method surpasses the
state-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU
in 1-shot and 5-shot scenarios, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A computational theory of evaluation for parameterisable subject 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hedong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation is critical to advance decision making across domains, yet
existing methodologies often struggle to balance theoretical rigor and
practical scalability. In order to reduce the cost of experimental evaluation,
we introduce a computational theory of evaluation for parameterisable subjects.
We prove upper bounds of generalized evaluation error and generalized causal
effect error of evaluation metric on subject. We also prove efficiency, and
consistency to estimated causal effect of subject on metric by prediction. To
optimize evaluation models, we propose a meta-learner to handle heterogeneous
evaluation subjects space. Comparing with other computational approaches, our
(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12
scenes, including individual medicine, scientific simulation, business
activities, and quantum trade. The evaluation time is reduced 3-7 order of
magnitude comparing with experiments or simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous
  Processor Co-Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunquan Gao, Zhiguo Zhang, Praveen Kumar Donta, Chinmaya Kumar Dehury, Xiujun Wang, Dusit Niyato, Qiyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are increasingly deployed across diverse
industries, driving demand for mobile device support. However, existing mobile
inference frameworks often rely on a single processor per model, limiting
hardware utilization and causing suboptimal performance and energy efficiency.
Expanding DNN accessibility on mobile platforms requires adaptive,
resource-efficient solutions to meet rising computational needs without
compromising functionality. Parallel inference of multiple DNNs on
heterogeneous processors remains challenging. Some works partition DNN
operations into subgraphs for parallel execution across processors, but these
often create excessive subgraphs based only on hardware compatibility,
increasing scheduling complexity and memory overhead.
  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)
strategy for optimizing multi-DNN inference on mobile heterogeneous processors.
ADMS constructs an optimal subgraph partitioning strategy offline, balancing
hardware operation support and scheduling granularity, and uses a
processor-state-aware algorithm to dynamically adjust workloads based on
real-time conditions. This ensures efficient workload distribution and
maximizes processor utilization. Experiments show ADMS reduces multi-DNN
inference latency by 4.04 times compared to vanilla frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alleviating LLM-based Generative Retrieval Hallucination in Alipay
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedan Shen, Kaixin Wu, Yuechen Ding, Jingyuan Wen, Hong Liu, Mingjie Zhong, Zhouhan Lin, Jia Xu, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval (GR) has revolutionized document retrieval with the
advent of large language models (LLMs), and LLM-based GR is gradually being
adopted by the industry. Despite its remarkable advantages and potential,
LLM-based GR suffers from hallucination and generates documents that are
irrelevant to the query in some instances, severely challenging its credibility
in practical applications. We thereby propose an optimized GR framework
designed to alleviate retrieval hallucination, which integrates knowledge
distillation reasoning in model training and incorporate decision agent to
further improve retrieval precision. Specifically, we employ LLMs to assess and
reason GR retrieved query-document (q-d) pairs, and then distill the reasoning
data as transferred knowledge to the GR model. Moreover, we utilize a decision
agent as post-processing to extend the GR retrieved documents through retrieval
model and select the most relevant ones from multi perspectives as the final
generative retrieval result. Extensive offline experiments on real-world
datasets and online A/B tests on Fund Search and Insurance Search in Alipay
demonstrate our framework's superiority and effectiveness in improving search
quality and conversion gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Adjusted Surprise Measure for Active Resourceful Trials
  (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material
  Discovery under Resource Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Shoyeb Raihan, Zhichao Liu, Tanveer Hossain Bhuiyan, Imtiaz Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the discovery and manufacturing of advanced materials with
specific properties is a critical yet formidable challenge due to vast search
space, high costs of experiments, and time-intensive nature of material
characterization. In recent years, active learning, where a surrogate machine
learning (ML) model mimics the scientific discovery process of a human
scientist, has emerged as a promising approach to address these challenges by
guiding experimentation toward high-value outcomes with a limited budget. Among
the diverse active learning philosophies, the concept of surprise (capturing
the divergence between expected and observed outcomes) has demonstrated
significant potential to drive experimental trials and refine predictive
models. Scientific discovery often stems from surprise thereby making it a
natural driver to guide the search process. Despite its promise, prior studies
leveraging surprise metrics such as Shannon and Bayesian surprise lack
mechanisms to account for prior confidence, leading to excessive exploration of
uncertain regions that may not yield useful information. To address this, we
propose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials
(CA-SMART), a novel Bayesian active learning framework tailored for optimizing
data-driven experimentation. On a high level, CA-SMART incorporates
Confidence-Adjusted Surprise (CAS) to dynamically balance exploration and
exploitation by amplifying surprises in regions where the model is more certain
while discounting them in highly uncertain areas. We evaluated CA-SMART on two
benchmark functions (Six-Hump Camelback and Griewank) and in predicting the
fatigue strength of steel. The results demonstrate superior accuracy and
efficiency compared to traditional surprise metrics, standard Bayesian
Optimization (BO) acquisition functions and conventional ML methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoming Xu, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, Shumin Deng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:
Unlearning Sensitive Content from Large Language Models. This task aims to
selectively erase sensitive knowledge from large language models, avoiding both
over-forgetting and under-forgetting issues. We propose an unlearning system
that leverages Model Merging (specifically TIES-Merging), combining two
specialized models into a more balanced unlearned model. Our system achieves
competitive results, ranking second among 26 teams, with an online score of
0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we
also conduct local experiments and perform a comprehensive analysis of the
unlearning process, examining performance trajectories, loss dynamics, and
weight perspectives, along with several supplementary experiments, to
understand the effectiveness of our method. Furthermore, we analyze the
shortcomings of our method and evaluation metrics, emphasizing that MIA scores
and ROUGE-based metrics alone are insufficient to fully evaluate successful
unlearning. Finally, we emphasize the need for more comprehensive evaluation
methodologies and rethinking of unlearning objectives in future research. Code
is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual
  Similarity Between Indus and Tibetan-Yi Corridor Writing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ooha Lakkadi Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis employs a hybrid CNN-Transformer architecture, in conjunction
with a detailed anthropological framework, to investigate potential historical
connections between the visual morphology of the Indus Valley script and
pictographic systems of the Tibetan-Yi Corridor. Through an ensemble
methodology of three target scripts across 15 independently trained models, we
demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold
higher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze
Age Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.
Additionally and contrarily to our current understanding of the networks of the
Indus Valley Civilization, the Indus script unexpectedly maps closer to
Tibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to
the aforementioned contemporaneous West Asian signaries, both of which recorded
mean cosine similarities of 0.104 and 0.080 despite their close geographic
proximity and evident trade relations. Across various dimensionality reduction
practices and clustering methodologies, the Indus script consistently clusters
closest to Tibetan-Yi Corridor scripts. Our computational results align with
qualitative observations of specific pictorial parallels in numeral systems,
gender markers, and key iconographic elements; this is further supported by
archaeological evidence of sustained contact networks along the ancient
Shu-Shendu road in tandem with the Indus Valley Civilization's decline,
providing a plausible transmission pathway. While alternative explanations
cannot be ruled out, the specificity and consistency of observed similarities
challenge conventional narratives of isolated script development and suggest
more complex ancient cultural transmission networks between South and East Asia
than previously recognized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21
  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this
  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at
  Duke Kunshan University; accepted for presentation at the 2025 Computer
  Applications and Quantitative Methods in Archaeology Conference, Athens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AskSport: Web Application for Sports Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo B Onofre, Leonardo M P Moraes, Cristina D Aguiar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AskSport, a question-answering web application about
sports. It allows users to ask questions using natural language and retrieve
the three most relevant answers, including related information and documents.
The paper describes the characteristics and functionalities of the application,
including use cases demonstrating its ability to return names and numerical
values. AskSport and its implementation are available for public access on
HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for accessing the application, see
  https://huggingface.co/spaces/leomaurodesenv/qasports-website</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Visual In-Context Tuning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual in-context learning (VICL), as a new paradigm in computer vision,
allows the model to rapidly adapt to various tasks with only a handful of
prompts and examples. While effective, the existing VICL paradigm exhibits poor
generalizability under distribution shifts. In this work, we propose test-time
Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly
with a single test sample. Specifically, we flip the role between the task
prompts and the test sample and use a cycle consistency loss to reconstruct the
original task prompt output. Our key insight is that a model should be aware of
a new test distribution if it can successfully recover the original task
prompts. Extensive experiments on six representative vision tasks ranging from
high-level visual understanding to low-level image processing, with 15 common
corruptions, demonstrate that our VICT can improve the generalizability of VICL
to unseen new domains. In addition, we show the potential of applying VICT for
unseen tasks at test time. Code: https://github.com/Jiahao000/VICT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Code: https://github.com/Jiahao000/VICT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Diffusion Bridge Problems: Flow Matching and
  Schrödinger Matching into One 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The bridge problem is to find an SDE (or sometimes an ODE) that bridges two
given distributions. The application areas of the bridge problem are enormous,
among which the recent generative modeling (e.g., conditional or unconditional
image generation) is the most popular. Also the famous Schr\"{o}dinger bridge
problem, a widely known problem for a century, is a special instance of the
bridge problem. Two most popular algorithms to tackle the bridge problems in
the deep learning era are: (conditional) flow matching and iterative fitting
algorithms, where the former confined to ODE solutions, and the latter
specifically for the Schr\"{o}dinger bridge problem. The main contribution of
this article is in two folds: i) We provide concise reviews of these algorithms
with technical details to some extent; ii) We propose a novel unified
perspective and framework that subsumes these seemingly unrelated algorithms
(and their variants) into one. In particular, we show that our unified
framework can instantiate the Flow Matching (FM) algorithm, the (mini-batch)
optimal transport FM algorithm, the (mini-batch) Schr\"{o}dinger bridge FM
algorithm, and the deep Schr\"{o}dinger bridge matching (DSBM) algorithm as its
special cases. We believe that this unified framework will be useful for
viewing the bridge problems in a more general and flexible perspective, and in
turn can help researchers and practitioners to develop new bridge algorithms in
their fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Skill Unlearning through Intervention and Abstention <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongce Li, Chung-En Sun, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language Models (LLMs) have demonstrated remarkable skills across
various domains. Understanding the mechanisms behind their abilities and
implementing controls over them is becoming increasingly important for
developing better models. In this paper, we focus on skill unlearning in LLMs,
specifically unlearning a particular skill while retaining their overall
capabilities. We introduce two lightweight, training-free machine skill
unlearning techniques for LLMs. First, we observe that the pre-activation
distribution of neurons in each Feed-Forward Layer (FFL) differs when the model
demonstrates different skills. Additionally, we find that queries triggering
the same skill cluster within the FFL key space and can be separated from other
queries using a hypercube. Based on these observations, we propose two
lightweight, training-free skill unlearning methods via \textit{intervention}
and \textit{abstention} respectively: \texttt{Neuron Adjust} and \texttt{Key
Space Detection}. We evaluate our methods on unlearning math-solving,
Python-coding, and comprehension skills across seven different languages. The
results demonstrate their strong unlearning capabilities for the designated
skills. Specifically, \texttt{Key Space Detection} achieves over 80\% relative
performance drop on the forgetting skill and less than 10\% relative
performance drop on other skills and the model's general knowledge (MMLU) for
most unlearning tasks. Our code is available at
https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Minimization for Participatory Federated Learning in IoT Analyzed
  via Game Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Buratto, Elia Guerra, Marco Miozzo, Paolo Dini, Leonardo Badia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Things requires intelligent decision making in many
scenarios. To this end, resources available at the individual nodes for sensing
or computing, or both, can be leveraged. This results in approaches known as
participatory sensing and federated learning, respectively. We investigate the
simultaneous implementation of both, through a distributed approach based on
empowering local nodes with game theoretic decision making. A global objective
of energy minimization is combined with the individual node's optimization of
local expenditure for sensing and transmitting data over multiple learning
rounds. We present extensive evaluations of this technique, based on both a
theoretical framework and experiments in a simulated network scenario with real
data. Such a distributed approach can reach a desired level of accuracy for
federated learning without a centralized supervision of the data collector.
However, depending on the weight attributed to the local costs of the single
node, it may also result in a significantly high Price of Anarchy (from 1.28
onwards). Thus, we argue for the need of incentive mechanisms, possibly based
on Age of Information of the single nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Represent Individual Differences for Choice Decision Making <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Ying Chen, Yue Weng, Alexandre Filipowicz, Rumen Iliev, Francine Chen, Shabnam Hakimi, Yanxia Zhang, Matthew Lee, Kent Lyons, Charlene Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human decision making can be challenging to predict because decisions are
affected by a number of complex factors. Adding to this complexity,
decision-making processes can differ considerably between individuals, and
methods aimed at predicting human decisions need to take individual differences
into account. Behavioral science offers methods by which to measure individual
differences (e.g., questionnaires, behavioral models), but these are often
narrowed down to low dimensions and not tailored to specific prediction tasks.
This paper investigates the use of representation learning to measure
individual differences from behavioral experiment data. Representation learning
offers a flexible approach to create individual embeddings from data that are
both structured (e.g., demographic information) and unstructured (e.g., free
text), where the flexibility provides more options for individual difference
measures for personalization, e.g., free text responses may allow for
open-ended questions that are less privacy-sensitive. In the current paper we
use representation learning to characterize individual differences in human
performance on an economic decision-making task. We demonstrate that models
using representation learning to capture individual differences consistently
improve decision predictions over models without representation learning, and
even outperform well-known theory-based behavioral models used in these
environments. Our results propose that representation learning offers a useful
and flexible tool to capture individual differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IJCAI MRC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Quantum <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuichi Kamata, Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer model, renowned for its powerful attention mechanism, has
achieved state-of-the-art performance in various artificial intelligence tasks
but faces challenges such as high computational cost and memory usage.
Researchers are exploring quantum computing to enhance the Transformer's
design, though it still shows limited success with classical data. With a
growing focus on leveraging quantum machine learning for quantum data,
particularly in quantum chemistry, we propose the Molecular Quantum Transformer
(MQT) for modeling interactions in molecular quantum systems. By utilizing
quantum circuits to implement the attention mechanism on the molecular
configurations, MQT can efficiently calculate ground-state energies for all
configurations. Numerical demonstrations show that in calculating ground-state
energies for H_2, LiH, BeH_2, and H_4, MQT outperforms the classical
Transformer, highlighting the promise of quantum effects in Transformer
structures. Furthermore, its pretraining capability on diverse molecular data
facilitates the efficient learning of new molecules, extending its
applicability to complex molecular systems with minimal additional effort. Our
method offers an alternative to existing quantum algorithms for estimating
ground-state energies, opening new avenues in quantum chemistry and materials
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Benchmark for RNA 3D Structure-Function Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Wyss, Vincent Mallet, Wissam Karroucha, Karsten Borgwardt, Carlos Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The RNA structure-function relationship has recently garnered significant
attention within the deep learning community, promising to grow in importance
as nucleic acid structure models advance. However, the absence of standardized
and accessible benchmarks for deep learning on RNA 3D structures has impeded
the development of models for RNA functional characteristics.
  In this work, we introduce a set of seven benchmarking datasets for RNA
structure-function prediction, designed to address this gap. Our library builds
on the established Python library rnaglib, and offers easy data distribution
and encoding, splitters and evaluation methods, providing a convenient
all-in-one framework for comparing models. Datasets are implemented in a fully
modular and reproducible manner, facilitating for community contributions and
customization. Finally, we provide initial baseline results for all tasks using
a graph neural network.
  Source code: https://github.com/cgoliver/rnaglib
  Documentation: https://rnaglib.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A tale of two goals: leveraging sequentiality in multi-goal scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Serris, Stéphane Doncieux, Olivier Sigaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several hierarchical reinforcement learning methods leverage planning to
create a graph or sequences of intermediate goals, guiding a lower-level
goal-conditioned (GC) policy to reach some final goals. The low-level policy is
typically conditioned on the current goal, with the aim of reaching it as
quickly as possible. However, this approach can fail when an intermediate goal
can be reached in multiple ways, some of which may make it impossible to
continue toward subsequent goals. To address this issue, we introduce two
instances of Markov Decision Process (MDP) where the optimization objective
favors policies that not only reach the current goal but also subsequent ones.
In the first, the agent is conditioned on both the current and final goals,
while in the second, it is conditioned on the next two goals in the sequence.
We conduct a series of experiments on navigation and pole-balancing tasks in
which sequences of intermediate goals are given. By evaluating policies trained
with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,
in most cases, conditioning on the next two goals improves stability and sample
efficiency over other approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How do language models learn facts? Dynamics, curricula and
  hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Zucchet, Jörg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models accumulate vast knowledge during pre-training, yet the
dynamics governing this acquisition remain poorly understood. This work
investigates the learning dynamics of language models on a synthetic factual
recall task, uncovering three key findings: First, language models learn in
three phases, exhibiting a performance plateau before acquiring precise factual
knowledge. Mechanistically, this plateau coincides with the formation of
attention-based circuits that support recall. Second, the training data
distribution significantly impacts learning dynamics, as imbalanced
distributions lead to shorter plateaus. Finally, hallucinations emerge
simultaneously with knowledge, and integrating new knowledge into the model
through fine-tuning is challenging, as it quickly corrupts its existing
parametric memories. Our results emphasize the importance of data distribution
in knowledge acquisition and suggest novel data scheduling strategies to
accelerate neural network training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Extreme Response Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Edwards, Michael D. Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A method to rapidly estimate extreme ship response events is developed in
this paper. The method involves training by a Long Short-Term Memory (LSTM)
neural network to correct a lower-fidelity hydrodynamic model to the level of a
higher-fidelity simulation. More focus is placed on larger responses by
isolating the time-series near peak events identified in the lower-fidelity
simulations and training on only the shorter time-series around the large
event. The method is tested on the estimation of pitch time-series maxima in
Sea State 5 (significant wave height of 4.0 meters and modal period of 15.0
seconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCode
and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP).
The results are also compared with an LSTM trained without special
considerations for large events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>From the 35th Symposium on Naval Hydrodynamics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusterSC: Advancing Synthetic Control with Donor Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeyoung Rho, Andrew Tang, Noah Bergam, Rachel Cummings, Vishal Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In causal inference with observational studies, synthetic control (SC) has
emerged as a prominent tool. SC has traditionally been applied to
aggregate-level datasets, but more recent work has extended its use to
individual-level data. As they contain a greater number of observed units, this
shift introduces the curse of dimensionality to SC. To address this, we propose
Cluster Synthetic Control (ClusterSC), based on the idea that groups of
individuals may exist where behavior aligns internally but diverges between
groups. ClusterSC incorporates a clustering step to select only the relevant
donors for the target. We provide theoretical guarantees on the improvements
induced by ClusterSC, supported by empirical demonstrations on synthetic and
real-world datasets. The results indicate that ClusterSC consistently
outperforms classical SC approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 figures, to be published in Proceedings of The 28th
  International Conference on Artificial Intelligence and Statistics (AIStats)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Reduction in Communication Rounds for Non-Smooth Convex
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karlo Palenzuela, Ali Dadras, Alp Yurtsever, Tommy Löfstedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple local steps are key to communication-efficient federated learning.
However, theoretical guarantees for such algorithms, without data
heterogeneity-bounding assumptions, have been lacking in general non-smooth
convex problems. Leveraging projection-efficient optimization methods, we
propose FedMLS, a federated learning algorithm with provable improvements from
multiple local steps. FedMLS attains an $\epsilon$-suboptimal solution in
$\mathcal{O}(1/\epsilon)$ communication rounds, requiring a total of
$\mathcal{O}(1/\epsilon^2)$ stochastic subgradient oracle calls.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Language Models for Analyzing Longitudinal Experiential Data
  in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahatsham Hayat, Bilal Khan, Mohammad Rashedul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to leveraging pre-trained language models (LMs)
for early forecasting of academic trajectories in STEM students using
high-dimensional longitudinal experiential data. This data, which captures
students' study-related activities, behaviors, and psychological states, offers
valuable insights for forecasting-based interventions. Key challenges in
handling such data include high rates of missing values, limited dataset size
due to costly data collection, and complex temporal variability across
modalities. Our approach addresses these issues through a comprehensive data
enrichment process, integrating strategies for managing missing values,
augmenting data, and embedding task-specific instructions and contextual cues
to enhance the models' capacity for learning temporal patterns. Through
extensive experiments on a curated student learning dataset, we evaluate both
encoder-decoder and decoder-only LMs. While our findings show that LMs
effectively integrate data across modalities and exhibit resilience to missing
data, they primarily rely on high-level statistical patterns rather than
demonstrating a deeper understanding of temporal dynamics. Furthermore, their
ability to interpret explicit temporal information remains limited. This work
advances educational data science by highlighting both the potential and
limitations of LMs in modeling student trajectories for early intervention
based on longitudinal experiential data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Multiple Response Regression and Learning of Latent Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Sanyou Wu, Long Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying low-dimensional latent structures within high-dimensional data
has long been a central topic in the machine learning community, driven by the
need for data compression, storage, transmission, and deeper data
understanding. Traditional methods, such as principal component analysis (PCA)
and autoencoders (AE), operate in an unsupervised manner, ignoring label
information even when it is available. In this work, we introduce a unified
method capable of learning latent spaces in both unsupervised and supervised
settings. We formulate the problem as a nonlinear multiple-response regression
within an index model context. By applying the generalized Stein's lemma, the
latent space can be estimated without knowing the nonlinear link functions. Our
method can be viewed as a nonlinear generalization of PCA. Moreover, unlike AE
and other neural network methods that operate as "black boxes", our approach
not only offers better interpretability but also reduces computational
complexity while providing strong theoretical guarantees. Comprehensive
numerical experiments and real data analyses demonstrate the superior
performance of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable Implicit Neural Representations via Parameterized Latent
  Dynamics for Baroclinic Ocean Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Zhao, Xihaier Luo, Seungjun Lee, Yihui Ren, Shinjae Yoo, Luke Van Roekel, Balu Nadiga, Sri Hari Krishna Narayanan, Yixuan Sun, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesoscale ocean dynamics play a critical role in climate systems, governing
heat transport, hurricane genesis, and drought patterns. However, simulating
these processes at high resolution remains computationally prohibitive due to
their nonlinear, multiscale nature and vast spatiotemporal domains. Implicit
neural representations (INRs) reduce the computational costs as
resolution-independent surrogates but fail in many-query scenarios (inverse
modeling) requiring rapid evaluations across diverse parameters. We present
PINROD, a novel framework combining dynamics-aware implicit neural
representations with parameterized neural ordinary differential equations to
address these limitations. By integrating parametric dependencies into latent
dynamics, our method efficiently captures nonlinear oceanic behavior across
varying boundary conditions and physical parameters. Experiments on ocean
mesoscale activity data show superior accuracy over existing baselines and
improved computational efficiency compared to standard numerical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Functional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixu Wang, Jiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional functional time series (HDFTS) are often characterized by
nonlinear trends and high spatial dimensions. Such data poses unique challenges
for modeling and forecasting due to the nonlinearity, nonstationarity, and high
dimensionality. We propose a novel probabilistic functional neural network
(ProFnet) to address these challenges. ProFnet integrates the strengths of
feedforward and deep neural networks with probabilistic modeling. The model
generates probabilistic forecasts using Monte Carlo sampling and also enables
the quantification of uncertainty in predictions. While capturing both temporal
and spatial dependencies across multiple regions, ProFnet offers a scalable and
unified solution for large datasets. Applications to Japan's mortality rates
demonstrate superior performance. This approach enhances predictive accuracy
and provides interpretable uncertainty estimates, making it a valuable tool for
forecasting complex high-dimensional functional data and HDFTS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion of Graph Neural Networks via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weronika Ormaniec, Michael Vollenweider, Elisa Hoskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the idea of combining GCNs into one model. To that
end, we align the weights of different models layer-wise using optimal
transport (OT). We present and evaluate three types of transportation costs and
show that the studied fusion method consistently outperforms the performance of
vanilla averaging. Finally, we present results suggesting that model fusion
using OT is harder in the case of GCNs than MLPs and that incorporating the
graph structure into the process does not improve the performance of the
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent Multigroup Low-Rank Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonis Matakos, Martino Ciaperoni, Heikki Mannila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of consistent low-rank approximation for multigroup
data: we ask for a sequence of $k$ basis vectors such that projecting the data
onto their spanned subspace treats all groups as equally as possible, by
minimizing the maximum error among the groups. Additionally, we require that
the sequence of basis vectors satisfies the natural consistency property: when
looking for the best $k$ vectors, the first $d<k$ vectors are the best possible
solution to the problem of finding $d$ basis vectors. Thus, this multigroup
low-rank approximation method naturally generalizes \svd and reduces to \svd
for data with a single group. We give an iterative algorithm for this task that
sequentially adds to the basis the vector that gives the best rank$-1$
projection according to the min-max criterion, and then projects the data onto
the orthogonal complement of that vector. For finding the best rank$-1$
projection, we use primal-dual approaches or semidefinite programming. We
analyze the theoretical properties of the algorithms and demonstrate
empirically that the proposed methods compare favorably to existing methods for
multigroup (or fair) PCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncSDE: A Probabilistic Framework for Diffusion Synchronization <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjun Lee, Hyunsoo Lee, Sookwan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been many attempts to leverage multiple diffusion models for
collaborative generation, extending beyond the original domain. A prominent
approach involves synchronizing multiple diffusion trajectories by mixing the
estimated scores to artificially correlate the generation processes. However,
existing methods rely on naive heuristics, such as averaging, without
considering task specificity. These approaches do not clarify why such methods
work and often fail when a heuristic suitable for one task is blindly applied
to others. In this paper, we present a probabilistic framework for analyzing
why diffusion synchronization works and reveal where heuristics should be
focused - modeling correlations between multiple trajectories and adapting them
to each specific task. We further identify optimal correlation models per task,
achieving better results than previous approaches that apply a single heuristic
across all tasks without justification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formation Shape Control using the Gromov-Wasserstein Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haruto Nakashima, Siddhartha Ganguly, Kohei Morimoto, Kenji Kashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces a formation shape control algorithm, in the optimal
control framework, for steering an initial population of agents to a desired
configuration via employing the Gromov-Wasserstein distance. The underlying
dynamical system is assumed to be a constrained linear system and the objective
function is a sum of quadratic control-dependent stage cost and a
Gromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost
transforms the resulting optimal control problem into a well-known NP-hard
problem, making it both numerically demanding and difficult to solve with high
accuracy. Towards that end, we employ a recent semi-definite relaxation-driven
technique to tackle the Gromov-Wasserstein distance. A numerical example is
provided to illustrate our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of Learning for Dynamics and Control
  (L4DC) conference, PMLR, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into
  Bosons, Hierarchical Learning and Symmetry Breaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Quetzalcóatl Toledo-Marin, Anindita Maiti, Geoffrey C. Fox, Roger G. Melko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have become ubiquitous due to their ability to learn
and sample from complex distributions. Despite the proliferation of various
frameworks, the relationships among these models remain largely unexplored, a
gap that hinders the development of a unified theory of AI learning. We address
two central challenges: clarifying the connections between different deep
generative models and deepening our understanding of their learning mechanisms.
We focus on Restricted Boltzmann Machines (RBMs), known for their universal
approximation capabilities for discrete distributions. By introducing a
reciprocal space formulation, we reveal a connection between RBMs, diffusion
processes, and coupled Bosons. We show that at initialization, the RBM operates
at a saddle point, where the local curvature is determined by the singular
values, whose distribution follows the Marcenko-Pastur law and exhibits
rotational symmetry. During training, this rotational symmetry is broken due to
hierarchical learning, where different degrees of freedom progressively capture
features at multiple levels of abstraction. This leads to a symmetry breaking
in the energy landscape, reminiscent of Landau theory. This symmetry breaking
in the energy landscape is characterized by the singular values and the weight
matrix eigenvector matrix. We derive the corresponding free energy in a
mean-field approximation. We show that in the limit of infinite size RBM, the
reciprocal variables are Gaussian distributed. Our findings indicate that in
this regime, there will be some modes for which the diffusion process will not
converge to the Boltzmann distribution. To illustrate our results, we trained
replicas of RBMs with different hidden layer sizes using the MNIST dataset. Our
findings bridge the gap between disparate generative frameworks and also shed
light on the processes underpinning learning in generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pp, 8figs, research article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Pseudo Posterior Mechanism for Differentially Private Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Chew, Matthew R. Williams, Elan A. Segarra, Alexander J. Preiss, Amanda Konet, Terrance D. Savitsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) is becoming increasingly important for deployed
machine learning applications because it provides strong guarantees for
protecting the privacy of individuals whose data is used to train models.
However, DP mechanisms commonly used in machine learning tend to struggle on
many real world distributions, including highly imbalanced or small labeled
training sets. In this work, we propose a new scalable DP mechanism for deep
learning models, SWAG-PPM, by using a pseudo posterior distribution that
downweights by-record likelihood contributions proportionally to their
disclosure risks as the randomized mechanism. As a motivating example from
official statistics, we demonstrate SWAG-PPM on a workplace injury text
classification task using a highly imbalanced public dataset published by the
U.S. Occupational Safety and Health Administration (OSHA). We find that
SWAG-PPM exhibits only modest utility degradation against a non-private
comparator while greatly outperforming the industry standard DP-SGD for a
similar privacy budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constraint-based causal discovery with tiered background knowledge and
  latent variables in single or overlapping <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine W. Bang, Vanessa Didelez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider the use of tiered background knowledge within
constraint based causal discovery. Our focus is on settings relaxing causal
sufficiency, i.e. allowing for latent variables which may arise because
relevant information could not be measured at all, or not jointly, as in the
case of multiple overlapping datasets. We first present novel insights into the
properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce
a new extension of the IOD (integrating overlapping datasets) algorithm
incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm.
We show that under full usage of the tiered background knowledge tFCI and tIOD
are sound, while simple versions of the tIOD and tFCI are sound and complete.
We further show that the tIOD algorithm can often be expected to be
considerably more efficient and informative than the IOD algorithm even beyond
the obvious restriction of the Markov equivalence classes. We provide a formal
result on the conditions for this gain in efficiency and informativeness. Our
results are accompanied by a series of examples illustrating the exact role and
usefulness of tiered background knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 4th Conference on Causal Learning and Reasoning
  (CLeaR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Bayesian machine learning modelling of land cover
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Bilson, Anna Pustogvar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land cover classification involves the production of land cover maps, which
determine the type of land through remote sensing imagery. Over recent years,
such classification is being performed by machine learning classification
models, which can give highly accurate predictions on land cover per pixel
using large quantities of input training data. However, such models do not
currently take account of input measurement uncertainty, which is vital for
traceability in metrology. In this work we propose a Bayesian classification
framework using generative modelling to take account of input measurement
uncertainty. We take the specific case of Bayesian quadratic discriminant
analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in
2020 and 2021. We benchmark the performance of the model against more popular
classification models used in land cover maps such as random forests and neural
networks. We find that such Bayesian models are more trustworthy, in the sense
that they are more interpretable, explicitly model the input measurement
uncertainty, and maintain predictive performance of class probability outputs
across datasets of different years and sizes, whilst also being computationally
efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ F-INR: Functional Tensor Decomposition for Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Karthikeya Vemuri, Tim Büchner, Joachim Denzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representation (INR) has emerged as a powerful tool for
encoding discrete signals into continuous, differentiable functions using
neural networks. However, these models often have an unfortunate reliance on
monolithic architectures to represent high-dimensional data, leading to
prohibitive computational costs as dimensionality grows. We propose F-INR, a
framework that reformulates INR learning through functional tensor
decomposition, breaking down high-dimensional tasks into lightweight,
axis-specific sub-networks. Each sub-network learns a low-dimensional data
component (e.g., spatial or temporal). Then, we combine these components via
tensor operations, reducing forward pass complexity while improving accuracy
through specialized learning. F-INR is modular and, therefore,
architecture-agnostic, compatible with MLPs, SIREN, WIRE, or other
state-of-the-art INR architecture. It is also decomposition-agnostic,
supporting CP, TT, and Tucker modes with user-defined rank for speed-accuracy
control. In our experiments, F-INR trains $100\times$ faster than existing
approaches on video tasks while achieving higher fidelity (+3.4 dB PSNR).
Similar gains hold for image compression, physics simulations, and 3D geometry
reconstruction. Through this, F-INR offers a new scalable, flexible solution
for high-dimensional signal modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 33 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust DNN Partitioning and Resource Allocation Under Uncertain
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaojun Nan, Yunchu Han, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In edge intelligence systems, deep neural network (DNN) partitioning and data
offloading can provide real-time task inference for resource-constrained mobile
devices. However, the inference time of DNNs is typically uncertain and cannot
be precisely determined in advance, presenting significant challenges in
ensuring timely task processing within deadlines. To address the uncertain
inference time, we propose a robust optimization scheme to minimize the total
energy consumption of mobile devices while meeting task probabilistic
deadlines. The scheme only requires the mean and variance information of the
inference time, without any prediction methods or distribution functions. The
problem is formulated as a mixed-integer nonlinear programming (MINLP) that
involves jointly optimizing the DNN model partitioning and the allocation of
local CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first
decompose the original problem into two subproblems: resource allocation and
DNN model partitioning. Subsequently, the two subproblems with probability
constraints are equivalently transformed into deterministic optimization
problems using the chance-constrained programming (CCP) method. Finally, the
convex optimization technique and the penalty convex-concave procedure (PCCP)
technique are employed to obtain the optimal solution of the resource
allocation subproblem and a stationary point of the DNN model partitioning
subproblem, respectively. The proposed algorithm leverages real-world data from
popular hardware platforms and is evaluated on widely used DNN models.
Extensive simulations show that our proposed algorithm effectively addresses
the inference time uncertainty with probabilistic deadline guarantees while
minimizing the energy consumption of mobile devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepRV: <span class="highlight-title">pre-train</span>ed spatial priors for accelerated disease mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jhonathan Navott, Daniel Jenson, Seth Flaxman, Elizaveta Semenova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently introduced prior-encoding deep generative models (e.g., PriorVAE,
$\pi$VAE, and PriorCVAE) have emerged as powerful tools for scalable Bayesian
inference by emulating complex stochastic processes like Gaussian processes
(GPs). However, these methods remain largely a proof-of-concept and
inaccessible to practitioners. We propose DeepRV, a lightweight, decoder-only
approach that accelerates training, and enhances real-world applicability in
comparison to current VAE-based prior encoding approaches. Leveraging
probabilistic programming frameworks (e.g., NumPyro) for inference, DeepRV
achieves significant speedups while also improving the quality of parameter
inference, closely matching full MCMC sampling. We showcase its effectiveness
in process emulation and spatial analysis of the UK using simulated data,
gender-wise cancer mortality rates for individuals under 50, and HIV prevalence
in Zimbabwe. To bridge the gap between theory and practice, we provide a
user-friendly API, enabling scalable and efficient Bayesian inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker
  Availability Windows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwen Chen, Jiannan Guo, Dazhuo Qiu, Yawen Li, Guanhua Ye, Yan Zhao, Kai Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of mobile networks and the widespread use of
mobile devices, spatial crowdsourcing, which involves assigning location-based
tasks to mobile workers, has gained significant attention. However, most
existing research focuses on task assignment at the current moment, overlooking
the fluctuating demand and supply between tasks and workers over time. To
address this issue, we introduce an adaptive task assignment problem, which
aims to maximize the number of assigned tasks by dynamically adjusting task
assignments in response to changing demand and supply. We develop a spatial
crowdsourcing framework, namely demand-based adaptive task assignment with
dynamic worker availability windows, which consists of two components including
task demand prediction and task assignment. In the first component, we
construct a graph adjacency matrix representing the demand dependency
relationships in different regions and employ a multivariate time series
learning approach to predict future task demands. In the task assignment
component, we adjust tasks to workers based on these predictions, worker
availability windows, and the current task assignments, where each worker has
an availability window that indicates the time periods they are available for
task assignments. To reduce the search space of task assignments and be
efficient, we propose a worker dependency separation approach based on graph
partition and a task value function with reinforcement learning. Experiments on
real data demonstrate that our proposals are both effective and efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Engrams for Efficient Continual Learning with Binarized
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Aguilar, Luis Fernando Herbozo Contreras, Omid Kavehei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to learn continuously in artificial neural networks (ANNs) is
often limited by catastrophic forgetting, a phenomenon in which new knowledge
becomes dominant. By taking mechanisms of memory encoding in neuroscience (aka.
engrams) as inspiration, we propose a novel approach that integrates
stochastically-activated engrams as a gating mechanism for metaplastic
binarized neural networks (mBNNs). This method leverages the computational
efficiency of mBNNs combined with the robustness of probabilistic memory traces
to mitigate forgetting and maintain the model's reliability. Previously
validated metaplastic optimization techniques have been incorporated to enhance
synaptic stability further. Compared to baseline binarized models and benchmark
fully connected continual learning approaches, our method is the only strategy
capable of reaching average accuracies over 20% in class-incremental scenarios
and achieving comparable domain-incremental results to full precision
state-of-the-art methods. Furthermore, we achieve a significant reduction in
peak GPU and RAM usage, under 5% and 20%, respectively. Our findings
demonstrate (A) an improved stability vs. plasticity trade-off, (B) a reduced
memory intensiveness, and (C) an enhanced performance in binarized
architectures. By uniting principles of neuroscience and efficient computing,
we offer new insights into the design of scalable and robust deep learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the flavor structure of leptons via diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satsuki Nishimura, Hajime Otsuka, Haruki Uchiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to explore the flavor structure of leptons using
diffusion models, which are known as one of generative artificial intelligence
(generative AI). We consider a simple extension of the Standard Model with the
type I seesaw mechanism and train a neural network to generate the neutrino
mass matrix. By utilizing transfer learning, the diffusion model generates 104
solutions that are consistent with the neutrino mass squared differences and
the leptonic mixing angles. The distributions of the CP phases and the sums of
neutrino masses, which are not included in the conditional labels but are
calculated from the solutions, exhibit non-trivial tendencies. In addition, the
effective mass in neutrinoless double beta decay is concentrated near the
boundaries of the existing confidence intervals, allowing us to verify the
obtained solutions through future experiments. An inverse approach using the
diffusion model is expected to facilitate the experimental verification of
flavor models from a perspective distinct from conventional analytical methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearest Neighbour Equilibrium Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David P. Hofmeyr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel and intuitive nearest neighbours based clustering algorithm is
introduced, in which a cluster is defined in terms of an equilibrium condition
which balances its size and cohesiveness. The formulation of the equilibrium
condition allows for a quantification of the strength of alignment of each
point to a cluster, with these cluster alignment strengths leading naturally to
a model selection criterion which renders the proposed approach fully
automatable. The algorithm is simple to implement and computationally
efficient, and produces clustering solutions of extremely high quality in
comparison with relevant benchmarks from the literature. R code to implement
the approach is available from https://github.com/DavidHofmeyr/NNEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently being considered for publication by IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram
  Model <span class="chip">ICDE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Zhang, Qingqing Ye, Haibo Hu, Jianliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The skip-gram model (SGM), which employs a neural network to generate node
vectors, serves as the basis for numerous popular graph embedding techniques.
However, since the training datasets contain sensitive linkage information, the
parameters of a released SGM may encode private information and pose
significant privacy risks. Differential privacy (DP) is a rigorous standard for
protecting individual privacy in data analysis. Nevertheless, when applying
differential privacy to skip-gram in graphs, it becomes highly challenging due
to the complex link relationships, which potentially result in high sensitivity
and necessitate substantial noise injection. To tackle this challenge, we
present AdvSGM, a differentially private skip-gram for graphs via adversarial
training. Our core idea is to leverage adversarial training to privatize
skip-gram while improving its utility. Towards this end, we develop a novel
adversarial training module by devising two optimizable noise terms that
correspond to the parameters of a skip-gram. By fine-tuning the weights between
modules within AdvSGM, we can achieve differentially private gradient updates
without additional noise injection. Extensive experimental results on six
real-world graph datasets show that AdvSGM preserves high data utility across
different downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Deep Learning to LLMs: A <span class="highlight-title">survey</span> of AI in Quantitative Investment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bokai Cao, Saizhuo Wang, Xinyi Lin, Xiaojun Wu, Haohan Zhang, Lionel M. Ni, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative investment (quant) is an emerging, technology-driven approach in
asset management, increasingy shaped by advancements in artificial
intelligence. Recent advances in deep learning and large language models (LLMs)
for quant finance have improved predictive modeling and enabled agent-based
automation, suggesting a potential paradigm shift in this field. In this
survey, taking alpha strategy as a representative example, we explore how AI
contributes to the quantitative investment pipeline. We first examine the early
stage of quant research, centered on human-crafted features and traditional
statistical models with an established alpha pipeline. We then discuss the rise
of deep learning, which enabled scalable modeling across the entire pipeline
from data processing to order execution. Building on this, we highlight the
emerging role of LLMs in extending AI beyond prediction, empowering autonomous
agents to process unstructured data, generate alphas, and support
self-iterative workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Xu, Yaoyu Cheng, Pinxi Shen, Lin Zhao,  Electrical, Computer Engineering, National University of Singapore,  Singapore, Mechanical Engineering, National University of Singapore,  Singapore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots can learn versatile locomotion skills but remain
vulnerable when one or more joints lose power. In contrast, dogs and cats can
adopt limping gaits when injured, demonstrating their remarkable ability to
adapt to physical conditions. Inspired by such adaptability, this paper
presents Action Learner (AcL), a novel teacher-student reinforcement learning
framework that enables quadrupeds to autonomously adapt their gait for stable
walking under multiple joint faults. Unlike conventional teacher-student
approaches that enforce strict imitation, AcL leverages teacher policies to
generate style rewards, guiding the student policy without requiring precise
replication. We train multiple teacher policies, each corresponding to a
different fault condition, and subsequently distill them into a single student
policy with an encoder-decoder architecture. While prior works primarily
address single-joint faults, AcL enables quadrupeds to walk with up to four
faulty joints across one or two legs, autonomously switching between different
limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot
under single- and double-joint faults, demonstrating fault-tolerant, stable
walking, smooth gait transitions between normal and lamb gaits, and robustness
against external disturbances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification
  via Multi-Depth Networks <span class="chip">CVPR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wallin, Fredrik Kahl, Lars Hammarstrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection in deep learning has traditionally been
framed as a binary task, where samples are either classified as belonging to
the known classes or marked as OOD, with little attention given to the semantic
relationships between OOD samples and the in-distribution (ID) classes. We
propose a framework for detecting and classifying OOD samples in a given class
hierarchy. Specifically, we aim to predict OOD data to their correct internal
nodes of the class hierarchy, whereas the known ID classes should be predicted
as their corresponding leaf nodes. Our approach leverages the class hierarchy
to create a probabilistic model and we implement this model by using networks
trained for ID classification at multiple hierarchy depths. We conduct
experiments on three datasets with predefined class hierarchies and show the
effectiveness of our method. Our code is available at
https://github.com/walline/prohoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Large Language Model with Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement
Learning (RL) has proven to be an effective approach. However, LLMs do not
inherently define the structure of an agent for RL training, particularly in
terms of defining the action space. This paper studies learning a compact
latent action space to enhance the controllability and exploration of RL for
LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),
a framework that integrates a latent action space into pre-trained LLMs. We
apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,
compared to RL with token-level actions, CoLA's latent action enables greater
semantic diversity in text generation. For enhancing downstream tasks, we show
that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing
the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo
Tree Search variant. Furthermore, CoLA with RL consistently improves
performance on agent-based tasks without degrading the pre-trained LLM's
capabilities, unlike the baseline. Finally, CoLA reduces computation time by
half in tasks involving enhanced thinking prompts for LLMs by RL. These results
highlight CoLA's potential to advance RL-based adaptation of LLMs for
downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning LLMs on Small Medical <span class="highlight-title">Dataset</span>s: Text Classification and
  Normalization Effectiveness on Cardiology reports and Discharge records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Losch, Lucas Plagwitz, Antonius Büscher, Julian Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effectiveness of fine-tuning large language models (LLMs)
on small medical datasets for text classification and named entity recognition
tasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge
dataset, we demonstrate that fine-tuning small LLMs locally on limited training
data can improve performance achieving comparable results to larger models. Our
experiments show that fine-tuning improves performance on both tasks, with
notable gains observed with as few as 200-300 training examples. Overall, the
study highlights the potential of task-specific fine-tuning of LLMs for
automating clinical workflows and efficiently extracting structured data from
unstructured medical text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Expectation Estimation with Subtractive Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Zellinger, Nicola Branchini, Víctor Elvira, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models
(MMs) for their simplicity and ability to capture multimodal distributions.
Recently, subtractive mixture models (SMMs), i.e. MMs with negative
coefficients, have shown greater expressiveness and success in generative
modeling. However, their negative parameters complicate sampling, requiring
costly auto-regressive techniques or accept-reject algorithms that do not scale
in high dimensions. In this work, we use the difference representation of SMMs
to construct an unbiased IS estimator ($\Delta\text{Ex}$) that removes the need
to sample from the SMM, enabling high-dimensional expectation estimation with
SMMs. In our experiments, we show that $\Delta\text{Ex}$ can achieve comparable
estimation quality to auto-regressive sampling while being considerably faster
in MC estimation. Moreover, we conduct initial experiments with
$\Delta\text{Ex}$ using hand-crafted proposals, gaining first insights into how
to construct safe proposals for $\Delta\text{Ex}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DuckSegmentation: A segmentation model based on the AnYue Hemp Duck
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Feng, Tianyu Xie, Wei Ma, Ruijie Fu, Yingxiao Zhang, Jun Li, Bei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modernization of smart farming is a way to improve agricultural
production efficiency, and improve the agricultural production environment.
Although many large models have achieved high accuracy in the task of object
recognition and segmentation, they cannot really be put into use in the farming
industry due to their own poor interpretability and limitations in
computational volume. In this paper, we built AnYue Shelduck Dateset, which
contains a total of 1951 Shelduck datasets, and performed target detection and
segmentation annotation with the help of professional annotators. Based on
AnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and
powerful module for duck identification based on real shelduckfarms. First of
all, using the YOLOv8 module designed to divide the mahjong between them,
Precision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on
the test set. Again using the DuckSegmentation segmentation model,
DuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation
was used as the teacher model, and through knowledge distillation, Deeplabv3
r50 was used as the student model, and the final student model achieved 94.49%
mIoU on the test set. The method provides a new way of thinking in practical
sisal duck smart farming.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Boosting Machine for Predicting Claim Severity and Frequency
  in Car Insurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markéta Krùpovà, Nabil Rachdi, Quentin Guibert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a context of constant increase in competition and heightened regulatory
pressure, accuracy, actuarial precision, as well as transparency and
understanding of the tariff, are key issues in non-life insurance.
Traditionally used generalized linear models (GLM) result in a multiplicative
tariff that favors interpretability. With the rapid development of machine
learning and deep learning techniques, actuaries and the rest of the insurance
industry have adopted these techniques widely. However, there is a need to
associate them with interpretability techniques. In this paper, our study
focuses on introducing an Explainable Boosting Machine (EBM) model that
combines intrinsically interpretable characteristics and high prediction
performance. This approach is described as a glass-box model and relies on the
use of a Generalized Additive Model (GAM) and a cyclic gradient boosting
algorithm. It accounts for univariate and pairwise interaction effects between
features and provides naturally explanations on them. We implement this
approach on car insurance frequency and severity data and extensively compare
the performance of this approach with classical competitors: a GLM, a GAM, a
CART model and an Extreme Gradient Boosting (XGB) algorithm. Finally, we
examine the interpretability of these models to capture the main determinants
of claim costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tricking Retrievers with Influential Tokens: An Efficient Black-Box
  Corpus Poisoning Attack <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, Yiwei Wang, Yujun Cai, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) systems enhance large language models by
incorporating external knowledge, addressing issues like outdated internal
knowledge and hallucination. However, their reliance on external knowledge
bases makes them vulnerable to corpus poisoning attacks, where adversarial
passages can be injected to manipulate retrieval results. Existing methods for
crafting such passages, such as random token replacement or training inversion
models, are often slow and computationally expensive, requiring either access
to retriever's gradients or large computational resources. To address these
limitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an
efficient black-box method that leverages two key properties of retrievers:
insensitivity to token order and bias towards influential tokens. By focusing
on these characteristics, DIGA dynamically adjusts its genetic operations to
generate effective adversarial passages with significantly reduced time and
memory usage. Our experimental evaluation shows that DIGA achieves superior
efficiency and scalability compared to existing methods, while maintaining
comparable or better attack success rates across multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation-informed deep learning for enhanced SWOT observations of
  fine-scale ocean dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenio Cutolo, Carlos Granero-Belinchon, Ptashanna Thiraux, Jinbo Wang, Ronan Fablet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oceanic processes at fine scales are crucial yet difficult to observe
accurately due to limitations in satellite and in-situ measurements. The
Surface Water and Ocean Topography (SWOT) mission provides high-resolution Sea
Surface Height (SSH) data, though noise patterns often obscure fine scale
structures. Current methods struggle with noisy data or require extensive
supervised training, limiting their effectiveness on real-world observations.
We introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative
Ensemble Networks), an unsupervised adversarial learning framework combining
real SWOT observations with simulated reference data. SIMPGEN leverages
wavelet-informed neural metrics to distinguish noisy from clean fields, guiding
realistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively
removes noise, preserving fine-scale features better than existing neural
methods. This robust, unsupervised approach not only improves SWOT SSH data
interpretation but also demonstrates strong potential for broader oceanographic
applications, including data assimilation and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOT: Hadamard-based Optimized Training <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonggon Kim, Juncheol Shin, Seung-taek Woo, Eunhyeok Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has become increasingly important to optimize backpropagation to reduce
memory usage and computational overhead. Achieving this goal is highly
challenging, as multiple objectives must be considered jointly while
maintaining training quality. In this paper, we focus on matrix multiplication,
which accounts for the largest portion of training costs, and analyze its
backpropagation in detail to identify lightweight techniques that offer the
best benefits. Based on this analysis, we introduce a novel method,
Hadamard-based Optimized Training (HOT). In this approach, we apply
Hadamard-based optimizations, such as Hadamard quantization and Hadamard
low-rank approximation, selectively and with awareness of the suitability of
each optimization for different backward paths. Additionally, we introduce two
enhancements: activation buffer compression and layer-wise quantizer selection.
Our extensive analysis shows that HOT achieves up to 75% memory savings and a
2.6 times acceleration on real GPUs, with negligible accuracy loss compared to
FP32 precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Learning for Entropy-regularized Markov Decision Processes via
  Multilevel Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meunier, Christoph Reisinger, Yufei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing efficient learning algorithms with complexity guarantees for Markov
decision processes (MDPs) with large or continuous state and action spaces
remains a fundamental challenge. We address this challenge for
entropy-regularized MDPs with Polish state and action spaces, assuming access
to a generative model of the environment. We propose a novel family of
multilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration
with MLMC techniques and a generic stochastic approximation of the Bellman
operator. We quantify the precise impact of the chosen approximate Bellman
operator on the accuracy of the resulting MLMC estimator. Leveraging this error
analysis, we show that using a biased plain MC estimate for the Bellman
operator results in quasi-polynomial sample complexity, whereas an unbiased
randomized multilevel approximation of the Bellman operator achieves polynomial
sample complexity in expectation. Notably, these complexity bounds are
independent of the dimensions or cardinalities of the state and action spaces,
distinguishing our approach from existing algorithms whose complexities scale
with the sizes of these spaces. We validate these theoretical performance
guarantees through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Graph Structure Learning in the Era of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the emergence of large language models (LLMs) has prompted
researchers to explore the integration of language descriptions into graphs,
aiming to enhance model encoding capabilities from a data-centric perspective.
This graph representation is called text-attributed graphs (TAGs). A review of
prior advancements highlights that graph structure learning (GSL) is a pivotal
technique for improving data utility, making it highly relevant to efficient
TAG learning. However, most GSL methods are tailored for traditional graphs
without textual information, underscoring the necessity of developing a new GSL
paradigm. Despite clear motivations, it remains challenging: (1) How can we
define a reasonable optimization objective for GSL in the era of LLMs,
considering the massive parameters in LLM? (2) How can we design an efficient
model architecture that enables seamless integration of LLM for this
optimization objective? For Question 1, we reformulate existing GSL
optimization objectives as a tree optimization framework, shifting the focus
from obtaining a well-trained edge predictor to a language-aware tree sampler.
For Question 2, we propose decoupled and training-free model design principles
for LLM integration, shifting the focus from computation-intensive fine-tuning
to more efficient inference. Based on this, we propose Large Language and Tree
Assistant (LLaTA), which leverages tree-based LLM in-context learning to
enhance the understanding of topology and text, enabling reliable inference and
generating improved graph structure. Extensive experiments on 10 TAG datasets
demonstrate that LLaTA enjoys flexibility - incorporated with any backbone;
scalability - outperforms other LLM-based GSL methods in terms of running
efficiency; effectiveness - achieves SOTA performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Federated Fine-Tuning Large Language Models for
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Liu, Yunming Liao, Hongli Xu, Yang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) via federated learning, i.e.,
FedLLM, has been proposed to adapt LLMs for various downstream applications in
a privacy-preserving way. To reduce the fine-tuning costs on
resource-constrained devices, FedLoRA is proposed to fine-tune only a small
subset of model parameters by integrating low-rank adaptation (LoRA) into
FedLLM. However, apart from resource constraints, there is still another
critical challenge, i.e., data heterogeneity, severely hindering the
implementation of FedLoRA in practical applications. Herein, inspired by the
previous group-based federated learning paradigm, we propose a hierarchical
FedLoRA framework, termed HierFedLoRA, to address these challenges.
Specifically, HierFedLoRA partitions all devices into multiple near-IID groups
and adjusts the intra-group aggregation frequency for each group to eliminate
the negative effects of non-IID data. Meanwhile, to reduce the computation and
communication cost, HierFedLoRA dynamically assigns diverse and suitable
fine-tuning depth (i.e., the number of continuous fine-tuning layers from the
output) for each group. HierFedLoRA explores jointly optimizing aggregation
frequency and depth upon their coupled relationship to better enhance the
performance of FedLoRA. Extensive experiments are conducted on a physical
platform with 80 commercial devices. The results show that HierFedLoRA improves
the final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process
by at least 2.1$\times$, compared to the strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO
  Skilfully Beyond 2 Years 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rixu Hao, Yuxin Zhao, Shaoqing Zhang, Guihua Wang, Xiong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  El Ni\~no-Southern Oscillation (ENSO) exerts global climate and societal
impacts, but real-time prediction with lead times beyond one year remains
challenging. Dynamical models suffer from large biases and uncertainties, while
deep learning struggles with interpretability and multi-scale dynamics. Here,
we introduce PTSTnet, an interpretable model that unifies dynamical processes
and cross-scale spatiotemporal learning in an innovative neural-network
framework with physics-encoding learning. PTSTnet produces interpretable
predictions significantly outperforming state-of-the-art benchmarks with lead
times beyond 24 months, providing physical insights into error propagation in
ocean-atmosphere interactions. PTSTnet learns feature representations with
physical consistency from sparse data to tackle inherent multi-scale and
multi-physics challenges underlying ocean-atmosphere processes, thereby
inherently enhancing long-term prediction skill. Our successful realizations
mark substantial steps forward in interpretable insights into innovative neural
ocean modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Generalizable Skills from Offline Multi-Task Data for
  Multi-Agent Cooperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Liu, Yang Shu, Chenjuan Guo, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning cooperative multi-agent policy from offline multi-task data that can
generalize to unseen tasks with varying numbers of agents and targets is an
attractive problem in many scenarios. Although aggregating general behavior
patterns among multiple tasks as skills to improve policy transfer is a
promising approach, two primary challenges hinder the further advancement of
skill learning in offline multi-task MARL. Firstly, extracting general
cooperative behaviors from various action sequences as common skills lacks
bringing cooperative temporal knowledge into them. Secondly, existing works
only involve common skills and can not adaptively choose independent knowledge
as task-specific skills in each task for fine-grained action execution. To
tackle these challenges, we propose Hierarchical and Separate Skill Discovery
(HiSSD), a novel approach for generalizable offline multi-task MARL through
skill learning. HiSSD leverages a hierarchical framework that jointly learns
common and task-specific skills. The common skills learn cooperative temporal
knowledge and enable in-sample exploitation for offline multi-task MARL. The
task-specific skills represent the priors of each task and achieve a
task-guided fine-grained action execution. To verify the advancement of our
method, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After
training the policy using HiSSD on offline multi-task data, the empirical
results show that HiSSD assigns effective cooperative behaviors and obtains
superior performance in unseen tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of Superexpressive Networks in Implicit Neural
  Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uvini Balasuriya Mudiyanselage, Woojin Cho, Minju Jo, Noseong Park, Kookjin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we examine the potential of one of the ``superexpressive''
networks in the context of learning neural functions for representing complex
signals and performing machine learning downstream tasks. Our focus is on
evaluating their performance on computer vision and scientific machine learning
tasks including signal representation/inverse problems and solutions of partial
differential equations. Through an empirical investigation in various benchmark
tasks, we demonstrate that superexpressive networks, as proposed by [Zhang et
al. NeurIPS, 2022], which employ a specialized network structure characterized
by having an additional dimension, namely width, depth, and ``height'', can
surpass recent implicit neural representations that use highly-specialized
nonlinear activation functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 Workshop on Neural Network Weights as a New
  Data Modality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data Balancing and Ensemble Learning Approach for Credit Card Fraud
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces an innovative method for identifying credit card
fraud by combining the SMOTE-KMEANS technique with an ensemble machine learning
model. The proposed model was benchmarked against traditional models such as
logistic regression, decision trees, random forests, and support vector
machines. Performance was evaluated using metrics, including accuracy, recall,
and area under the curve (AUC). The results demonstrated that the proposed
model achieved superior performance, with an AUC of 0.96 when combined with the
SMOTE-KMEANS algorithm. This indicates a significant improvement in detecting
fraudulent transactions while maintaining high precision and recall. The study
also explores the application of different oversampling techniques to enhance
the performance of various classifiers. The findings suggest that the proposed
method is robust and effective for classification tasks on balanced datasets.
Future research directions include further optimization of the SMOTE-KMEANS
approach and its integration into existing fraud detection systems to enhance
financial security and consumer protection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Sardana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article surveys Evaluation models to automatically detect hallucinations
in Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark
of their performance across six RAG applications. Methods included in our study
include: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation
Model (HHEM), and the Trustworthy Language Model (TLM). These approaches are
all reference-free, requiring no ground-truth answers/labels to catch incorrect
LLM responses. Our study reveals that, across diverse RAG applications, some of
these approaches consistently detect incorrect RAG responses with high
precision/recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Domain-Specific Knowledge from LLMs into the Feature
  Engineering Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Eduardo Batista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature engineering is mandatory in the machine learning pipeline to obtain
robust models. While evolutionary computation is well-known for its great
results both in feature selection and feature construction, its methods are
computationally expensive due to the large number of evaluations required to
induce the final model. Part of the reason why these algorithms require a large
number of evaluations is their lack of domain-specific knowledge, resulting in
a lot of random guessing during evolution. In this work, we propose using Large
Language Models (LLMs) as an initial feature construction step to add knowledge
to the dataset. By doing so, our results show that the evolution can converge
faster, saving us computational resources. The proposed approach only provides
the names of the features in the dataset and the target objective to the LLM,
making it usable even when working with datasets containing private data. While
consistent improvements to test performance were only observed for one-third of
the datasets (CSS, PM, and IM10), possibly due to problems being easily
explored by LLMs, this approach only decreased the model performance in 1/77
test cases. Additionally, this work introduces the M6GP feature engineering
algorithm to symbolic regression, showing it can improve the results of the
random forest regressor and produce competitive results with its predecessor,
M3GP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoQa: Rethinking MoE Quantization with Multi-stage Data-model
  Distribution Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zheng, Xiuping Cui, Size Zheng, Maoliang Li, Jiayu Chen,  Yun,  Liang, Xiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advances in artificial intelligence, Mix-of-Experts (MoE) has become
the main form of Large Language Models (LLMs), and its demand for model
compression is increasing. Quantization is an effective method that not only
compresses the models but also significantly accelerates their performance.
Existing quantization methods have gradually shifted the focus from parameter
scaling to the analysis of data distributions. However, their analysis is
designed for dense LLMs and relies on the simple one-model-all-data mapping,
which is unsuitable for MoEs. This paper proposes a new quantization framework
called MoQa. MoQa decouples the data-model distribution complexity of MoEs in
multiple analysis stages, quantitively revealing the dynamics during sparse
data activation, data-parameter mapping, and inter-expert correlations. Based
on these, MoQa identifies particular experts' and parameters' significance with
optimal data-model distribution awareness and proposes a series of fine-grained
mix-quantization strategies adaptive to various data activation and expert
combination scenarios. Moreover, MoQa discusses the limitations of existing
quantization and analyzes the impact of each stage analysis, showing novel
insights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18
perplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy
improvement in zero-shot inference tasks. We believe MoQa will play a role in
future MoE construction, optimization, and compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Squared families: Searching beyond regular probability models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Russell Tsuchida, Jiawei Liu, Cheng Soon Ong, Dino Sejdinovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce squared families, which are families of probability densities
obtained by squaring a linear transformation of a statistic. Squared families
are singular, however their singularity can easily be handled so that they form
regular models. After handling the singularity, squared families possess many
convenient properties. Their Fisher information is a conformal transformation
of the Hessian metric induced from a Bregman generator. The Bregman generator
is the normalising constant, and yields a statistical divergence on the family.
The normalising constant admits a helpful parameter-integral factorisation,
meaning that only one parameter-independent integral needs to be computed for
all normalising constants in the family, unlike in exponential families.
Finally, the squared family kernel is the only integral that needs to be
computed for the Fisher information, statistical divergence and normalising
constant. We then describe how squared families are special in the broader
class of $g$-families, which are obtained by applying a sufficiently regular
function $g$ to a linear transformation of a statistic. After removing special
singularities, positively homogeneous families and exponential families are the
only $g$-families for which the Fisher information is a conformal
transformation of the Hessian metric, where the generator depends on the
parameter only through the normalising constant. Even-order monomial families
also admit parameter-integral factorisations, unlike exponential families. We
study parameter estimation and density estimation in squared families, in the
well-specified and misspecified settings. We use a universal approximation
property to show that squared families can learn sufficiently well-behaved
target densities at a rate of $\mathcal{O}(N^{-1/2})+C n^{-1/4}$, where $N$ is
the number of datapoints, $n$ is the number of parameters, and $C$ is some
constant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AugWard: Augmentation-Aware Representation Learning for Accurate Graph
  Classification <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjun Kim, Jaehyeon Choi, SeungJoo Lee, Jinhong Jung, U Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we accurately classify graphs? Graph classification is a pivotal task
in data mining with applications in social network analysis, web analysis, drug
discovery, molecular property prediction, etc. Graph neural networks have
achieved the state-of-the-art performance in graph classification, but they
consistently struggle with overfitting. To mitigate overfitting, researchers
have introduced various representation learning methods utilizing graph
augmentation. However, existing methods rely on simplistic use of graph
augmentation, which loses augmentation-induced differences and limits the
expressiveness of representations.
  In this paper, we propose AugWard (Augmentation-Aware Training with Graph
Distance and Consistency Regularization), a novel graph representation learning
framework that carefully considers the diversity introduced by graph
augmentation. AugWard applies augmentation-aware training to predict the graph
distance between the augmented graph and its original one, aligning the
representation difference directly with graph distance at both feature and
structure levels. Furthermore, AugWard employs consistency regularization to
encourage the classifier to handle richer representations. Experimental results
show that AugWard gives the state-of-the-art performance in supervised,
semi-supervised graph classification, and transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to PAKDD 2025 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Stein Discrepancy via Message-Passing Monte Carlo <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Kirk, T. Konstantin Rusch, Jakob Zech, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-Passing Monte Carlo (MPMC) was recently introduced as a novel
low-discrepancy sampling approach leveraging tools from geometric deep
learning. While originally designed for generating uniform point sets, we
extend this framework to sample from general multivariate probability
distributions with known probability density function. Our proposed method,
Stein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein
discrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC
outperforms competing methods, such as Stein Variational Gradient Descent and
(greedy) Stein Points, by achieving a lower Stein discrepancy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, Accepted at the ICLR 2025 Workshop on Frontiers
  in Probabilistic Inference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geographical hotspot prediction based on point cloud-voxel-community
  partition clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing solutions to the hotspot prediction problem in the field of
geographic information remain at a relatively preliminary stage. This study
presents a novel approach for detecting and predicting geographical hotspots,
utilizing point cloud-voxel-community partition clustering. By analyzing
high-dimensional data, we represent spatial information through point clouds,
which are then subdivided into multiple voxels to enhance analytical
efficiency. Our method identifies spatial voxels with similar characteristics
through community partitioning, thereby revealing underlying patterns in
hotspot distributions. Experimental results indicate that when applied to a
dataset of archaeological sites in Turkey, our approach achieves a 19.31%
increase in processing speed, with an accuracy loss of merely 6%, outperforming
traditional clustering methods. This method not only provides a fresh
perspective for hotspot prediction but also serves as an effective tool for
high-dimensional data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in
  Alignment with Bioinformaticians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Liu, Shuyu Chen, Ye Zhang, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in machine learning have significantly improved the
identification of disease-associated genes from gene expression datasets.
However, these processes often require extensive expertise and manual effort,
limiting their scalability. Large Language Model (LLM)-based agents have shown
promise in automating these tasks due to their increasing problem-solving
abilities. To support the evaluation and development of such methods, we
introduce GenoTEX, a benchmark dataset for the automated analysis of gene
expression data. GenoTEX provides annotated code and results for solving a wide
range of gene identification problems, encompassing dataset selection,
preprocessing, and statistical analysis, in a pipeline that follows
computational genomics standards. The benchmark includes expert-curated
annotations from bioinformaticians to ensure accuracy and reliability. To
provide baselines for these tasks, we present GenoAgent, a team of LLM-based
agents that adopt a multi-step programming workflow with flexible
self-correction, to collaboratively analyze gene expression datasets. Our
experiments demonstrate the potential of LLM-based methods in analyzing genomic
data, while error analysis highlights the challenges and areas for future
improvement. We propose GenoTEX as a promising resource for benchmarking and
enhancing automated methods for gene expression data analysis. The benchmark is
available at https://github.com/Liu-Hy/GenoTex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIA: Unified Spatiotemporal Video Adaptation Framework for Global and
  Local Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video editing serves as a fundamental pillar of digital media, spanning
applications in entertainment, education, and professional communication.
However, previous methods often overlook the necessity of comprehensively
understanding both global and local contexts, leading to inaccurate and
inconsistent edits in the spatiotemporal dimension, especially for long videos.
In this paper, we introduce VIA, a unified spatiotemporal Video Adaptation
framework for global and local video editing, pushing the limits of
consistently editing minute-long videos. First, to ensure local consistency
within individual frames, we designed test-time editing adaptation to adapt a
pre-trained image editing model for improving consistency between potential
editing directions and the text instruction, and adapts masked latent variables
for precise local control. Furthermore, to maintain global consistency over the
video sequence, we introduce spatiotemporal adaptation that recursively gather
consistent attention variables in key frames and strategically applies them
across the whole sequence to realize the editing effects. Extensive experiments
demonstrate that, compared to baseline methods, our VIA approach produces edits
that are more faithful to the source videos, more coherent in the
spatiotemporal context, and more precise in local control. More importantly, we
show that VIA can achieve consistent long video editing in minutes, unlocking
the potential for advanced video editing tasks over long video sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video
  Understanding? <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Awareness, the ability to reason dynamically based on the timestamp
when a question is raised, is the key distinction between offline and online
video LLMs. Unlike offline models, which rely on complete videos for static,
post hoc analysis, online models process video streams incrementally and
dynamically adapt their responses based on the timestamp at which the question
is posed. Despite its significance, temporal awareness has not been adequately
evaluated in existing benchmarks. To fill this gap, we present OVO-Bench
(Online-VideO-Benchmark), a novel video benchmark that emphasizes the
importance of timestamps for advanced online video understanding capability
benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and
respond to events occurring at specific timestamps under three distinct
scenarios: (1) Backward tracing: trace back to past events to answer the
question. (2) Real-time understanding: understand and respond to events as they
unfold at the current timestamp. (3) Forward active responding: delay the
response until sufficient future information becomes available to answer the
question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos
and approximately human-curated 2,800 fine-grained meta-annotations with
precise timestamps. We combine automated generation pipelines with human
curation. With these high-quality samples, we further developed an evaluation
pipeline to systematically query video LLMs along the video timeline.
Evaluations of nine Video-LLMs reveal that, despite advancements on traditional
benchmarks, current models struggle with online video understanding, showing a
significant gap compared to human agents. We hope OVO-Bench will drive progress
in video LLMs and inspire future research in online video reasoning. Our
benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-View and Multi-Scale Alignment for Contrastive Language-Image
  <span class="highlight-title">Pre-train</span>ing in Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, John Onofrey, Nicha C. Dvornek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential
in medical image analysis but requires substantial data and computational
resources. Due to these restrictions, existing CLIP applications in medical
imaging focus mainly on modalities like chest X-rays that have abundant
image-report data available, leaving many other important modalities
underexplored. Here, we propose one of the first adaptations of the full CLIP
model to mammography, which presents significant challenges due to labeled data
scarcity, high-resolution images with small regions of interest, and class-wise
imbalance. We first develop a specialized supervision framework for mammography
that leverages its multi-view nature. Furthermore, we design a symmetric local
alignment module to better focus on detailed features in high-resolution
images. Lastly, we incorporate a parameter-efficient fine-tuning approach for
large language models pre-trained with medical knowledge to address data
limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms
state-of-the-art baselines for three different tasks on two large real-world
mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared
with the largest baseline. The code is available at
https://github.com/XYPB/MaMA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IPMI 2025 for Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06608v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06608v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion techniques have propelled image and video
generation to unprecedented levels of quality, significantly accelerating the
deployment and application of generative AI. However, 3D shape generation
technology has so far lagged behind, constrained by limitations in 3D data
scale, complexity of 3D data processing, and insufficient exploration of
advanced techniques in the 3D domain. Current approaches to 3D shape generation
face substantial challenges in terms of output quality, generalization
capability, and alignment with input conditions. We present TripoSG, a new
streamlined shape diffusion paradigm capable of generating high-fidelity 3D
meshes with precise correspondence to input images. Specifically, we propose:
1) A large-scale rectified flow transformer for 3D shape generation, achieving
state-of-the-art fidelity through training on extensive, high-quality data. 2)
A hybrid supervised training strategy combining SDF, normal, and eikonal losses
for 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data
processing pipeline to generate 2 million high-quality 3D samples, highlighting
the crucial rules for data quality and quantity in training 3D generative
models. Through comprehensive experiments, we have validated the effectiveness
of each component in our new framework. The seamless integration of these parts
has enabled TripoSG to achieve state-of-the-art performance in 3D shape
generation. The resulting 3D shapes exhibit enhanced detail due to
high-resolution capabilities and demonstrate exceptional fidelity to input
images. Moreover, TripoSG demonstrates improved versatility in generating 3D
models from diverse image styles and contents, showcasing strong generalization
capabilities. To foster progress and innovation in the field of 3D generation,
we will make our model publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Orchestration for Large-Scale Inference on Heterogeneous
  Accelerator Systems Balancing Cost, Performance, and Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahav Biran, Imry Kissos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge in generative AI workloads has created a need for scalable
inference systems that can flexibly harness both GPUs and specialized
accelerators while containing operational costs. This paper proposes a
hardware-agnostic control loop that adaptively allocates requests across
heterogeneous accelerators based on real-time cost and capacity signals. The
approach sustains low latency and high throughput by dynamically shifting
between cost-optimized and capacity-optimized modes, ensuring the most
efficient use of expensive compute resources under fluctuating availability.
Evaluated using the Stable Diffusion model, the framework consistently meets
latency targets, automatically redirects traffic during capacity shortfalls,
and capitalizes on lower-cost accelerators when possible. These results
highlight how a feedback-driven deployment strategy, spanning the entire
software and hardware stack, can help organizations efficiently scale
generative AI workloads while maintaining resilience in the face of limited
accelerator capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time
  Series Forecasting <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Chae Hong, Bei Xiao, Yangho Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting has long been a focus of research across diverse
fields, including economics, energy, healthcare, and traffic management. Recent
works have introduced innovative architectures for time series models, such as
the Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs)
to enhance prediction accuracy by effectively capturing both spatial and
temporal dependencies within the data. In this paper, we investigate the
capabilities of the Kolmogorov-Arnold Networks (KANs) for time-series
forecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental
results demonstrate that TSKANMixer tends to improve prediction accuracy over
the original TSMixer across multiple datasets, ranking among the top-performing
models compared to other time series approaches. Our results show that the KANs
are promising alternatives to improve the performance of time series
forecasting by replacing or extending traditional MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 7 tables and accepted at the AI4TS: AI for Time
  Series Analysis workshop, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniBench: Towards The Future of Universal Omni-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have focused
on integrating multiple modalities, yet their ability to simultaneously process
and reason across different inputs remains underexplored. We introduce
OmniBench, a novel benchmark designed to evaluate models' ability to recognize,
interpret, and reason across visual, acoustic, and textual inputs
simultaneously. We define language models capable of such tri-modal processing
as omni-language models (OLMs). OmniBench features high-quality human
annotations that require integrated understanding across all modalities. Our
evaluation reveals that: i) open-source OLMs show significant limitations in
instruction-following and reasoning in tri-modal contexts; and ii) most
baseline models perform poorly (around 50% accuracy) even with textual
alternatives to image/audio inputs. To address these limitations, we develop
OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We
advocate for developing more robust tri-modal integration techniques and
training strategies to enhance OLM performance. Codes and data could be found
at our repo (https://github.com/multimodal-art-projection/OmniBench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of large language models (LLMs) in generation
tasks has enabled practitioners to leverage publicly available models to power
custom applications, such as chatbots and virtual assistants. However, the data
used to train or fine-tune these LLMs is often undisclosed, allowing an
attacker to compromise the data and inject backdoors into the models. In this
paper, we develop a novel inference time defense, named CLEANGEN, to mitigate
backdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and
effective decoding strategy that is compatible with the state-of-the-art (SOTA)
LLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored
LLMs assign significantly higher probabilities to tokens representing the
attacker-desired contents. These discrepancies in token probabilities enable
CLEANGEN to identify suspicious tokens favored by the attacker and replace them
with tokens generated by another LLM that is not compromised by the same
attacker, thereby avoiding generation of attacker-desired content. We evaluate
CLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN
achieves lower attack success rates (ASR) compared to five SOTA baseline
defenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN
maintain helpfulness in their responses when serving benign user queries with
minimal added computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is presented at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision language models are blind: Failing to translate detailed visual
  features into words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06581v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06581v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models with vision capabilities (VLMs), e.g., GPT-4o and
Gemini 1.5 Pro, score high on many vision-understanding benchmarks, they are
still struggling with low-level vision tasks that are easy to humans.
Specifically, on BlindTest, our suite of 7 very simple tasks, including
identifying (a) whether two circles overlap; (b) how many times two lines
intersect; (c) which letter is being circled in a word; and (d) the number of
circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%
accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,
far from the human expected accuracy of 100%. Across different image
resolutions and line widths, VLMs including slow-thinking models consistently
struggle with those tasks that require precise spatial information when
geometric primitives overlap or are close. Yet, VLMs perform at near-100%
accuracy when much more space is added to separate shapes and letters. Linear
probing experiments show that vision encoders contain sufficient visual
information to solve BlindTest and that language models fail to decode this
information into correct answers. Code and data are at:
https://vlmsareblind.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing LLM Character-Level Manipulation via Divide and Conquer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated strong generalization
capabilities across a wide range of natural language processing (NLP) tasks.
However, they exhibit notable weaknesses in character-level string
manipulation, struggling with fundamental operations such as character
deletion, insertion, and substitution. These challenges stem primarily from
tokenization constraints, despite the critical role of such operations in data
preprocessing and code generation. Through systematic analysis, we derive two
key insights: (1) LLMs face significant difficulties in leveraging intrinsic
token knowledge for character-level reasoning, and (2) atomized word structures
can substantially enhance LLMs' ability to process token-level structural
information. Building on these insights, we propose Character-Level
Manipulation via Divide and Conquer, a novel approach designed to bridge the
gap between token-level processing and character-level manipulation. Our method
decomposes complex operations into explicit character-level subtasks coupled
with controlled token reconstruction phases, leading to significant
improvements in accuracy. Without additional training, our method significantly
improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and
$\texttt{Substitution}$ tasks. To support further research, we open-source our
implementation and benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrastive Forward-Forward Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Chen, Dongshu Liu, Jeremie Laydevant, Julie Grollier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents that operate autonomously benefit from lifelong learning capabilities.
However, compatible training algorithms must comply with the decentralized
nature of these systems, which imposes constraints on both the parameter counts
and the computational resources. The Forward-Forward (FF) algorithm is one of
these. FF relies only on feedforward operations, the same used for inference,
for optimizing layer-wise objectives. This purely forward approach eliminates
the need for transpose operations required in traditional backpropagation.
Despite its potential, FF has failed to reach state-of-the-art performance on
most standard benchmark tasks, in part due to unreliable negative data
generation methods for unsupervised learning.
  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)
algorithm, a competitive training method aimed at closing this performance gap.
Inspired by standard self-supervised contrastive learning for vision tasks,
SCFF generates positive and negative inputs applicable across various datasets.
The method demonstrates superior performance compared to existing unsupervised
local learning algorithms on several benchmark datasets, including MNIST,
CIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training
recurrent neural networks, expanding its utility to sequential data tasks.
These findings pave the way for high-accuracy, real-time learning on
resource-constrained edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for
  Efficient Diffusion <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, Eli Shechtman, Sohrab Amirghodsi, Yingyan Celine Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image
generation quality but suffer from high latency and memory inefficiency, making
them difficult to deploy on resource-constrained devices. One major efficiency
bottleneck is that existing DiTs apply equal computation across all regions of
an image. However, not all image tokens are equally important, and certain
localized areas require more computation, such as objects. To address this, we
propose DiffCR, a dynamic DiT inference framework with differentiable
compression ratios, which automatically learns to dynamically route computation
across layers and timesteps for each image token, resulting in efficient DiTs.
Specifically, DiffCR integrates three features: (1) A token-level routing
scheme where each DiT layer includes a router that is fine-tuned jointly with
model weights to predict token importance scores. In this way, unimportant
tokens bypass the entire layer's computation; (2) A layer-wise differentiable
ratio mechanism where different DiT layers automatically learn varying
compression ratios from a zero initialization, resulting in large compression
ratios in redundant layers while others remain less compressed or even
uncompressed; (3) A timestep-wise differentiable ratio mechanism where each
denoising timestep learns its own compression ratio. The resulting pattern
shows higher ratios for noisier timesteps and lower ratios as the image becomes
clearer. Extensive experiments on text-to-image and inpainting tasks show that
DiffCR effectively captures dynamism across token, layer, and timestep axes,
achieving superior trade-offs between generation quality and efficiency
compared to prior works. The project website is available at
https://www.haoranyou.com/diffcr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15220v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15220v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in surgical computer vision applications have been driven
by vision-only models, which do not explicitly integrate the rich semantics of
language into their design. These methods rely on manually annotated surgical
videos to predict a fixed set of object categories, limiting their
generalizability to unseen surgical procedures and downstream tasks. In this
work, we put forward the idea that the surgical video lectures available
through open surgical e-learning platforms can provide effective vision and
language supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. Extensive
experiments across diverse surgical procedures and tasks demonstrate that the
multi-modal representations learned by SurgVLP exhibit strong transferability
and adaptability in surgical video analysis. Furthermore, our zero-shot
evaluations highlight SurgVLP's potential as a general-purpose foundation model
for surgical workflow analysis, reducing the reliance on extensive manual
annotations for downstream tasks, and facilitating adaptation methods such as
few-shot learning to build a scalable and data-efficient solution for various
downstream surgical applications. The [training
code](https://github.com/CAMMA-public/SurgVLP) and
[weights](https://github.com/CAMMA-public/PeskaVLP) are public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TREAD: Token Routing for Efficient Architecture-agnostic Diffusion
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as the mainstream approach for visual
generation. However, these models typically suffer from sample inefficiency and
high training costs. Consequently, methods for efficient finetuning, inference
and personalization were quickly adopted by the community. However, training
these models in the first place remains very costly. While several recent
approaches - including masking, distillation, and architectural modifications -
have been proposed to improve training efficiency, each of these methods comes
with a tradeoff: they achieve enhanced performance at the expense of increased
computational cost or vice versa. In contrast, this work aims to improve
training efficiency as well as generative performance at the same time through
routes that act as a transport mechanism for randomly selected tokens from
early layers to deeper layers of the model. Our method is not limited to the
common transformer-based model - it can also be applied to state-space models
and achieves this without architectural modifications or additional parameters.
Finally, we show that TREAD reduces computational cost and simultaneously
boosts model performance on the standard ImageNet-256 benchmark in
class-conditional synthesis. Both of these benefits multiply to a convergence
speedup of 14x at 400K training iterations compared to DiT and 37x compared to
the best benchmark performance of DiT at 7M training iterations. Furthermore,
we achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided
setting, which improves upon the DiT, without architectural changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Counterfactual Inference in Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Lally, Milad Kazemi, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a key limitation in existing counterfactual inference
methods for Markov Decision Processes (MDPs). Current approaches assume a
specific causal model to make counterfactuals identifiable. However, there are
usually many causal models that align with the observational and interventional
distributions of an MDP, each yielding different counterfactual distributions,
so fixing a particular causal model limits the validity (and usefulness) of
counterfactual inference. We propose a novel non-parametric approach that
computes tight bounds on counterfactual transition probabilities across all
compatible causal models. Unlike previous methods that require solving
prohibitively large optimisation problems (with variables that grow
exponentially in the size of the MDP), our approach provides closed-form
expressions for these bounds, making computation highly efficient and scalable
for non-trivial MDPs. Once such an interval counterfactual MDP is constructed,
our method identifies robust counterfactual policies that optimise the
worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate
our method on various case studies, demonstrating improved robustness over
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed typo in Equation (5)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Influence in Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Kazemi, Jessica Lally, Ekaterina Tishchenko, Hana Chockler, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work addresses a fundamental problem in the context of counterfactual
inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, this
kind of inference allows us to derive counterfactual paths $\tau'$ describing
what-if versions of $\tau$ obtained under different action sequences than those
observed in $\tau$. However, as the counterfactual states and actions deviate
from the observed ones over time, the observation $\tau$ may no longer
influence the counterfactual world, meaning that the analysis is no longer
tailored to the individual observation, resulting in interventional outcomes
rather than counterfactual ones. Even though this issue specifically affects
the popular Gumbel-max structural causal model used for MDP counterfactuals, it
has remained overlooked until now. In this work, we introduce a formal
characterisation of influence based on comparing counterfactual and
interventional distributions. We devise an algorithm to construct
counterfactual models that automatically satisfy influence constraints.
Leveraging such models, we derive counterfactual policies that are not just
optimal for a given reward structure but also remain tailored to the observed
path. Even though there is an unavoidable trade-off between policy optimality
and strength of influence constraints, our experiments demonstrate that it is
possible to derive (near-)optimal policies while remaining under the influence
of the observation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhening Liu, Yingdong Hu, Xinjie Zhang, Rui Song, Jiawei Shao, Zehong Lin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of 3D Gaussian Splatting (3DGS) has led to great
interest in 4D dynamic spatial reconstruction. Existing approaches mainly rely
on full-length multi-view videos, while there has been limited exploration of
online reconstruction methods that enable on-the-fly training and per-timestep
streaming. Current 3DGS-based streaming methods treat the Gaussian primitives
uniformly and constantly renew the densified Gaussians, thereby overlooking the
difference between dynamic and static features as well as neglecting the
temporal continuity in the scene. To address these limitations, we propose a
novel three-stage pipeline for iterative streamable 4D dynamic spatial
reconstruction. Our pipeline comprises a selective inheritance stage to
preserve temporal continuity, a dynamics-aware shift stage to distinguish
dynamic and static primitives and optimize their movements, and an error-guided
densification stage to accommodate emerging objects. Our method achieves
state-of-the-art performance in online 4D reconstruction, demonstrating the
fastest on-the-fly training, superior representation quality, and real-time
rendering capability. Project page: https://www.liuzhening.top/DASS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.liuzhening.top/DASS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiased Offline Representation Learning for Fast Online Adaptation in
  Non-stationary Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing policies that can adjust to non-stationary environments is
essential for real-world reinforcement learning applications. However, learning
such adaptable policies in offline settings, with only a limited set of
pre-collected trajectories, presents significant challenges. A key difficulty
arises because the limited offline data makes it hard for the context encoder
to differentiate between changes in the environment dynamics and shifts in the
behavior policy, often leading to context misassociations. To address this
issue, we introduce a novel approach called Debiased Offline Representation for
fast online Adaptation (DORA). DORA incorporates an information bottleneck
principle that maximizes mutual information between the dynamics encoding and
the environmental data, while minimizing mutual information between the
dynamics encoding and the actions of the behavior policy. We present a
practical implementation of DORA, leveraging tractable bounds of the
information bottleneck principle. Our experimental evaluation across six
benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only
achieves a more precise dynamics encoding but also significantly outperforms
existing baselines in terms of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Cut-informed Graph Embedding and Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, Wei Ju, Pengfei Wang, Yuanchun Zhou, Erik Cambria, Chong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering aims to divide the graph into different clusters. The
recently emerging deep graph clustering approaches are largely built on graph
neural networks (GNN). However, GNN is designed for general graph encoding and
there is a common issue of representation collapse in existing GNN-based deep
graph clustering algorithms. We attribute two main reasons for such issues: (i)
the inductive bias of GNN models: GNNs tend to generate similar representations
for proximal nodes. Since graphs often contain a non-negligible amount of
inter-cluster links, the bias results in error message passing and leads to
biased clustering; (ii) the clustering guided loss function: most traditional
approaches strive to make all samples closer to pre-learned cluster centers,
which causes a degenerate solution assigning all data points to a single label
thus make all samples and less discriminative. To address these challenges, we
investigate graph clustering from a graph cut perspective and propose an
innovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering
framework, namely DCGC. This framework includes two modules: (i) cut-informed
graph encoding; (ii) self-supervised graph clustering via optimal transport.
For the encoding module, we derive a cut-informed graph embedding objective to
fuse graph structure and attributes by minimizing their joint normalized cut.
For the clustering module, we utilize the optimal transport theory to obtain
the clustering assignments, which can balance the guidance of "proximity to the
pre-learned cluster center". With the above two tailored designs, DCGC is more
suitable for the graph clustering task, which can effectively alleviate the
problem of representation collapse and achieve better performance. We conduct
extensive experiments to demonstrate that our method is simple but effective
compared with benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Self-play Methods in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng Ye, Wenbo Ding, Yaodong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-play, characterized by agents' interactions with copies or past versions
of themselves, has recently gained prominence in reinforcement learning (RL).
This paper first clarifies the preliminaries of self-play, including the
multi-agent reinforcement learning framework and basic game theory concepts.
Then, it provides a unified framework and classifies existing self-play
algorithms within this framework. Moreover, the paper bridges the gap between
the algorithms and their practical implications by illustrating the role of
self-play in different scenarios. Finally, the survey highlights open
challenges and future research directions in self-play. This paper is an
essential guide map for understanding the multifaceted landscape of self-play
in RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Continual Adaptation of <span class="highlight-title">Pretrain</span>ed Robotic Policy with Online
  Meta-Learned Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Zhu, Endong Sun, Guanhe Huang, Oya Celiktutan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual adaptation is essential for general autonomous agents. For example,
a household robot pretrained with a repertoire of skills must still adapt to
unseen tasks specific to each household. Motivated by this, building upon
parameter-efficient fine-tuning in language models, prior works have explored
lightweight adapters to adapt pretrained policies, which can preserve learned
features from the pretraining phase and demonstrate good adaptation
performances. However, these approaches treat task learning separately,
limiting knowledge transfer between tasks. In this paper, we propose Online
Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can
facilitate knowledge transfer from previously learned tasks to current learning
tasks through a novel meta-learning objective. Extensive experiments in both
simulated and real-world environments demonstrate that OMLA can lead to better
adaptation performances compared to the baseline methods. The project link:
https://ricky-zhu.github.io/OMLA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project link: https://ricky-zhu.github.io/OMLA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ing with random noise for uncertainty calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghwan Cheon, Se-Bum Paik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty calibration is crucial for various machine learning applications,
yet it remains challenging. Many models exhibit hallucinations - confident yet
inaccurate responses - due to miscalibrated confidence. Here, we show that the
common practice of random initialization in deep learning, often considered a
standard technique, is an underlying cause of this miscalibration, leading to
excessively high confidence in untrained networks. Our method, inspired by
developmental neuroscience, addresses this issue by simply pretraining networks
with random noise and labels, reducing overconfidence and bringing initial
confidence levels closer to chance. This ensures optimal calibration, aligning
confidence with accuracy during subsequent data training, without the need for
additional pre- or post-processing. Pre-calibrated networks excel at
identifying "unknown data," showing low confidence for out-of-distribution
inputs, thereby resolving confidence miscalibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Motion Transfer with Diffusion <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DiTFlow, a method for transferring the motion of a reference video
to a newly synthesized one, designed specifically for Diffusion Transformers
(DiT). We first process the reference video with a pre-trained DiT to analyze
cross-frame attention maps and extract a patch-wise motion signal called the
Attention Motion Flow (AMF). We guide the latent denoising process in an
optimization-based, training-free, manner by optimizing latents with our AMF
loss to generate videos reproducing the motion of the reference one. We also
apply our optimization strategy to transformer positional embeddings, granting
us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow
against recently published methods, outperforming all across multiple metrics
and human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 - Project page: https://ditflow.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Video Tokenization: A Conditioned Diffusion-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video tokenizers typically use the traditional Variational
Autoencoder (VAE) architecture for video compression and reconstruction.
However, to achieve good performance, its training process often relies on
complex multi-stage training tricks that go beyond basic reconstruction loss
and KL regularization. Among these tricks, the most challenging is the precise
tuning of adversarial training with additional Generative Adversarial Networks
(GANs) in the final stage, which can hinder stable convergence. In contrast to
GANs, diffusion models offer more stable training processes and can generate
higher-quality results. Inspired by these advantages, we propose CDT, a novel
Conditioned Diffusion-based video Tokenizer, that replaces the GAN-based
decoder with a conditional causal diffusion model. The encoder compresses
spatio-temporal information into compact latents, while the decoder
reconstructs videos through a reverse diffusion process conditioned on these
latents. During inference, we incorporate a feature cache mechanism to generate
videos of arbitrary length while maintaining temporal continuity and adopt
sampling acceleration technique to enhance efficiency. Trained using only a
basic MSE diffusion loss for reconstruction, along with KL term and LPIPS
perceptual loss from scratch, extensive experiments demonstrate that CDT
achieves state-of-the-art performance in video reconstruction tasks with just a
single-step sampling. Even a scaled-down version of CDT (3$\times$ inference
speedup) still performs comparably with top baselines. Moreover, the latent
video generation model trained with CDT also exhibits superior performance. The
source code and pretrained weights are available at
https://github.com/ali-vilab/CDT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal-Guided Spiking Neural Networks for Event-Based Human Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the promising interplay between spiking neural networks
(SNNs) and event-based cameras for privacy-preserving human action recognition
(HAR). The unique feature of event cameras in capturing only the outlines of
motion, combined with SNNs' proficiency in processing spatiotemporal data
through spikes, establishes a highly synergistic compatibility for event-based
HAR. Previous studies, however, have been limited by SNNs' ability to process
long-term temporal information, essential for precise HAR. In this paper, we
introduce two novel frameworks to address this: temporal segment-based SNN
(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The
\textit{TS-SNN} extracts long-term temporal information by dividing actions
into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements
with 3D components to facilitate the transmission of temporal information. To
promote further research in event-based HAR, we create a dataset,
\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V
event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive
experimental results show that our proposed frameworks surpass state-of-the-art
SNN methods on our newly collected dataset and three other neuromorphic
datasets, showcasing their effectiveness in handling long-range temporal
information for event-based HAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online POMDP Planning with Anytime Deterministic Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Barenboim, Vadim Indelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making under uncertainty is a critical aspect of many practical
autonomous systems due to incomplete information. Partially Observable Markov
Decision Processes (POMDPs) offer a mathematically principled framework for
formulating decision-making problems under such conditions. However, finding an
optimal solution for a POMDP is generally intractable. In recent years, there
has been a significant progress of scaling approximate solvers from small to
moderately sized problems, using online tree search solvers. Often, such
approximate solvers are limited to probabilistic or asymptotic guarantees
towards the optimal solution. In this paper, we derive a deterministic
relationship for discrete POMDPs between an approximated and the optimal
solution. We show that at any time, we can derive bounds that relate between
the existing solution and the optimal one. We show that our derivations provide
an avenue for a new set of algorithms and can be attached to existing
algorithms that have a certain structure to provide them with deterministic
guarantees with marginal computational overhead. In return, not only do we
certify the solution quality, but we demonstrate that making a decision based
on the deterministic guarantee may result in superior performance compared to
the original algorithm without the deterministic certification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaceID-6M: A Large-Scale, Open-Source FaceID Customization <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Xiaoya Li, Jiwei Li, Guoyin Wang, Xiaofei Sun, Bob Zhu, Han Qiu, Mo Yu, Shengjie Shen, Tianwei Zhang, Eduard Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the data-driven nature of current face identity (FaceID) customization
methods, all state-of-the-art models rely on large-scale datasets containing
millions of high-quality text-image pairs for training. However, none of these
datasets are publicly available, which restricts transparency and hinders
further advancements in the field.
  To address this issue, in this paper, we collect and release FaceID-6M, the
first large-scale, open-source FaceID dataset containing 6 million high-quality
text-image pairs. Filtered from LAION-5B \cite{schuhmann2022laion}, FaceID-6M
undergoes a rigorous image and text filtering steps to ensure dataset quality,
including resolution filtering to maintain high-quality images and faces, face
filtering to remove images that lack human faces, and keyword-based strategy to
retain descriptions containing human-related terms (e.g., nationality,
professions and names). Through these cleaning processes, FaceID-6M provides a
high-quality dataset optimized for training powerful FaceID customization
models, facilitating advancements in the field by offering an open resource for
research and development.
  We conduct extensive experiments to show the effectiveness of our FaceID-6M,
demonstrating that models trained on our FaceID-6M dataset achieve performance
that is comparable to, and slightly better than currently available industrial
models. Additionally, to support and advance research in the FaceID
customization community, we make our code, datasets, and models fully publicly
available. Our codes, models, and datasets are available at:
https://github.com/ShuheSH/FaceID-6M.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2501.15407</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Starjob: <span class="highlight-title">Dataset</span> for LLM-Driven Job Shop Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities across
various domains, but their potential for solving combinatorial optimization
problems remains largely unexplored. In this paper, we investigate the
applicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic
challenge in combinatorial optimization that requires efficient job allocation
to machines to minimize makespan. To this end, we introduce Starjob, the first
supervised dataset for JSSP, comprising 130k instances specifically designed
for training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit
quantized model with the LoRA method to develop an end-to-end scheduling
approach. Our evaluation on standard benchmarks demonstrates that the proposed
LLM-based method not only surpasses traditional Priority Dispatching Rules
(PDRs) but also achieves notable improvements over state-of-the-art neural
approaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on
Taillard benchmarks. These results highlight the untapped potential of LLMs in
tackling combinatorial optimization problems, paving the way for future
advancements in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2408.06993</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Logic for Reasoning About Aggregate-Combine Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Nunn, Marco Sälzer, François Schwarzentruber, Nicolas Troquard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a modal logic in which counting modalities appear in linear
inequalities. We show that each formula can be transformed into an equivalent
graph neural network (GNN). We also show that a broad class of GNNs can be
transformed efficiently into a formula, thus significantly improving upon the
literature about the logical expressiveness of GNNs. We also show that the
satisfiability problem is PSPACE-complete. These results bring together the
promise of using standard logical methods for reasoning about GNNs and their
properties, particularly in applications such as GNN querying, equivalence
checking, etc. We prove that such natural problems can be solved in polynomial
space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2307.05150</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatically Adaptive Conformal Risk Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17819v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17819v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Blot, Anastasios N Angelopoulos, Michael I Jordan, Nicolas J-B Brunel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Science and technology have a growing need for effective mechanisms that
ensure reliable, controlled performance from black-box machine learning
algorithms. These performance guarantees should ideally hold conditionally on
the input-that is the performance guarantees should hold, at least
approximately, no matter what the input. However, beyond stylized discrete
groupings such as ethnicity and gender, the right notion of conditioning can be
difficult to define. For example, in problems such as image segmentation, we
want the uncertainty to reflect the intrinsic difficulty of the test sample,
but this may be difficult to capture via a conditioning event. Building on the
recent work of Gibbs et al. [2023], we propose a methodology for achieving
approximate conditional control of statistical risks-the expected value of loss
functions-by adapting to the difficulty of test samples. Our framework goes
beyond traditional conditional risk control based on user-provided conditioning
events to the algorithmic, data-driven determination of appropriate function
classes for conditioning. We apply this framework to various regression and
segmentation tasks, enabling finer-grained control over model performance and
demonstrating that by continuously monitoring and adjusting these parameters,
we can achieve superior precision compared to conventional risk-control
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of
  Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RGB-Th-Bench, the first benchmark designed to evaluate the
ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.
While VLMs have demonstrated remarkable progress in visual reasoning and
multimodal understanding, their evaluation has been predominantly limited to
RGB-based benchmarks, leaving a critical gap in assessing their capabilities in
infrared vision tasks. Existing visible-infrared datasets are either
task-specific or lack high-quality annotations necessary for rigorous model
evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive
evaluation framework covering 14 distinct skill dimensions, with a total of
1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy
metrics: a standard question-level accuracy and a stricter skill-level
accuracy, which evaluates model robustness across multiple questions within
each skill dimension. This design ensures a thorough assessment of model
performance, including resilience to adversarial and hallucinated responses. We
conduct extensive evaluations on 19 state-of-the-art VLMs, revealing
significant performance gaps in RGB-Thermal understanding. Our results show
that even the strongest models struggle with thermal image comprehension, with
performance heavily constrained by their RGB-based capabilities. Additionally,
the lack of large-scale application-specific and expert-annotated
thermal-caption-pair datasets in pre-training is an important reason of the
observed performance gap. RGB-Th-Bench highlights the urgent need for further
advancements in multimodal learning to bridge the gap between visible and
thermal image understanding. The dataset is available through this link, and
the evaluation code will also be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Bi-Elman Attention Networks: A Dual-Directional Context-Aware
  Test-Time Learning for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15469v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15469v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZhengLin Lai, MengYao Liao, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification, a fundamental task in natural language processing, aims
to categorize textual data into predefined labels. Traditional methods
struggled with complex linguistic structures and semantic dependencies.
However, the advent of deep learning, particularly recurrent neural networks
and Transformer-based models, has significantly advanced the field by enabling
nuanced feature extraction and context-aware predictions. Despite these
improvements, existing models still exhibit limitations in balancing
interpretability, computational efficiency, and long-range contextual
understanding. To address these challenges, this paper proposes the Dynamic
Bidirectional Elman with Attention Network (DBEAN). DBEAN integrates
bidirectional temporal modeling with self-attention mechanisms. It dynamically
assigns weights to critical segments of input, improving contextual
representation while maintaining computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATM: Improving Model Merging by Alternating Tuning and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, if the optimization is performed via gradient descent,
task vectors are after one step mathematically equivalent to the gradients
obtained via gradient descent in a multi-task setting, and still approximate
these gradients in subsequent epochs. Furthermore, we show that the
effectiveness of task vectors is largely driven by the first epoch's gradient.
Given this parallel between task vectors and gradients, we propose viewing
model merging as a single step in an iterative process that alternates between
tuning and merging (ATM). We then propose two ways to utilize ATM. The first is
to replace multi-task learning with ATM in scenarios where data sharing is
prohibited, such as federated learning. The second is to improve the outcome of
any model merging algorithm by applying a few post-hoc iterations of ATM on a
small validation dataset, which is commonly available for hyperparameter
tuning. Finally, we provide both empirical and theoretical support for the
effectiveness of ATM, demonstrating that it minimizes an upper bound on the
loss obtained by jointly finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 9 Pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and
  Wisdom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, Lingpeng Kong, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have witnessed significant progress on
visual understanding tasks. However, they often prioritize language knowledge
over image information on visual reasoning tasks, incurring performance
degradation. To tackle this issue, we first identify the drawbacks of existing
solutions (i.e., insufficient and irrelevant visual descriptions, and limited
multi-modal capacities). We then decompose visual reasoning process into two
stages: visual perception (i.e., eyesight) and textual reasoning (i.e.,
wisdom), and introduce a novel visual reasoning framework named ProReason. This
framework features multi-run proactive perception and decoupled
vision-reasoning capabilities. Briefly, given a multi-modal question, ProReason
iterates proactive information collection and reasoning until the answer can be
concluded with necessary and sufficient visual descriptions. Notably, the
disassociation of capabilities allows seamless integration of existing large
language models (LLMs) to compensate for the reasoning deficits of LVLMs. Our
extensive experiments demonstrate that ProReason outperforms both existing
multi-step reasoning frameworks and passive peer methods on a wide range of
benchmarks for both open-source and closed-source models. In addition, with the
assistance of LLMs, ProReason achieves a performance improvement of up to 15%
on MMMU benchmark. Our insights into existing solutions and the decoupled
perspective for feasible integration of LLMs illuminate future research on
visual reasoning techniques, especially LLM-assisted ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Training for De-biasing Text-to-Image Generation: Unlocking
  the Potential of Stable Diffusion <span class="chip">CVPR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Kim, Siwon Kim, Minjun Park, Rahim Entezari, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image models, such as Stable Diffusion, show
significant demographic biases. Existing de-biasing techniques rely heavily on
additional training, which imposes high computational costs and risks of
compromising core image generation functionality. This hinders them from being
widely adopted to real-world applications. In this paper, we explore Stable
Diffusion's overlooked potential to reduce bias without requiring additional
training. Through our analysis, we uncover that initial noises associated with
minority attributes form "minority regions" rather than scattered. We view
these "minority regions" as opportunities in SD to reduce bias. To unlock the
potential, we propose a novel de-biasing method called 'weak guidance,'
carefully designed to guide a random noise to the minority regions without
compromising semantic integrity. Through analysis and experiments on various
versions of SD, we demonstrate that our proposed approach effectively reduces
bias without additional training, achieving both efficiency and preservation of
core image generation functionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; First two authors contributed equally; Accepted at CVPR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on
  Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12767v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12767v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have combined Large Language Models (LLMs) with Knowledge
Graphs (KGs) to enhance reasoning, improving inference accuracy without
additional training while mitigating hallucination. However, existing
frameworks are often rigid, struggling to adapt to KG or task changes. They
also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.
To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that
separates reasoning into two roles: an Operator (a low-capacity LLM) that
gathers evidence and a Supervisor (a high-capacity LLM) that makes final
judgments. This design is cost-efficient for LLM inference while still
maintaining strong reasoning accuracy. Additionally, R2-KG employs an
Abstention mechanism, generating answers only when sufficient evidence is
collected from KG, which significantly enhances reliability. Experiments across
multiple KG-based reasoning tasks show that R2-KG consistently outperforms
baselines in both accuracy and reliability, regardless of the inherent
capability of LLMs used as the Operator. Further experiments reveal that the
single-agent version of R2-KG, equipped with a strict self-consistency
strategy, achieves significantly higher-than-baseline reliability while
reducing inference cost. However, it also leads to a higher abstention rate in
complex KGs. Our findings establish R2-KG as a flexible and cost-effective
solution for KG-based reasoning. It reduces reliance on high-capacity LLMs
while ensuring trustworthy inference. The code is available at
https://github.com/ekrxjwh2009/R2-KG/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive-Associative Meta-learning Pipeline with Human Cognitive
  Patterns for Unseen Drug-Target Interaction Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Lian, Jie Zhu, Tianxu Lv, Shiyun Nie, Hang Fan, Guosheng Wu, Yunjun Ge, Lihua Li, Xiangxiang Zeng, Xiang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant differences in protein structures hinder the generalization of
existing drug-target interaction (DTI) models, which often rely heavily on
pre-learned binding principles or detailed annotations. In contrast, BioBridge
designs an Inductive-Associative pipeline inspired by the workflow of
scientists who base their accumulated expertise on drawing insights into novel
drug-target pairs from weakly related references. BioBridge predicts novel
drug-target interactions using limited sequence data, incorporating multi-level
encoders with adversarial training to accumulate transferable binding
principles. On these principles basis, BioBridge employs a dynamic prototype
meta-learning framework to associate insights from weakly related annotations,
enabling robust predictions for previously unseen drug-target pairs. Extensive
experiments demonstrate that BioBridge surpasses existing models, especially
for unseen proteins. Notably, when only homologous protein binding data is
available, BioBridge proves effective for virtual screening of the epidermal
growth factor receptor and adenosine receptor, underscoring its potential in
drug discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context
  Learning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyundong Cho, Karishma Sharma, Nicolaas Jedema, Leonardo F. R. Ribeiro, Alessandro Moschitti, Ravi Krishnan, Jonathan May
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are aligned to the collective voice of many, resulting in
generic outputs that do not align with specific users' styles. In this work, we
present Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free method
that personalizes language models for text generation tasks with fewer than 10
examples per user. TICL iteratively expands an in-context learning prompt via a
trial-error-explain process, adding model-generated negative samples and
explanations that provide fine-grained guidance towards a specific user's
style. TICL achieves favorable win rates on pairwise comparisons with
LLM-as-a-judge up to 91.5% against the previous state-of-the-art and
outperforms competitive tuning-free baselines for personalized alignment tasks
of writing emails, essays and news articles. Both lexical and qualitative
analyses show that the negative samples and explanations enable language models
to learn stylistic context more effectively and overcome the bias towards
structural and formal phrases observed in their zero-shot outputs. By
front-loading inference compute to create a user-specific in-context learning
prompt that does not require extra generation steps at test time, TICL presents
a novel yet simple approach for personalized alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for
  Mental Health Prediction via Online Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Patil, Amardeep Kour Gedhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated potential in predicting mental
health outcomes from online text, yet traditional classification methods often
lack interpretability and robustness. This study evaluates structured reasoning
techniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and
Tree-of-Thought (ToT)-to improve classification accuracy across multiple mental
health datasets sourced from Reddit. We analyze reasoning-driven prompting
strategies, including Zero-shot CoT and Few-shot CoT, using key performance
metrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our
findings indicate that reasoning-enhanced techniques improve classification
performance over direct prediction, particularly in complex cases. Compared to
baselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained
transformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs
such as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable
gains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) and
SDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines in
Depression Severity, and CSSRS predictions suggest dataset-specific
limitations, likely due to our using a more extensive test set. Among prompting
strategies, Few-shot CoT consistently outperforms others, reinforcing the
effectiveness of reasoning-driven LLMs. Nonetheless, dataset variability
highlights challenges in model reliability and interpretability. This study
provides a comprehensive benchmark of reasoning-based LLM techniques for mental
health text classification. It offers insights into their potential for
scalable clinical applications while identifying key challenges for future
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 Figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs
  Better Solvers for Math Word Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14963v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14963v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting has enhanced the performance of Large
Language Models (LLMs) across various reasoning tasks. However, CoT still falls
short in dealing with complex math word problems, as it usually suffers from
three pitfalls: semantic misunderstanding errors, calculation errors, and
step-missing errors. Prior studies involve addressing the calculation errors
and step-missing errors, but neglect the semantic misunderstanding errors,
which is the major factor limiting the reasoning performance of LLMs. To this
end, we propose a simple-yet-effective method, namely Deeply Understanding the
Problems (DUP), to improve the LLMs' math problem-solving ability by addressing
semantic misunderstanding errors. The core of our method is to encourage the
LLMs to deeply understand the problems and extract the key problem-solving
information used for better reasoning. Extensive experiments on 10 diverse
reasoning benchmarks show that our DUP method consistently outperforms the
other counterparts by a large margin. More encouragingly, DUP achieves a new
SOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the
zero-shot setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: { 10.1007/s11704-025-41102-z }</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time and Memory Trade-off of KV-Cache Compression in Tensor <span class="highlight-title">Transformer</span>
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key-value (KV) cache in the tensor version of transformers presents a
significant bottleneck during inference. While previous work analyzes the
fundamental space complexity barriers in standard attention mechanisms [Haris
and Onak, 2025], our work generalizes the space complexity barriers result to
tensor attention version. Our theoretical contributions rely on a reduction
from communication complexity and deduce the memory lower bound for
tensor-structured attention mechanisms when $d = \Omega(\log n)$. Furthermore,
we introduce two types of tensor attention cache and present a trade-off
between time and memory for two scenarios. Overall, our work provides a
theoretical foundation for us to understand the time-memory tradeoff of
KV-Cache compression in tensor attention decoding and offers more perspectives
in developing more memory-efficient tensor attention Transformer architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Language-Specific LLMs to a Reasoning Model in One Day via
  Model Merging -- An Open Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates data selection and model merging methodologies aimed
at incorporating advanced reasoning capabilities such as those of DeepSeek R1
into language-specific large language models (LLMs), with a particular focus on
the Thai LLM. Our goal is to enhance the reasoning capabilities of
language-specific LLMs while maintaining their target language abilities.
DeepSeek R1 excels in reasoning but primarily benefits high-resource languages
such as English and Chinese. However, low-resource languages remain underserved
due to the dominance of English-centric training data and model optimizations,
which limit performance in these languages. This limitation results in
unreliable code-switching and diminished effectiveness on tasks in low-resource
languages. Meanwhile, local and regional LLM initiatives have attempted to
bridge this gap by developing language-specific LLMs that focus on improving
local linguistic fidelity. We demonstrate that, with only publicly available
datasets and a computational budget of $120, it is possible to enhance the
reasoning capabilities of language-specific LLMs to match the level of DeepSeek
R1, without compromising their performance on target language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typhoon T1: An Open Thai Reasoning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Typhoon T1, an open effort to develop an open Thai
reasoning model. A reasoning model is a relatively new type of generative model
built on top of large language models (LLMs). A reasoning model generates a
long chain of thought before arriving at a final answer, an approach found to
improve performance on complex tasks. However, details on developing such a
model are limited, especially for reasoning models that can generate traces in
a low-resource language. Typhoon T1 presents an open effort that dives into the
details of developing a reasoning model in a more cost-effective way by
leveraging supervised fine-tuning using open datasets, instead of reinforcement
learning. This paper shares the details about synthetic data generation and
training, as well as our dataset and model weights. Additionally, we provide
insights gained from developing a reasoning model that generalizes across
domains and is capable of generating reasoning traces in a low-resource
language, using Thai as an example. We hope this open effort provides a
foundation for further research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment
  of Rheumatoid Arthritis with <span class="highlight-title">Dataset</span> based Traditional Chinese Medicine <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, Yong Liang, Liang Liu, Hudan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) primarily trained on English texts, often face
biases and inaccuracies in Chinese contexts. Their limitations are pronounced
in fields like Traditional Chinese Medicine (TCM), where cultural and clinical
subtleties are vital, further hindered by a lack of domain-specific data, such
as rheumatoid arthritis (RA). To address these issues, this paper introduces
Hengqin-RA-v1, the first large language model specifically tailored for TCM
with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a
comprehensive RA-specific dataset curated from ancient Chinese medical
literature, classical texts, and modern clinical studies. This dataset empowers
Hengqin-RA-v1 to deliver accurate and culturally informed responses,
effectively bridging the gaps left by general-purpose models. Extensive
experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,
even surpassing the diagnostic accuracy of TCM practitioners in certain cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, AAAI-2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Reasoning Emission Estimation Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate greenhouse gas (GHG) emission reporting is critical for governments,
businesses, and investors. However, adoption remains limited particularly among
small and medium enterprises due to high implementation costs, fragmented
emission factor databases, and a lack of robust sector classification methods.
To address these challenges, we introduce Group Reasoning Emission Estimation
Networks (GREEN), an AI-driven carbon accounting framework that standardizes
enterprise-level emission estimation, constructs a large-scale benchmark
dataset, and leverages a novel reasoning approach with large language models
(LLMs). Specifically, we compile textual descriptions for 20,850 companies with
validated North American Industry Classification System (NAICS) labels and
align these with an economic model of carbon intensity factors. By reframing
sector classification as an information retrieval task, we fine-tune
Sentence-BERT models using a contrastive learning loss. To overcome the
limitations of single-stage models in handling thousands of hierarchical
categories, we propose a Group Reasoning method that ensembles LLM classifiers
based on the natural NAICS ontology, decomposing the task into multiple
sub-classification steps. We theoretically prove that this approach reduces
classification uncertainty and computational complexity. Experiments on 1,114
NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%
Top-10 accuracy), and case studies on 20 companies report a mean absolute
percentage error (MAPE) of 45.88%. The project is available at:
https://huggingface.co/datasets/Yvnminc/ExioNAICS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D
automated breast ultrasound (ABUS) is crucial for clinical diagnosis and
treatment planning. Therefore, developing an automated system for nodule
segmentation can enhance user independence and expedite clinical analysis.
Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can
streamline the laborious and intricate annotation process. However, current WSS
methods face challenges in achieving precise nodule segmentation, as many of
them depend on inaccurate activation maps or inefficient pseudo-mask generation
algorithms. In this study, we introduce a novel multi-agent reinforcement
learning-based WSS framework called Flip Learning, which relies solely on 2D/3D
boxes for accurate segmentation. Specifically, multiple agents are employed to
erase the target from the box to facilitate classification tag flipping, with
the erased region serving as the predicted segmentation mask. The key
contributions of this research are as follows: (1) Adoption of a
superpixel/supervoxel-based approach to encode the standardized environment,
capturing boundary priors and expediting the learning process. (2) Introduction
of three meticulously designed rewards, comprising a classification score
reward and two intensity distribution rewards, to steer the agents' erasing
process precisely, thereby avoiding both under- and over-segmentation. (3)
Implementation of a progressive curriculum learning strategy to enable agents
to interact with the environment in a progressively challenging manner, thereby
enhancing learning efficiency. Extensively validated on the large in-house BUS
and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS
methods and foundation models, and achieves comparable performance as
fully-supervised learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis. 24 pages, 13 figures, 20 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,
exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating
reasoning with external search processes remains challenging, especially for
complex multi-hop questions requiring multiple retrieval steps. We propose
ReSearch, a novel framework that trains LLMs to Reason with Search via
reinforcement learning without using any supervised data on reasoning steps.
Our approach treats search operations as integral components of the reasoning
chain, where when and how to perform searches is guided by text-based thinking,
and search results subsequently influence further reasoning. We train ReSearch
on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct
extensive experiments. Despite being trained on only one dataset, our models
demonstrate strong generalizability across various benchmarks. Analysis reveals
that ReSearch naturally elicits advanced reasoning capabilities such as
reflection and self-correction during the reinforcement learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Feature Circuits: Discovering and Editing Interpretable Causal
  Graphs in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce methods for discovering and applying sparse feature circuits.
These are causally implicated subnetworks of human-interpretable features for
explaining language model behaviors. Circuits identified in prior work consist
of polysemantic and difficult-to-interpret units like attention heads or
neurons, rendering them unsuitable for many downstream applications. In
contrast, sparse feature circuits enable detailed understanding of
unanticipated mechanisms. Because they are based on fine-grained units, sparse
feature circuits are useful for downstream tasks: We introduce SHIFT, where we
improve the generalization of a classifier by ablating features that a human
judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised
and scalable interpretability pipeline by discovering thousands of sparse
feature circuits for automatically discovered model behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data at https://github.com/saprmarks/feature-circuits.
  Demonstration at https://feature-circuits.xyz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OODFace: Benchmarking Robustness of Face Recognition under Common
  Corruptions and Appearance Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caixin Kang, Yubo Chen, Shouwei Ruan, Shiji Zhao, Ruochen Zhang, Jiayi Wang, Shan Fu, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of deep learning, facial recognition technology has seen
extensive research and rapid development. Although facial recognition is
considered a mature technology, we find that existing open-source models and
commercial algorithms lack robustness in certain complex Out-of-Distribution
(OOD) scenarios, raising concerns about the reliability of these systems. In
this paper, we introduce OODFace, which explores the OOD challenges faced by
facial recognition models from two perspectives: common corruptions and
appearance variations. We systematically design 30 OOD scenarios across 9 major
categories tailored for facial recognition. By simulating these challenges on
public datasets, we establish three robustness benchmarks: LFW-C/V, CFP-FP-C/V,
and YTF-C/V. We then conduct extensive experiments on 19 facial recognition
models and 3 commercial APIs, along with extended physical experiments on face
masks to assess their robustness. Next, we explore potential solutions from two
perspectives: defense strategies and Vision-Language Models (VLMs). Based on
the results, we draw several key insights, highlighting the vulnerability of
facial recognition systems to OOD data and suggesting possible solutions.
Additionally, we offer a unified toolkit that includes all corruption and
variation types, easily extendable to other datasets. We hope that our
benchmarks and findings can provide guidance for future improvements in facial
recognition model robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DefectFill: Realistic Defect Generation with Inpainting Diffusion Model
  for Visual Inspection <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi, Eunji Kim, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing effective visual inspection models remains challenging due to the
scarcity of defect data. While image generation models have been used to
synthesize defect images, producing highly realistic defects remains difficult.
We propose DefectFill, a novel method for realistic defect generation that
requires only a few reference defect images. It leverages a fine-tuned
inpainting diffusion model, optimized with our custom loss functions
incorporating defect, object, and attention terms. It enables precise capture
of detailed, localized defect features and their seamless integration into
defect-free objects. Additionally, our Low-Fidelity Selection method further
enhances the defect sample quality. Experiments show that DefectFill generates
high-quality defect images, enabling visual inspection models to achieve
state-of-the-art performance on the MVTec AD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoReVQA: Exploring Modular Reasoning Models for Video Question Answering <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the task of video question answering (videoQA) via a
decomposed multi-stage, modular reasoning framework. Previous modular methods
have shown promise with a single planning stage ungrounded in visual content.
However, through a simple and effective baseline, we find that such systems can
lead to brittle behavior in practice for challenging videoQA settings. Thus,
unlike traditional single-stage planning methods, we propose a multi-stage
system consisting of an event parser, a grounding stage, and a final reasoning
stage in conjunction with an external memory. All stages are training-free, and
performed using few-shot prompting of large models, creating interpretable
intermediate outputs at each stage. By decomposing the underlying planning and
task complexity, our method, MoReVQA, improves over prior work on standard
videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with
state-of-the-art results, and extensions to related tasks (grounded videoQA,
paragraph captioning).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; updated NExT-GQA results in Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17125v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17125v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Kim, Seung-Woo Seo, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) has demonstrated strong performance in
robotic control but remains susceptible to out-of-distribution (OOD) states,
often resulting in unreliable actions and task failure. While previous methods
have focused on minimizing or preventing OOD occurrences, they largely neglect
recovery once an agent encounters such states. Although the latest research has
attempted to address this by guiding agents back to in-distribution states,
their reliance on uncertainty estimation hinders scalability in complex
environments. To overcome this limitation, we introduce Language Models for
Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without
relying on uncertainty estimation. LaMOuR generates dense reward codes that
guide the agent back to a state where it can successfully perform its original
task, leveraging the capabilities of LVLMs in image description, logical
reasoning, and code generation. Experimental results show that LaMOuR
substantially enhances recovery efficiency across diverse locomotion tasks and
even generalizes effectively to complex environments, including humanoid
locomotion and mobile manipulation, where existing methods struggle. The code
and supplementary materials are available at https://lamour-rl.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under security review and will be re-released
  once the review is complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iTool: Boosting Tool Use of Large Language Models via Iterative
  Reinforced Fine-Tuning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external tools is known as a
promising approach to enhancing their capabilities, especially for complex
tasks. Synthesizing tool-use data through real-world simulations is an
effective way to achieve it. Nevertheless, our investigation reveals that (1)
training gains significantly decay as synthetic data increases. The model
struggles to benefit from more synthetic data due to potential data diversity
issues, resulting in poor performance in complex scenarios. Moreover, we find
that (2) this challenge primarily manifests as minor discrepancies between the
model's output and the ground truth response (termed as deficiency), such as
errors in parameter values that require complex reasoning from the context to
resolve. To this end, we propose an iterative reinforced fine-tuning strategy
designed to alleviate these challenges. This strategy involves: (1) enhancing
the diversity of synthetic data through path exploration of Monte Carlo Tree
Search. (2) iteratively identifying deficiency-related data, constructing
fine-grained preference pairs to pinpoint deficiencies, and then applying
preference optimization to optimize these deficiencies. Our experiments show
that models trained using our method achieve about 12\% better performance than
baseline models, outperforming larger open-source and closed-source models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review ACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Controllable Speech Synthesis in the Era of Large Language
  Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-speech (TTS), also known as speech synthesis, is a prominent research
area that aims to generate natural-sounding human speech from text. Recently,
with the increasing industrial demand, TTS technologies have evolved beyond
synthesizing human-like speech to enabling controllable speech generation. This
includes fine-grained control over various attributes of synthesized speech
such as emotion, prosody, timbre, and duration. In addition, advancements in
deep learning, such as diffusion and large language models, have significantly
enhanced controllable TTS over the past several years. In this work, we conduct
a comprehensive survey of controllable TTS, covering approaches ranging from
basic control techniques to methods utilizing natural language prompts, aiming
to provide a clear understanding of the current state of research. We examine
the general controllable TTS pipeline, challenges, model architectures, and
control strategies, offering a comprehensive and clear taxonomy of existing
methods. Additionally, we provide a detailed summary of datasets and evaluation
metrics and shed some light on the applications and future directions of
controllable TTS. To the best of our knowledge, this survey paper provides the
first comprehensive review of emerging controllable TTS methods, which can
serve as a beneficial resource for both academic researchers and industrial
practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6
  figures, 317 references. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reasoning abilities play a crucial role in understanding complex
multimodal data, advancing both domain-specific applications and artificial
general intelligence (AGI). Existing methods improve VLM reasoning via
Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated
training data to enhance visual reasoning capabilities. However, this training
paradigm may lead to overfitting and cognitive rigidity, restricting the
model's ability to transfer visual reasoning skills across domains and limiting
its real-world applicability. To address these limitations, we propose
Reason-RFT, a novel reinforcement fine-tuning framework that significantly
enhances generalization capabilities in visual reasoning tasks. Reason-RFT
introduces a two-phase training framework for visual reasoning: (1) Supervised
Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the
reasoning potential of Vision-Language Models (VLMs), followed by (2) Group
Relative Policy Optimization (GRPO)-based reinforcement learning that generates
multiple reasoning-response pairs, significantly enhancing generalization in
visual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,
we reconstructed a comprehensive dataset spanning visual counting, structure
perception, and spatial transformation. Experimental results demonstrate
Reasoning-RFT's three key advantages: (1) Performance Enhancement: achieving
state-of-the-art results across multiple tasks, outperforming most mainstream
open-source and proprietary models; (2) Generalization Superiority:
consistently maintaining robust performance across diverse tasks and domains,
outperforming alternative training paradigms; (3) Data Efficiency: excelling in
few-shot learning scenarios while surpassing full-dataset SFT baselines.
Project website: https://tanhuajie.github.io/ReasonRFT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GR00T N1: An Open Foundation Model for Generalist Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         NVIDIA,  :, Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi "Jim" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose robots need a versatile body and an intelligent mind. Recent
advancements in humanoid robots have shown great promise as a hardware platform
for building generalist autonomy in the human world. A robot foundation model,
trained on massive and diverse data sources, is essential for enabling the
robots to reason about novel situations, robustly handle real-world
variability, and rapidly learn new tasks. To this end, we introduce GR00T N1,
an open foundation model for humanoid robots. GR00T N1 is a
Vision-Language-Action (VLA) model with a dual-system architecture. The
vision-language module (System 2) interprets the environment through vision and
language instructions. The subsequent diffusion transformer module (System 1)
generates fluid motor actions in real time. Both modules are tightly coupled
and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture
of real-robot trajectories, human videos, and synthetically generated datasets.
We show that our generalist robot model GR00T N1 outperforms the
state-of-the-art imitation learning baselines on standard simulation benchmarks
across multiple robot embodiments. Furthermore, we deploy our model on the
Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation
tasks, achieving strong performance with high data efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are listed alphabetically. Project leads are Linxi "Jim" Fan
  and Yuke Zhu. For more information, see
  https://developer.nvidia.com/isaac/gr00t</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Self-Learning Enhanced Music Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Sun, Xulong Zhang, Monan Zhou, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music emotion recognition (MER) aims to identify the emotions conveyed in a
given musical piece. However, currently, in the field of MER, the available
public datasets have limited sample sizes. Recently, segment-based methods for
emotion-related tasks have been proposed, which train backbone networks on
shorter segments instead of entire audio clips, thereby naturally augmenting
training samples without requiring additional resources. Then, the predicted
segment-level results are aggregated to obtain the entire song prediction. The
most commonly used method is that the segment inherits the label of the clip
containing it, but music emotion is not constant during the whole clip. Doing
so will introduce label noise and make the training easy to overfit. To handle
the noisy label issue, we propose a semi-supervised self-learning (SSSL)
method, which can differentiate between samples with correct and incorrect
labels in a self-learning manner, thus effectively utilizing the augmented
segment-level data. Experiments on three public emotional datasets demonstrate
that the proposed method can achieve better or comparable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyBimanual: Transferring Unimanual Policy for General Bimanual
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxing Lu, Tengbo Yu, Haoyuan Deng, Season Si Chen, Yansong Tang, Ziwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing general language-conditioned bimanual manipulation tasks is of
great importance for many applications ranging from household service to
industrial assembly. However, collecting bimanual manipulation data is
expensive due to the high-dimensional action space, which poses challenges for
conventional methods to handle general bimanual manipulation tasks. In
contrast, unimanual policy has recently demonstrated impressive
generalizability across a wide range of tasks because of scaled model
parameters and training data, which can provide sharable manipulation knowledge
for bimanual systems. To this end, we propose a plug-and-play method named
AnyBimanual, which transfers pre-trained unimanual policy to general bimanual
manipulation policy with few bimanual demonstrations. Specifically, we first
introduce a skill manager to dynamically schedule the skill representations
discovered from pre-trained unimanual policy for bimanual manipulation tasks,
which linearly combines skill primitives with task-oriented compensation to
represent the bimanual manipulation instruction. To mitigate the observation
discrepancy between unimanual and bimanual systems, we present a visual aligner
to generate soft masks for visual embedding of the workspace, which aims to
align visual input of unimanual policy model for each arm with those during
pretraining stage. AnyBimanual shows superiority on 12 simulated tasks from
RLBench2 with a sizable 12.67% improvement in success rate over previous
methods. Experiments on 9 real-world tasks further verify its practicality with
an average success rate of 84.62%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://anybimanual.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Holistic Evaluation of Piano Sound Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monan Zhou, Shangda Wu, Shaohua Ji, Zijin Li, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to develop a holistic evaluation method for piano sound
quality to assist in purchasing decisions. Unlike previous studies that focused
on the effect of piano performance techniques on sound quality, this study
evaluates the inherent sound quality of different pianos. To derive quality
evaluation systems, the study uses subjective questionnaires based on a piano
sound quality dataset. The method selects the optimal piano classification
models by comparing the fine-tuning results of different pre-training models of
Convolutional Neural Networks (CNN). To improve the interpretability of the
models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The
results reveal that musically trained individuals are better able to
distinguish between the sound quality differences of different pianos. The best
fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the
piano classifier. However, the dataset is limited, and the audio is sliced to
increase its quantity, resulting in a lack of diversity and balance, so we use
focal loss to reduce the impact of data imbalance. To optimize the method, the
dataset will be expanded, or few-shot learning techniques will be employed in
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VERA: Explainable Video Anomaly Detection via Verbalized Learning of
  Vision-Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muchao Ye, Weiyang Liu, Pan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of vision-language models (VLMs) has established a new
paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously
detect anomalies and provide comprehendible explanations for the decisions.
Existing work in this direction often assumes the complex reasoning required
for VAD exceeds the capabilities of pretrained VLMs. Consequently, these
approaches either incorporate specialized reasoning modules during inference or
rely on instruction tuning datasets through additional training to adapt VLMs
for VAD. However, such strategies often incur substantial computational costs
or data annotation overhead. To address these challenges in explainable VAD, we
introduce a verbalized learning framework named VERA that enables VLMs to
perform VAD without model parameter modifications. Specifically, VERA
automatically decomposes the complex reasoning required for VAD into
reflections on simpler, more focused guiding questions capturing distinct
abnormal patterns. It treats these reflective questions as learnable parameters
and optimizes them through data-driven verbal interactions between learner and
optimizer VLMs, using coarsely labeled training data. During inference, VERA
embeds the learned questions into model prompts to guide VLMs in generating
segment-level anomaly scores, which are then refined into frame-level scores
via the fusion of scene and temporal contexts. Experimental results on
challenging benchmarks demonstrate that the learned questions of VERA are
highly adaptable, significantly improving both detection performance and
explainability of VLMs for VAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object
  Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkun He, Yun Liu, Ruitao Liu, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic human-object interaction motions is a critical problem
in VR/AR and human animation. Unlike the commonly studied scenarios involving a
single human or hand interacting with one object, we address a more generic
multi-body setting with arbitrary numbers of humans, hands, and objects. This
complexity introduces significant challenges in synchronizing motions due to
the high correlations and mutual influences among bodies. To address these
challenges, we introduce SyncDiff, a novel method for multi-body interaction
synthesis using a synchronized motion diffusion strategy. SyncDiff employs a
single diffusion model to capture the joint distribution of multi-body motions.
To enhance motion fidelity, we propose a frequency-domain motion decomposition
scheme. Additionally, we introduce a new set of alignment scores to emphasize
the synchronization of different body motions. SyncDiff jointly optimizes both
data sample likelihood and alignment likelihood through an explicit
synchronization strategy. Extensive experiments across four datasets with
various multi-body configurations demonstrate the superiority of SyncDiff over
existing state-of-the-art motion synthesis methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation-Driven Development of LLM Agents: A Process Model and
  Reference Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Dehai Zhao, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have enabled the emergence of LLM agents:
autonomous systems capable of achieving under-specified goals and adapting
post-deployment, often without explicit code or model changes. Evaluating these
agents is critical to ensuring their performance and safety, especially given
their dynamic, probabilistic, and evolving nature. However, traditional
approaches such as predefined test cases and standard redevelopment pipelines
struggle to address the unique challenges of LLM agent evaluation. These
challenges include capturing open-ended behaviors, handling emergent outcomes,
and enabling continuous adaptation over the agent's lifecycle. To address these
issues, we propose an evaluation-driven development approach, inspired by
test-driven and behavior-driven development but reimagined for the unique
characteristics of LLM agents. Through a multivocal literature review (MLR), we
synthesize the limitations of existing LLM evaluation methods and introduce a
novel process model and reference architecture tailored for evaluation-driven
development of LLM agents. Our approach integrates online (runtime) and offline
(redevelopment) evaluations, enabling adaptive runtime adjustments and
systematic iterative refinement of pipelines, artifacts, system architecture,
and LLMs themselves. By continuously incorporating evaluation results,
including fine-grained feedback from human and AI evaluators, into each stage
of development and operation, this framework ensures that LLM agents remain
aligned with evolving goals, user needs, and governance standards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSEAttention is All You Need for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23749v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23749v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dizhen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based architectures have achieved remarkable success in natural
language processing and computer vision. However, their performance in
multivariate long-term forecasting often falls short compared to simpler linear
baselines. Previous research has identified the traditional attention mechanism
as a key factor limiting their effectiveness in this domain. To bridge this
gap, we introduce LATST, a novel approach designed to mitigate entropy collapse
and training instability common challenges in Transformer-based time series
forecasting. We rigorously evaluate LATST across multiple real-world
multivariate time series datasets, demonstrating its ability to outperform
existing state-of-the-art Transformer models. Notably, LATST manages to achieve
competitive performance with fewer parameters than some linear models on
certain datasets, highlighting its efficiency and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages with referencing, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: How Robust is Audio Watermarking in Generative AI models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio watermarking is increasingly used to verify the provenance of
AI-generated content, enabling applications such as detecting AI-generated
speech, protecting music IP, and defending against voice cloning. To be
effective, audio watermarks must resist removal attacks that distort signals to
evade detection. While many schemes claim robustness, these claims are
typically tested in isolation and against a limited set of attacks. A
systematic evaluation against diverse removal attacks is lacking, hindering
practical deployment. In this paper, we investigate whether recent watermarking
schemes that claim robustness can withstand a broad range of removal attacks.
First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we
summarize their underlying technologies and potential vulnerabilities. We then
present a large-scale empirical study to assess their robustness. To support
this, we build an evaluation framework encompassing 22 types of removal attacks
(109 configurations) including signal-level, physical-level, and AI-induced
distortions. We reproduce 9 watermarking schemes using open-source code,
identify 8 new highly effective attacks, and highlight 11 key findings that
expose the fundamental limitations of these methods across 3 public datasets.
Our results reveal that none of the surveyed schemes can withstand all tested
distortions. This evaluation offers a comprehensive view of how current
watermarking methods perform under real-world threats. Our demo and code are
available at https://sokaudiowm.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCA-SUNet: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Wu, Yipeng Xu, Tianyu Xu, Jialu Zhang, Jianfeng Ren, Xudong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-Free Counting aims to count objects of interest without intensive
annotations of objects or exemplars. To achieve this, we propose a Gated
Context-Aware Swin-UNet (GCA-SUNet) to directly map an input image to the
density map of countable objects. Specifically, a set of Swin transformers form
an encoder to derive a robust feature representation, and a Gated Context-Aware
Modulation block is designed to suppress irrelevant objects or background
through a gate mechanism and exploit the attentive support of objects of
interest through a self-similarity matrix. The gate strategy is also
incorporated into the bottleneck network and the decoder of the Swin-UNet to
highlight the features most relevant to objects of interest. By explicitly
exploiting the attentive support among countable objects and eliminating
irrelevant features through the gate mechanisms, the proposed GCA-SUNet focuses
on and counts objects of interest without relying on predefined categories or
exemplars. Experimental results on the real-world datasets such as FSC-147 and
CARPK demonstrate that GCA-SUNet significantly and consistently outperforms
state-of-the-art methods. The code is available at
https://github.com/Amordia/GCA-SUNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Gromov-Wasserstein Metric <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03664v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03664v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Bai, Rocio Diaz Martin, Abihith Kothapalli, Hengrong Du, Xinran Liu, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Gromov-Wasserstein (GW) distance has gained increasing interest in the
machine learning community in recent years, as it allows for the comparison of
measures in different metric spaces. To overcome the limitations imposed by the
equal mass requirements of the classical GW problem, researchers have begun
exploring its application in unbalanced settings. However, Unbalanced GW (UGW)
can only be regarded as a discrepancy rather than a rigorous metric/distance
between two metric measure spaces (mm-spaces). In this paper, we propose a
particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We
establish that PGW is a well-defined metric between mm-spaces and discuss its
theoretical properties, including the existence of a minimizer for the PGW
problem and the relationship between PGW and GW, among others. We then propose
two variants of the Frank-Wolfe algorithm for solving the PGW problem and show
that they are mathematically and computationally equivalent. Moreover, based on
our PGW metric, we introduce the analogous concept of barycenters for
mm-spaces. Finally, we validate the effectiveness of our PGW metric and related
solvers in applications such as shape matching, shape retrieval, and shape
interpolation, comparing them against existing baselines. Our code is available
at https://github.com/mint-vu/PGW_Metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Selective Homomorphic Encryption Approach for Faster
  Privacy-Preserving Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Korkmaz, Praveen Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a machine learning method that supports training models
on decentralized devices or servers, where each holds its local data, removing
the need for data exchange. This approach is especially useful in healthcare,
as it enables training on sensitive data without needing to share them. The
nature of federated learning necessitates robust security precautions due to
data leakage concerns during communication. To address this issue, we propose a
new approach that employs selective encryption, homomorphic encryption,
differential privacy, and bit-wise scrambling to minimize data leakage while
achieving good execution performance. Our technique , FAS (fast and secure
federated learning) is used to train deep learning models on medical imaging
data. We implemented our technique using the Flower framework and compared with
a state-of-the-art federated learning approach that also uses selective
homomorphic encryption. Our experiments were run in a cluster of eleven
physical machines to create a real-world federated learning scenario on
different datasets. We observed that our approach is up to 90\% faster than
applying fully homomorphic encryption on the model weights. In addition, we can
avoid the pretraining step that is required by our competitor and can save up
to 46% in terms of total execution time. While our approach was faster, it
obtained similar security results as the competitor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffery L Painter, Gregory E Powell, Andrew Bate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable drug safety reference databases are essential for pharmacovigilance,
yet existing resources like SIDER are outdated and static. We introduce PVLens,
an automated system that extracts labeled safety information from FDA
Structured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates
automation with expert oversight through a web-based review tool. In validation
against 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall
(0.983) and moderate precision (0.799). By offering a scalable, more accurate
and continuously updated alternative to SIDER, PVLens enhances real-time
pharamcovigilance with improved accuracy and contemporaneous insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising VAE as an Explainable Feature Reduction and Diagnostic
  Pipeline for Autism Based on Resting state fMRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Zheng, Orren Ravid, Robert A. J. Barry, Yoojean Kim, Qian Wang, Young-geun Kim, Xi Zhu, Xiaofu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism spectrum disorders (ASDs) are developmental conditions characterized
by restricted interests and difficulties in communication. The complexity of
ASD has resulted in a deficiency of objective diagnostic biomarkers. Deep
learning methods have gained recognition for addressing these challenges in
neuroimaging analysis, but finding and interpreting such diagnostic biomarkers
are still challenging computationally. Here, we propose a feature reduction
pipeline using resting-state fMRI data. We used Craddock atlas and Power atlas
to extract functional connectivity data from rs-fMRI, resulting in over 30
thousand features. By using a denoising variational autoencoder, our proposed
pipeline further compresses the connectivity features into 5 latent Gaussian
distributions, providing is a low-dimensional representation of the data to
promote computational efficiency and interpretability. To test the method, we
employed the extracted latent representations to classify ASD using traditional
classifiers such as SVM on a large multi-site dataset. The 95% confidence
interval for the prediction accuracy of SVM is [0.63, 0.76] after site
harmonization using the extracted latent distributions. Without using DVAE for
dimensionality reduction, the prediction accuracy is 0.70, which falls within
the interval. The DVAE successfully encoded the diagnostic information from
rs-fMRI data without sacrificing prediction performance. The runtime for
training the DVAE and obtaining classification results from its extracted
latent features was 7 times shorter compared to training classifiers directly
on the raw data. Our findings suggest that the Power atlas provides more
effective brain connectivity insights for diagnosing ASD than Craddock atlas.
Additionally, we visualized the latent representations to gain insights into
the brain networks contributing to the differences between ASD and neurotypical
brains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Context-Aware Approach for Enhancing Data Imputation with <span class="highlight-title">Pre-train</span>ed
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahatsham Hayat, Mohammad Rashedul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach named \textbf{C}ontextually
\textbf{R}elevant \textbf{I}mputation leveraging pre-trained \textbf{L}anguage
\textbf{M}odels (\textbf{CRILM}) for handling missing data in tabular datasets.
Instead of relying on traditional numerical estimations, CRILM uses pre-trained
language models (LMs) to create contextually relevant descriptors for missing
values. This method aligns datasets with LMs' strengths, allowing large LMs to
generate these descriptors and small LMs to be fine-tuned on the enriched
datasets for enhanced downstream task performance. Our evaluations demonstrate
CRILM's superior performance and robustness across MCAR, MAR, and challenging
MNAR scenarios, with up to a 10\% improvement over the best-performing
baselines. By mitigating biases, particularly in MNAR settings, CRILM improves
downstream task performance and offers a cost-effective solution for
resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Exponential Separation Between Quantum and Quantum-Inspired Classical
  Algorithms for Linear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02087v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02087v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allan Grønlund, Kasper Green Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving a provable exponential quantum speedup for an important machine
learning task has been a central research goal since the seminal HHL quantum
algorithm for solving linear systems and the subsequent quantum recommender
systems algorithm by Kerenidis and Prakash. These algorithms were initially
believed to be strong candidates for exponential speedups, but a lower bound
ruling out similar classical improvements remained absent. In breakthrough work
by Tang, it was demonstrated that this lack of progress in classical lower
bounds was for good reasons. Concretely, she gave a classical counterpart of
the quantum recommender systems algorithm, reducing the quantum advantage to a
mere polynomial. Her approach is quite general and was named quantum-inspired
classical algorithms. Since then, almost all the initially exponential quantum
machine learning speedups have been reduced to polynomial via new
quantum-inspired classical algorithms. From the current state-of-affairs, it is
unclear whether we can hope for exponential quantum speedups for any natural
machine learning task.
  In this work, we present the first such provable exponential separation
between quantum and quantum-inspired classical algorithms for the basic problem
of solving a linear system when the input matrix is well-conditioned and has
sparse rows and columns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Anomaly Detection in Time Series: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00058v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00058v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi Kieu Khanh Ho, Ali Karami, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advances in technology, a wide range of systems continue to
collect a large amount of data over time and thus generate time series.
Time-Series Anomaly Detection (TSAD) is an important task in various
time-series applications such as e-commerce, cybersecurity, vehicle
maintenance, and healthcare monitoring. However, this task is very challenging
as it requires considering both the intra-variable dependency (relationships
within a variable over time) and the inter-variable dependency (relationships
between multiple variables) existing in time-series data. Recent graph-based
approaches have made impressive progress in tackling the challenges of this
field. In this survey, we conduct a comprehensive and up-to-date review of TSAD
using graphs, referred to as G-TSAD. First, we explore the significant
potential of graph representation for time-series data and and its
contributions to facilitating anomaly detection. Then, we review
state-of-the-art graph anomaly detection techniques, mostly leveraging deep
learning architectures, in the context of time series. For each method, we
discuss its strengths, limitations, and the specific applications where it
excels. Finally, we address both the technical and application challenges
currently facing the field, and suggest potential future directions for
advancing research and improving practical outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Inference of Plate Bending from Heterogeneous Data:
  Physics-informed Gaussian Processes via Kirchhoff-Love Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Kavrakov, Gledson Rodrigo Tondo, Guido Morgenthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in machine learning and an abundance of structural monitoring
data have inspired the integration of mechanical models with probabilistic
models to identify a structure's state and quantify the uncertainty of its
physical parameters and response. In this paper, we propose an inference
methodology for classical Kirchhoff-Love plates via physics-informed Gaussian
Processes (GP). A probabilistic model is formulated as a multi-output GP by
placing a GP prior on the deflection and deriving the covariance function using
the linear differential operators of the plate governing equations. The
posteriors of the flexural rigidity, hyperparameters, and plate response are
inferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling
from noisy measurements. We demonstrate the applicability with two examples: a
simply supported plate subjected to a sinusoidal load and a fixed plate
subjected to a uniform load. The results illustrate how the proposed
methodology can be employed to perform stochastic inference for plate rigidity
and physical quantities by integrating measurements from various sensor types
and qualities. Potential applications of the presented methodology are in
structural health monitoring and uncertainty quantification of plate-like
structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNMerge: Merging of GNN Models Without Accessing Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Garg, Ishita Thakre, Sayan Ranu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has gained prominence in machine learning as a method to
integrate multiple trained models into a single model without accessing the
original training data. While existing approaches have demonstrated success in
domains such as computer vision and NLP, their application to Graph Neural
Networks (GNNs) remains unexplored. These methods often rely on the assumption
of shared initialization, which is seldom applicable to GNNs. In this work, we
undertake the first benchmarking study of model merging algorithms for GNNs,
revealing their limited effectiveness in this context. To address these
challenges, we propose GNNMerge, which utilizes a task-agnostic node embedding
alignment strategy to merge GNNs. Furthermore, we establish that under a mild
relaxation, the proposed optimization objective admits direct analytical
solutions for widely used GNN architectures, significantly enhancing its
computational efficiency. Empirical evaluations across diverse datasets, tasks,
and architectures establish GNNMerge to be up to 24% more accurate than
existing methods while delivering over 2 orders of magnitude speed-up compared
to training from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScalingNoise: Scaling Inference-Time Search for Generating Infinite
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, Junjun He, Zongyuan Ge, Imran Razzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video diffusion models (VDMs) facilitate the generation of high-quality
videos, with current research predominantly concentrated on scaling efforts
during training through improvements in data quality, computational resources,
and model complexity. However, inference-time scaling has received less
attention, with most approaches restricting models to a single generation
attempt. Recent studies have uncovered the existence of "golden noises" that
can enhance video quality during generation. Building on this, we find that
guiding the scaling inference-time search of VDMs to identify better noise
candidates not only evaluates the quality of the frames generated in the
current step but also preserves the high-level object features by referencing
the anchor frame from previous multi-chunks, thereby delivering long-term
value. Our analysis reveals that diffusion models inherently possess flexible
adjustments of computation by varying denoising steps, and even a one-step
denoising approach, when guided by a reward signal, yields significant
long-term benefits. Based on the observation, we proposeScalingNoise, a
plug-and-play inference-time search strategy that identifies golden initial
noises for the diffusion sampling process to improve global content consistency
and visual diversity. Specifically, we perform one-step denoising to convert
initial noises into a clip and subsequently evaluate its long-term value,
leveraging a reward model anchored by previously generated content. Moreover,
to preserve diversity, we sample candidates from a tilted noise distribution
that up-weights promising noises. In this way, ScalingNoise significantly
reduces noise-induced errors, ensuring more coherent and spatiotemporally
consistent video generation. Extensive experiments on benchmark datasets
demonstrate that the proposed ScalingNoise effectively improves long video
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demand Estimation with Text and Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Compiani, Ilya Morozov, Stephan Seiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a demand estimation method that leverages unstructured text and
image data to infer substitution patterns. Using pre-trained deep learning
models, we extract embeddings from product images and textual descriptions and
incorporate them into a random coefficients logit model. This approach enables
researchers to estimate demand even when they lack data on product attributes
or when consumers value hard-to-quantify attributes, such as visual design or
functional benefits. Using data from a choice experiment, we show that our
approach outperforms standard attribute-based models in counterfactual
predictions of consumers' second choices. We also apply it across 40 product
categories on Amazon and consistently find that text and image data help
identify close substitutes within each category.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-<span class="highlight-title">Transformer</span> Cooperative Architecture for Trustworthy Graph
  Contrastive Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16218v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16218v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Liang, Xinkai Wei, Min Chen, Zhiqiang Wang, Jiye Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning (GCL) has become a hot topic in the field of graph
representation learning. In contrast to traditional supervised learning relying
on a large number of labels, GCL exploits augmentation strategies to generate
multiple views and positive/negative pairs, both of which greatly influence the
performance. Unfortunately, commonly used random augmentations may disturb the
underlying semantics of graphs. Moreover, traditional GNNs, a type of widely
employed encoders in GCL, are inevitably confronted with over-smoothing and
over-squashing problems. To address these issues, we propose GNN-Transformer
Cooperative Architecture for Trustworthy Graph Contrastive Learning (GTCA),
which inherits the advantages of both GNN and Transformer, incorporating graph
topology to obtain comprehensive graph representations. Theoretical analysis
verifies the trustworthiness of the proposed method. Extensive experiments on
benchmark datasets demonstrate state-of-the-art empirical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latency Minimization for UAV-Enabled Federated Learning: Trajectory
  Design and Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Zhang, Wenchao Liu, Jinke Ren, Huijun Xing, Gui Gui, Yanyan Shen, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has become a transformative paradigm for distributed
machine learning across wireless networks. However, the performance of FL is
often hindered by the unreliable communication links between
resource-constrained Internet of Things (IoT) devices and the central server.
To overcome this challenge, we propose a novel framework that employs an
unmanned aerial vehicle (UAV) as a mobile server to enhance the FL training
process. By capitalizing on the UAV's mobility, we establish strong
line-of-sight connections with IoT devices, thereby enhancing communication
reliability and capacity. To maximize training efficiency, we formulate a
latency minimization problem that jointly optimizes bandwidth allocation,
computing frequencies, transmit power for both the UAV and IoT devices, and the
UAV's flight trajectory. Subsequently, we analyze the required rounds of the
IoT devices training and the UAV aggregation for FL convergence. Based on the
convergence constraint, we transform the problem into three subproblems and
develop an efficient alternating optimization algorithm to solve this problem
effectively. Additionally, we provide a thorough analysis of the algorithm's
convergence and computational complexity. Extensive numerical results
demonstrate that our proposed scheme not only surpasses existing benchmark
schemes in reducing latency up to 15.29%, but also achieves training efficiency
that nearly matches the ideal scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passenger hazard perception based on EEG signals for highly automated
  driving vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashton Yu Xuan Tan, Yingkai Yang, Xiaofei Zhang, Bowen Li, Xiaorong Gao, Sifa Zheng, Jianqiang Wang, Xinyu Gu, Jun Li, Yang Zhao, Yuxin Zhang, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the safety of autonomous vehicles is crucial, especially given
recent accidents involving automated systems. As passengers in these vehicles,
humans' sensory perception and decision-making can be integrated with
autonomous systems to improve safety. This study explores neural mechanisms in
passenger-vehicle interactions, leading to the development of a Passenger
Cognitive Model (PCM) and the Passenger EEG Decoding Strategy (PEDS). Central
to PEDS is a novel Convolutional Recurrent Neural Network (CRNN) that captures
spatial and temporal EEG data patterns. The CRNN, combined with stacking
algorithms, achieves an accuracy of $85.0\% \pm 3.18\%$. Our findings highlight
the predictive power of pre-event EEG data, enhancing the detection of
hazardous scenarios and offering a network-driven framework for safer
autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have decided to withdraw this submission due to ongoing revisions
  and further refinements in our research. A revised version may be resubmitted
  in the future. We appreciate the feedback and interest from the community</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAPAYA Federated Analytics Stack: Engineering Privacy, Scalability and
  Practicality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish Srinivas, Graham Cormode, Mehrdad Honarkhah, Samuel Lurye, Jonathan Hehir, Lunwen He, George Hong, Ahmed Magdy, Dzmitry Huba, Kaikai Wang, Shen Guo, Shoubhik Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-device Federated Analytics (FA) is a distributed computation paradigm
designed to answer analytics queries about and derive insights from data held
locally on users' devices. On-device computations combined with other privacy
and security measures ensure that only minimal data is transmitted off-device,
achieving a high standard of data protection. Despite FA's broad relevance, the
applicability of existing FA systems is limited by compromised accuracy; lack
of flexibility for data analytics; and an inability to scale effectively. In
this paper, we describe our approach to combine privacy, scalability, and
practicality to build and deploy a system that overcomes these limitations. Our
FA system leverages trusted execution environments (TEEs) and optimizes the use
of on-device computing resources to facilitate federated data processing across
large fleets of devices, while ensuring robust, defensible, and verifiable
privacy safeguards. We focus on federated analytics (statistics and
monitoring), in contrast to systems for federated learning (ML workloads), and
we flag the key differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic neural operators for functional uncertainty quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Bülte, Philipp Scholl, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural operators aim to approximate the solution operator of a system of
differential equations purely from data. They have shown immense success in
modeling complex dynamical systems across various domains. However, the
occurrence of uncertainties inherent in both model and data has so far rarely
been taken into account\textemdash{}a critical limitation in complex, chaotic
systems such as weather forecasting. In this paper, we introduce the
probabilistic neural operator (PNO), a framework for learning probability
distributions over the output function space of neural operators. PNO extends
neural operators with generative modeling based on strictly proper scoring
rules, integrating uncertainty information directly into the training process.
We provide a theoretical justification for the approach and demonstrate
improved performance in quantifying uncertainty across different domains and
with respect to different baselines. Furthermore, PNO requires minimal
adjustment to existing architectures, shows improved performance for most
probabilistic prediction tasks, and leads to well-calibrated predictive
distributions and adequate uncertainty representations even for long dynamical
trajectories. Implementing our approach into large-scale models for physical
applications can lead to improvements in corresponding uncertainty
quantification and extreme event identification, ultimately leading to a deeper
understanding of the prediction of such surrogate models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning during Learning: An Efficient Federated Machine Unlearning
  Method <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Federated Learning (FL) has garnered significant attention
as a distributed machine learning paradigm. To facilitate the implementation of
the right to be forgotten, the concept of federated machine unlearning (FMU)
has also emerged. However, current FMU approaches often involve additional
time-consuming steps and may not offer comprehensive unlearning capabilities,
which renders them less practical in real FL scenarios. In this paper, we
introduce FedAU, an innovative and efficient FMU framework aimed at overcoming
these limitations. Specifically, FedAU incorporates a lightweight auxiliary
unlearning module into the learning process and employs a straightforward
linear operation to facilitate unlearning. This approach eliminates the
requirement for extra time-consuming steps, rendering it well-suited for FL.
Furthermore, FedAU exhibits remarkable versatility. It not only enables
multiple clients to carry out unlearning tasks concurrently but also supports
unlearning at various levels of granularity, including individual data samples,
specific classes, and even at the client level. We conducted extensive
experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the
performance of FedAU. The results demonstrate that FedAU effectively achieves
the desired unlearning effect while maintaining model accuracy. Our code is
availiable at https://github.com/Liar-Mask/FedAU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedMIA: An Effective Membership Inference Attack Exploiting "All for
  One" Principle in Federated Learning <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongxi Zhu, Donghao Li, Hanlin Gu, Yuan Yao, Lixin Fan, Yuxing Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a promising approach for training machine learning
models on decentralized data while preserving privacy. However, privacy risks,
particularly Membership Inference Attacks (MIAs), which aim to determine
whether a specific data point belongs to a target client's training set, remain
a significant concern. Existing methods for implementing MIAs in FL primarily
analyze updates from the target client, focusing on metrics such as loss,
gradient norm, and gradient difference. However, these methods fail to leverage
updates from non-target clients, potentially underutilizing available
information. In this paper, we first formulate a one-tailed likelihood-ratio
hypothesis test based on the likelihood of updates from non-target clients.
Building upon this formulation, we introduce a three-step Membership Inference
Attack (MIA) method, called FedMIA, which follows the "all for one"--leveraging
updates from all clients across multiple communication rounds to enhance MIA
effectiveness. Both theoretical analysis and extensive experimental results
demonstrate that FedMIA outperforms existing MIAs in both classification and
generative tasks. Additionally, it can be integrated as an extension to
existing methods and is robust against various defense strategies, Non-IID
data, and different federated structures. Our code is available in
https://github.com/Liar-Mask/FedMIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures; Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On best approximation by multivariate ridge functions with applications
  to generalized translation networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Geuchen, Palina Salanevich, Olov Schavemaker, Felix Voigtlaender
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove sharp upper and lower bounds for the approximation of Sobolev
functions by sums of multivariate ridge functions, i.e., functions of the form
$\mathbb{R}^d \ni x \mapsto \sum_{k=1}^n h_k(A_k x) \in \mathbb{R}$ with $h_k :
\mathbb{R}^\ell \to \mathbb{R}$ and $A_k \in \mathbb{R}^{\ell \times d}$. We
show that the order of approximation asymptotically behaves as
$n^{-r/(d-\ell)}$, where $r$ is the regularity of the Sobolev functions to be
approximated. Our lower bound even holds when approximating $L^\infty$-Sobolev
functions of regularity $r$ with error measured in $L^1$, while our upper bound
applies to the approximation of $L^p$-Sobolev functions in $L^p$ for any $1
\leq p \leq \infty$. These bounds generalize well-known results about the
approximation properties of univariate ridge functions to the multivariate
case. Moreover, we use these bounds to obtain sharp asymptotic bounds for the
approximation of Sobolev functions using generalized translation networks and
complex-valued neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Esposito, Anpei Chen, Christian Reiser, Samuel Rota Bulò, Lorenzo Porzi, Katja Schwarz, Christian Richardt, Michael Zollhöfer, Peter Kontschieder, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality view synthesis relies on volume rendering, splatting, or surface
rendering. While surface rendering is typically the fastest, it struggles to
accurately model fuzzy geometry like hair. In turn, alpha-blending techniques
excel at representing fuzzy materials but require an unbounded number of
samples per ray (P1). Further overheads are induced by empty space skipping in
volume rendering (P2) and sorting input primitives in splatting (P3). We
present a novel representation for real-time view synthesis where the (P1)
number of sampling locations is small and bounded, (P2) sampling locations are
efficiently found via rasterization, and (P3) rendering is sorting-free. We
achieve this by representing objects as semi-transparent multi-layer meshes
rendered in a fixed order. First, we model surface layers as signed distance
function (SDF) shells with optimal spacing learned during training. Then, we
bake them as meshes and fit UV textures. Unlike single-surface methods, our
multi-layer representation effectively models fuzzy objects. In contrast to
volume and splatting-based methods, our approach enables real-time rendering on
low-power laptops and smartphones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Estimation and Prediction of City-wide Delivery Demand: A Large
  Language Model Empowered Graph-based Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Nie, Junlin He, Yuewen Mei, Guoyang Qin, Guilong Li, Jian Sun, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of e-commerce and urbanization has significantly
intensified delivery operations in urban areas, boosting the volume and
complexity of delivery demand. Data-driven predictive methods, especially those
utilizing machine learning techniques, have emerged to handle these
complexities in urban delivery demand management problems. One particularly
pressing issue that has yet to be sufficiently addressed is the joint
estimation and prediction of city-wide delivery demand, as well as the
generalization of the model to new cities. To this end, we formulate this
problem as a transferable graph-based spatiotemporal learning task. First, an
individual-collective message-passing neural network model is formalized to
capture the interaction between demand patterns of associated regions. Second,
by exploiting recent advances in large language models (LLMs), we extract
general geospatial knowledge encodings from the unstructured locational data
using the embedding generated by LLMs. Last, to encourage the cross-city
generalization of the model, we integrate the encoding into the demand
predictor in a transferable way. Comprehensive empirical evaluation results on
two real-world delivery datasets, including eight cities in China and the US,
demonstrate that our model significantly outperforms state-of-the-art baselines
in accuracy, efficiency, and transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DR-PETS: Learning-Based Control With Planning in Adversarial
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hozefa Jesawada, Antonio Acernese, Giovanni Russo, Carmen Del Vecchio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring robustness against epistemic, possibly adversarial, perturbations is
essential for reliable real-world decision-making. While the Probabilistic
Ensembles with Trajectory Sampling (PETS) algorithm inherently handles
uncertainty via ensemble-based probabilistic models, it lacks guarantees
against structured adversarial or worst-case uncertainty distributions. To
address this, we propose DR-PETS, a distributionally robust extension of PETS
that certifies robustness against adversarial perturbations. We formalize
uncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware
planning through a min-max optimization framework. While PETS passively
accounts for stochasticity, DR-PETS actively optimizes robustness via a
tractable convex approximation integrated into PETS planning loop. Experiments
on pendulum stabilization and cart-pole balancing show that DR-PETS certifies
robustness against adversarial parameter perturbations, achieving consistent
performance in worst-case scenarios where PETS deteriorates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving clustering quality evaluation in noisy Gaussian mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato Cordeiro de Amorim, Vladimir Makarenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering is a well-established technique in machine learning and data
analysis, widely used across various domains. Cluster validity indices, such as
the Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldin indices,
play a crucial role in assessing clustering quality when external ground truth
labels are unavailable. However, these measures can be affected by the feature
relevance issue, potentially leading to unreliable evaluations in
high-dimensional or noisy data sets.
  We introduce a theoretically grounded Feature Importance Rescaling (FIR)
method that enhances the quality of clustering validation by adjusting feature
contributions based on their dispersion. It attenuates noise features,
clarifies clustering compactness and separation, and thereby aligns clustering
validation more closely with the ground truth. Through extensive experiments on
synthetic data sets under different configurations, we demonstrate that FIR
consistently improves the correlation between the values of cluster validity
indices and the ground truth, particularly in settings with noisy or irrelevant
features.
  The results show that FIR increases the robustness of clustering evaluation,
reduces variability in performance across different data sets, and remains
effective even when clusters exhibit significant overlap. These findings
highlight the potential of FIR as a valuable enhancement of clustering
validation, making it a practical tool for unsupervised learning tasks where
labelled data is unavailable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On learning higher-order cumulants in diffusion models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gert Aarts, Diaa E. Habibi, Lingxiao Wang, Kai Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To analyse how diffusion models learn correlations beyond Gaussian ones, we
study the behaviour of higher-order cumulants, or connected n-point functions,
under both the forward and backward process. We derive explicit expressions for
the moment- and cumulant-generating functionals, in terms of the distribution
of the initial data and properties of forward process. It is shown analytically
that during the forward process higher-order cumulants are conserved in models
without a drift, such as the variance-expanding scheme, and that therefore the
endpoint of the forward process maintains nontrivial correlations. We
demonstrate that since these correlations are encoded in the score function,
higher-order cumulants are learnt in the backward process, also when starting
from a normal prior. We confirm our analytical results in an exactly solvable
toy model with nonzero cumulants and in scalar lattice field theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, many figures. Extended version of contribution awarded
  "best 'physics for AI' paper award" in the NeurIPS 2024 workshop "Machine
  Learning and the Physical Sciences"; v2: references and minor clarifications
  added, version to appear in Machine Learning: Science and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond [cls]: Exploring the true potential of Masked Image Modeling
  representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Przewięźlikowski, Randall Balestriero, Wojciech Jasiński, Marek Śmieja, Bartosz Zieliński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) has emerged as a promising approach for
Self-Supervised Learning (SSL) of visual representations. However, the
out-of-the-box performance of MIMs is typically inferior to competing
approaches. Most users cannot afford fine-tuning due to the need for large
amounts of data, high GPU consumption, and specialized user knowledge.
Therefore, the practical use of MIM representations is limited. In this paper
we ask what is the reason for the poor out-of-the-box performance of MIMs. Is
it due to weaker features produced by MIM models, or is it due to suboptimal
usage? Through detailed analysis, we show that attention in MIMs is spread
almost uniformly over many patches, leading to ineffective aggregation by the
[cls] token. Based on this insight, we propose Selective Aggregation to better
capture the rich semantic information retained in patch tokens, which
significantly improves the out-of-the-box performance of MIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongViTU: Instruction Tuning for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, Qing Li, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),
automatically generated dataset for long-form video understanding. We propose a
systematic approach that organizes videos into a hierarchical tree structure
for QA generation and incorporates self-revision mechanisms to ensure
high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context
(average certificate length of 4.6 minutes); 2) rich knowledge and condensed
reasoning (commonsense, causality, planning, etc.)). We also offer explicit
timestamp annotations of relevant events for each QA pair. We have conducted
extensive human studies on LongViTU, and the results prove the quality of our
dataset. To better evaluate the challenges posed by LongViTU's emphasis on
long-term context and condensed reasoning, we manually curate a subset of
LongViTU into a benchmark. Evaluations using a state-of-the-art open-source
model (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators
yield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the
substantial difficulty presented by LongViTU questions. Performing supervised
fine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average
performance gains of 2.5% and 3.7%, respectively, across a suite of long video
understanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Relevance and Magnitude for Resource-Aware DNN Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carla Fabiana Chiasserini, Francesco Malandrino, Nuria Molner, Zhiqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning neural networks, i.e., removing some of their parameters whilst
retaining their accuracy, is one of the main ways to reduce the latency of a
machine learning pipeline, especially in resource- and/or bandwidth-constrained
scenarios. In this context, the pruning technique, i.e., how to choose the
parameters to remove, is critical to the system performance. In this paper, we
propose a novel pruning approach, called FlexRel and predicated upon combining
training-time and inference-time information, namely, parameter magnitude and
relevance, in order to improve the resulting accuracy whilst saving both
computational resources and bandwidth. Our performance evaluation shows that
FlexRel is able to achieve higher pruning factors, saving over 35% bandwidth
for typical accuracy targets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Expansion of <span class="highlight-title">Pre-train</span>ed Models with Mixture of Adapters for
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyi Wang, Haodong Lu, Lina Yao, Dong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) aims to continually accumulate knowledge from a
non-stationary data stream without catastrophic forgetting of learned
knowledge, requiring a balance between stability and adaptability. Relying on
the generalizable representation in pre-trained models (PTMs), PTM-based CL
methods perform effective continual adaptation on downstream tasks by adding
learnable adapters or prompts upon the frozen PTMs. However, many existing
PTM-based CL methods use restricted adaptation on a fixed set of these modules
to avoid forgetting, suffering from limited CL ability. Periodically adding
task-specific modules results in linear model growth rate and impaired
knowledge reuse. We propose Self-Expansion of pre-trained models with
Modularized Adaptation (SEMA), a novel approach to enhance the control of
stability-plasticity balance in PTM-based CL. SEMA automatically decides to
reuse or add adapter modules on demand in CL, depending on whether significant
distribution shift that cannot be handled is detected at different
representation levels. We design modular adapter consisting of a functional
adapter and a representation descriptor. The representation descriptors are
trained as a distribution shift indicator and used to trigger self-expansion
signals. For better composing the adapters, an expandable weighting router is
learned jointly for mixture of adapter outputs. SEMA enables better knowledge
reuse and sub-linear expansion rate. Extensive experiments demonstrate the
effectiveness of the proposed self-expansion method, achieving state-of-the-art
performance compared to PTM-based CL methods without memory rehearsal. Code is
available at https://github.com/huiyiwang01/SEMA-CL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https: https://github.com/huiyiwang01/SEMA-CL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Large Model Training through Overlapped Activation
  Recomputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Chen, Wenjie Zhang, Shuibing He, Weijian Chen, Siling Yang, Kexin Huang, Yanlong Yin, Xuan Zhan, Yingjie Gu, Zhuwei Peng, Yi Zheng, Zhefeng Wang, Gang Chen Yingjie Gu, Zhuwei Peng, Kexin Huang, Xuan Zhan, Weijian Chen, Yi Zheng, Zhefeng Wang, Yanlong Yin, Gang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large model training often uses recomputation to alleviate memory pressure
and pipelines to exploit the parallelism of data, tensors, and devices.
However, existing recomputation approaches may incur high overhead when
training real-world models, as they are executed on demand in the critical
training path. In this paper, we present Lynx, a new recomputation framework to
reduce overhead by overlapping recomputation with communication in training
pipelines. To reduce the large search space for recomputation strategies, we
propose a heuristic-based recomputation scheduling algorithm, which is based on
the observation that there are identical structures in large DNN models so that
we can apply the same scheduling policy to all such structures. Additionally,
we propose a recomputation-aware model partitioning method to balance each
stage's execution time for improved training throughput. Our comprehensive
evaluation using GPT models with 1.3B-23B parameters shows that Lynx
outperforms existing recomputation approaches by up to 1.37x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Estimation of Conditional Mean and Covariance for Unbalanced
  Panels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21858v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21858v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damir Filipovic, Paul Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a nonparametric, kernel-based joint estimator for conditional mean
and covariance matrices in large and unbalanced panels. The estimator is
supported by rigorous consistency results and finite-sample guarantees,
ensuring its reliability for empirical applications. We apply it to an
extensive panel of monthly US stock excess returns from 1962 to 2021, using
macroeconomic and firm-specific covariates as conditioning variables. The
estimator effectively captures time-varying cross-sectional dependencies,
demonstrating robust statistical and economic performance. We find that
idiosyncratic risk explains, on average, more than 75% of the cross-sectional
variance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback-driven object detection and iterative model improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub. To support the understanding of our labeling process, we
have created an explanatory video demonstrating the methodology using
microscopy images of E. coli bacteria as an example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ml-lab-htw/iterative-annotate Video:
  https://www.youtube.com/watch?v=CM9uhE8NN5E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the effects of Data Sparsity on the Link-level Bicycling
  Volume Estimation: A Graph Convolutional Neural Network Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohit Gupta, Debjit Bhowmick, Meead Saberi, Shirui Pan, Ben Beck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate bicycling volume estimation is crucial for making informed decisions
and planning about future investments in bicycling infrastructure. However,
traditional link-level volume estimation models are effective for motorized
traffic but face significant challenges when applied to the bicycling context
because of sparse data and the intricate nature of bicycling mobility patterns.
To the best of our knowledge, we present the first study to utilize a Graph
Convolutional Network (GCN) architecture to model link-level bicycling volumes
and systematically investigate the impact of varying levels of data sparsity
(0%--99%) on model performance, simulating real-world scenarios. We have
leveraged Strava Metro data as the primary source of bicycling counts across
15,933 road segments/links in the City of Melbourne, Australia. To evaluate the
effectiveness of the GCN model, we benchmark it against traditional machine
learning models, such as linear regression, support vector machines, and random
forest. Our results show that the GCN model outperforms these traditional
models in predicting Annual Average Daily Bicycle (AADB) counts, demonstrating
its ability to capture the spatial dependencies inherent in bicycle traffic
networks. While GCN remains robust up to 80% sparsity, its performance declines
sharply beyond this threshold, highlighting the challenges of extreme data
sparsity. These findings underscore the potential of GCNs in enhancing
bicycling volume estimation, while also emphasizing the need for further
research on methods to improve model resilience under high-sparsity conditions.
Our findings offer valuable insights for city planners aiming to improve
bicycling infrastructure and promote sustainable transportation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Alignment Enhanced Latent Diffusion <span class="highlight-title">Transformer</span> for Zero-Shot
  Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent zero-shot text-to-speech (TTS) models have significantly
improved speech quality and expressiveness, mainstream systems still suffer
from issues related to speech-text alignment modeling: 1) models without
explicit speech-text alignment modeling exhibit less robustness, especially for
hard sentences in practical applications; 2) predefined alignment-based models
suffer from naturalness constraints of forced alignments. This paper introduces
\textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment
algorithm that guides the latent diffusion transformer (DiT). Specifically, we
provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of
alignment without limiting the search space, thereby achieving high
naturalness. Moreover, we employ a multi-condition classifier-free guidance
strategy for accent intensity adjustment and adopt the piecewise rectified flow
technique to accelerate the generation process. Experiments demonstrate that
MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports
highly flexible control over accent intensity. Notably, our system can generate
high-quality one-minute speech with only 8 sampling steps. Audio samples are
available at https://sditdemo.github.io/sditdemo/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Feature Learning for Multi-Index Models in High Dimensions <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Mousavi-Hosseini, Adel Javanmard, Murat A. Erdogdu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been numerous studies on feature learning with neural
networks, specifically on learning single- and multi-index models where the
target is a function of a low-dimensional projection of the input. Prior works
have shown that in high dimensions, the majority of the compute and data
resources are spent on recovering the low-dimensional projection; once this
subspace is recovered, the remainder of the target can be learned independently
of the ambient dimension. However, implications of feature learning in
adversarial settings remain unexplored. In this work, we take the first steps
towards understanding adversarially robust feature learning with neural
networks. Specifically, we prove that the hidden directions of a multi-index
model offer a Bayes optimal low-dimensional projection for robustness against
$\ell_2$-bounded adversarial perturbations under the squared loss, assuming
that the multi-index coordinates are statistically independent from the rest of
the coordinates. Therefore, robust learning can be achieved by first performing
standard feature learning, then robustly tuning a linear readout layer on top
of the standard representations. In particular, we show that adversarially
robust learning is just as easy as standard learning. Specifically, the
additional number of samples needed to robustly learn multi-index models when
compared to standard learning does not depend on dimensionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 1 figure. To appear in the International Conference on
  Learning Representations (ICLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-Index Models with Neural Networks via Mean-Field Langevin
  Dynamics <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Mousavi-Hosseini, Denny Wu, Murat A. Erdogdu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of learning multi-index models in high-dimensions using
a two-layer neural network trained with the mean-field Langevin algorithm.
Under mild distributional assumptions on the data, we characterize the
effective dimension $d_{\mathrm{eff}}$ that controls both sample and
computational complexity by utilizing the adaptivity of neural networks to
latent low-dimensional structures. When the data exhibit such a structure,
$d_{\mathrm{eff}}$ can be significantly smaller than the ambient dimension. We
prove that the sample complexity grows almost linearly with $d_{\mathrm{eff}}$,
bypassing the limitations of the information and generative exponents that
appeared in recent analyses of gradient-based feature learning. On the other
hand, the computational complexity may inevitably grow exponentially with
$d_{\mathrm{eff}}$ in the worst-case scenario. Motivated by improving
computational complexity, we take the first steps towards polynomial time
convergence of the mean-field Langevin algorithm by investigating a setting
where the weights are constrained to be on a compact manifold with positive
Ricci curvature, such as the hypersphere. There, we study assumptions under
which polynomial time convergence is achievable, whereas similar assumptions in
the Euclidean setting lead to exponential time complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 2 figures. To appear in the International Conference on
  Learning Representations (ICLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Alignment: Training Diffusion Models with Reinforcement
  Learning Against Sparse Rewards <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijing Hu, Fengda Zhang, Long Chen, Kun Kuang, Jiahui Li, Kaifeng Gao, Jun Xiao, Xin Wang, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable success in text-to-image
generation. However, their practical applications are hindered by the
misalignment between generated images and corresponding text prompts. To tackle
this issue, reinforcement learning (RL) has been considered for diffusion model
fine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse
reward, where feedback is only available at the end of the generation process.
This makes it difficult to identify which actions during the denoising process
contribute positively to the final generated image, potentially leading to
ineffective or unnecessary denoising policies. To this end, this paper presents
a novel RL-based framework that addresses the sparse reward problem when
training diffusion models. Our framework, named $\text{B}^2\text{-DiffuRL}$,
employs two strategies: \textbf{B}ackward progressive training and
\textbf{B}ranch-based sampling. For one thing, backward progressive training
focuses initially on the final timesteps of denoising process and gradually
extends the training interval to earlier timesteps, easing the learning
difficulty from sparse rewards. For another, we perform branch-based sampling
for each training interval. By comparing the samples within the same branch, we
can identify how much the policies of the current training interval contribute
to the final image, which helps to learn effective policies instead of
unnecessary ones. $\text{B}^2\text{-DiffuRL}$ is compatible with existing
optimization algorithms. Extensive experiments demonstrate the effectiveness of
$\text{B}^2\text{-DiffuRL}$ in improving prompt-image alignment and maintaining
diversity in generated images. The code for this work is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025, add references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of the Impact of Federated Learning on Machine
  Learning Model Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Yang, Zhuoran Wang, Benson Chou, Sophie Xu, Hao Wang, Jingxian Wang, Qizhen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables distributed ML model training on private user
data at the global scale. Despite the potential of FL demonstrated in many
domains, an in-depth view of its impact on model accuracy remains unclear. In
this paper, we investigate, systematically, how this learning paradigm can
affect the accuracy of state-of-the-art ML models for a variety of ML tasks. We
present an empirical study that involves various data types: text, image,
audio, and video, and FL configuration knobs: data distribution, FL scale,
client sampling, and local and global computations. Our experiments are
conducted in a unified FL framework to achieve high fidelity, with substantial
human efforts and resource investments. Based on the results, we perform a
quantitative analysis of the impact of FL, and highlight challenging scenarios
where applying FL degrades the accuracy of the model drastically and identify
cases where the impact is negligible. The detailed and extensive findings can
benefit practical deployments and future development of FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Ma, Jiacheng Chen, Hongshu Guo, Yue-Jiao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that
meta-trained neural networks can effectively guide the design of black-box
optimizers, significantly reducing the need for expert tuning and delivering
robust performance across complex problem distributions. Despite their success,
a paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape
Analysis features to inform the meta-level agent about the low-level
optimization progress. To address the gap, this paper proposes Neural
Exploratory Landscape Analysis (NeurELA), a novel framework that dynamically
profiles landscape features through a two-stage, attention-based neural
network, executed in an entirely end-to-end fashion. NeurELA is pre-trained
over a variety of MetaBBO algorithms using a multi-task neuroevolution
strategy. Extensive experiments show that NeurELA achieves consistently
superior performance when integrated into different and even unseen MetaBBO
tasks and can be efficiently fine-tuned for further performance boost. This
advancement marks a pivotal step in making MetaBBO algorithms more autonomous
and broadly applicable. The source code of NeurELA can be accessed at
https://github.com/GMC-DRL/Neur-ELA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QCPINN: Quantum Classical Physics-Informed Neural Networks for Solving
  PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afrah Farea, Saiful Khan, Mustafa Serdar Celebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have emerged as promising methods
for solving partial differential equations (PDEs) by embedding physical laws
into neural architectures. However, these classical approaches often require
large number of parameters for solving complex problems or achieving reasonable
accuracy. We investigate whether quantum-enhanced architectures can achieve
comparable performance while significantly reducing model complexity. We
propose a quantum-classical physics-informed neural network (QCPINN) combining
quantum and classical components to solve PDEs with fewer parameters while
maintaining comparable accuracy and training convergence. Our approach
systematically evaluates two quantum circuit paradigms (e.g.,
continuous-variable (CV) and discrete-variable (DV)) implementations with four
circuit topologies (e.g., alternate, cascade, cross-mesh, and layered), two
embedding schemes (e.g., amplitude and angle) on five benchmark PDEs (e.g.,
Helmholtz, lid-driven cavity, wave, Klein-Gordon, and convection-diffusion
equations). Results demonstrate that QCPINNs achieve comparable accuracy to
classical PINNs while requiring approximately 10% trainable parameters across
different PDEs, and resulting in a further 40% reduction in relative L2 error
for the convection-diffusion equation. DV-based circuits with angle embedding
and cascade configurations consistently exhibited enhanced convergence
stability across all problem types. Our finding establishes parameter
efficiency as a quantifiable quantum advantage in physics-informed machine
learning. By significantly reducing model complexity while maintaining solution
quality, QCPINNs represent a potential direction for overcoming computational
bottlenecks in scientific computing applications where traditional approaches
require large parameter spaces.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-26T00:00:00Z">2025-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalability Evaluation of HPC Multi-GPU Training for ECG-based LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitar Mileski, Nikola Petrovski, Marjan Gusev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models requires extensive processing, made possible
by many high-performance computing resources. This study compares multi-node
and multi-GPU environments for training large language models of
electrocardiograms. It provides a detailed mapping of current frameworks for
distributed deep learning in multinode and multi-GPU settings, including
Horovod from Uber, DeepSpeed from Microsoft, and the built-in distributed
capabilities of PyTorch and TensorFlow. We compare various multi-GPU setups for
different dataset configurations, utilizing multiple HPC nodes independently
and focusing on scalability, speedup, efficiency, and overhead. The analysis
leverages HPC infrastructure with SLURM, Apptainer (Singularity) containers,
CUDA, PyTorch, and shell scripts to support training workflows and automation.
We achieved a sub-linear speedup when scaling the number of GPUs, with values
of 1.6x for two and 1.9x for four.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ History-Independent Concurrent Hash Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hagit Attiya, Michael A. Bender, Martín Farach-Colton, Rotem Oshman, Noa Schiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A history-independent data structure does not reveal the history of
operations applied to it, only its current logical state, even if its internal
state is examined. This paper studies history-independent concurrent
dictionaries, in particular, hash tables, and establishes inherent bounds on
their space requirements.
  This paper shows that there is a lock-free history-independent concurrent
hash table, in which each memory cell stores two elements and two bits, based
on Robin Hood hashing. Our implementation is linearizable, and uses the shared
memory primitive LL/SC. The expected amortized step complexity of the hash
table is $O(c)$, where $c$ is an upper bound on the number of concurrent
operations that access the same element, assuming the hash table is not
overpopulated. We complement this positive result by showing that even if we
have only two concurrent processes, no history-independent concurrent
dictionary that supports sets of any size, with wait-free membership queries
and obstruction-free insertions and deletions, can store only two elements of
the set and a constant number of bits in each memory cell. This holds even if
the step complexity of operations on the dictionary is unbounded.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AllReduce Scheduling with Hierarchical Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Wei, Mickel Liu, Wenfei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AllReduce is a technique in distributed computing which saw use in many
critical applications of deep learning. Existing methods of AllReduce
scheduling oftentimes lack flexibility due to being topology-specific or
relying on extensive handcrafted designs that require domain-specific
knowledge. In this work, we aim to alleviate this inflexibility by proposing a
deep-reinforcement-learning (DRL)-based pipeline that can generate AllReduce
scheduling for various network topologies without topology-specific design
features. The flow scheduling module of this pipeline consists of two
hierarchically-structured DRL policies that work cooperatively to find optimal
scheduling. We showcase the performance of our method compared to the baseline
methods on three topologies: BCube, DCell, and Jellyfish. Finally, we
contributed a Python-based simulation environment simulating AllReduce
scheduling on these network topologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usama Zafar, André Teixeira, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training across
decentralized devices without sharing raw data, but it remains vulnerable to
poisoning attacks that compromise model integrity. Existing defenses often rely
on external datasets or predefined heuristics (e.g. number of malicious
clients), limiting their effectiveness and scalability. To address these
limitations, we propose a privacy-preserving defense framework that leverages a
Conditional Generative Adversarial Network (cGAN) to generate synthetic data at
the server for authenticating client updates, eliminating the need for external
datasets. Our framework is scalable, adaptive, and seamlessly integrates into
FL workflows. Extensive experiments on benchmark datasets demonstrate its
robust performance against a variety of poisoning attacks, achieving high True
Positive Rate (TPR) and True Negative Rate (TNR) of malicious and benign
clients, respectively, while maintaining model accuracy. The proposed framework
offers a practical and effective solution for securing federated learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Semantic Patching for HPC-oriented Refactorings with
  Coccinelle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Martone, Julia Lawall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, the most energy-efficient hardware platforms for floating
point-intensive calculations (also known as High Performance Computing, or HPC)
are graphical processing units (GPUs). However, porting existing scientific
codes to GPUs can be far from trivial. This article summarizes our recent
advances in enabling machine-assisted, HPC-oriented refactorings with reference
to existing APIs and programming idioms available in C and C++. The tool we are
extending and using for the purpose is called Coccinelle. An important workflow
we aim to support is that of writing and maintaining tersely written
application code, while deferring circumstantial, ad-hoc, performance-related
changes to specific, separate rules called semantic patches. GPUs currently
offer very limited debugging facilities. The approach we are developing aims at
preserving intelligibility, longevity, and relatedly, debuggability of existing
code on CPUs, while at the same time enabling HPC-oriented code evolutions such
as introducing support for GPUs, in a scriptable and possibly parametric
manner. This article sketches a number of self-contained use cases, including
further HPC-oriented cases which are independent from GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NotebookOS: A Notebook Operating System for Interactive Training with
  On-Demand GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Carver, Jingyuan Zhang, Haoliang Wang, Kanak Mahadik, Yue Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive notebook programming is universal in modern ML (machine learning)
and AI (artificial intelligence) workflows. Notebook software like Jupyter and
Google Colab provides a user-friendly, interactive, web-based programming
interface and is widely used across science and engineering domains. A dominant
application of production notebook workloads is interactive deep learning
training (IDLT). To guarantee high interactivity, modern notebook platforms
typically reserve GPU resources within actively running notebook sessions.
These notebook sessions are long-running but exhibit intermittent and sporadic
GPU usage. Consequently, during most of their lifetimes, notebook sessions do
not use the reserved GPUs, resulting in extremely low GPU utilization and
prohibitively high cost.
  In this paper, we introduce NotebookOS, a GPU-efficient notebook platform
designed to meet the unique requirements of IDLT. NotebookOS uses a replicated
notebook kernel design, where each kernel consists of three replicas
distributed across separate GPU servers and synchronized via Raft. To optimize
GPU utilization, NotebookOS oversubscribes server resources via kernel
replication to leverage the relatively high task inter-arrival times in IDLT
workloads. By dynamically allocating GPUs to kernel replicas only while they
are actively executing notebook cells, NotebookOS maximizes the likelihood of
immediate and interactive training upon notebook notebook-cell task submission.
NotebookOS also migrates kernel replicas and automatically scales the GPU
cluster under overload conditions. We evaluate NotebookOS extensively using
production notebook workloads. Evaluation results show that NotebookOS saves
1,187+ GPU hours over a 17.5-hour real-world IDLT workload while greatly
enhancing interactivity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and
  Throughput via Attention Disaggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkai Liang, Zhangyu Chen, Pengfei Zuo, Zhi Zhou, Xu Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language model (LLM) serving systems, executing each request
consists of two phases: the compute-intensive prefill phase and the
memory-intensive decoding phase. To prevent performance interference between
the two phases, current LLM serving systems typically adopt prefill-decoding
disaggregation, where the two phases are split across separate machines.
However, we observe this approach leads to significant resource
underutilization. Specifically, prefill instances that are compute-intensive
suffer from low memory utilization, while decoding instances that are
memory-intensive experience low compute utilization. To address this problem,
this paper proposes Adrenaline, an attention disaggregation and offloading
mechanism designed to enhance resource utilization and performance in LLM
serving systems. Adrenaline's key innovation lies in disaggregating part of the
attention computation in the decoding phase and offloading them to prefill
instances. The memory-bound nature of decoding-phase attention computation
inherently enables an effective offloading strategy, yielding two complementary
advantages: 1) improved memory capacity and bandwidth utilization in prefill
instances, and 2) increased decoding batch sizes that enhance compute
utilization in decoding instances, collectively boosting overall system
performance. Adrenaline achieves these gains through three key techniques:
low-latency decoding synchronization, resource-efficient prefill colocation,
and load-aware offloading scheduling. Experimental results show that Adrenaline
achieves 2.28x higher memory capacity and 2.07x better memory bandwidth
utilization in prefill instances, up to 1.67x improvements in compute
utilization for decoding instances, and 1.68x higher overall inference
throughput compared to state-of-the-art systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmonia: A Multi-Agent Reinforcement Learning Approach to Data
  Placement and Migration in Hybrid Storage Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid storage systems (HSS) combine multiple storage devices with diverse
characteristics to achieve high performance and capacity at low cost. The
performance of an HSS highly depends on the effectiveness of two key policies:
(1) the data-placement policy, which determines the best-fit storage device for
incoming data, and (2) the data-migration policy, which rearranges stored data
across the devices to sustain high HSS performance. Prior works focus on
improving only data placement or only data migration in HSS, which leads to
sub-optimal HSS performance. Unfortunately, no prior work tries to optimize
both policies together. Our goal is to design a holistic data-management
technique for HSS that optimizes both data-placement and data-migration
policies to fully exploit the potential of an HSS. We propose Harmonia, a
multi-agent reinforcement learning (RL)-based data-management technique that
employs two light-weight autonomous RL agents, a data-placement agent and a
data-migration agent, which adapt their policies for the current workload and
HSS configuration, and coordinate with each other to improve overall HSS
performance. We evaluate Harmonia on a real HSS with up to four heterogeneous
storage devices with diverse characteristics. Our evaluation using 17
data-intensive workloads on performance-optimized (cost-optimized) HSS with two
storage devices shows that, on average, Harmonia (1) outperforms the
best-performing prior approach by 49.5% (31.7%), (2) bridges the performance
gap between the best-performing prior work and Oracle by 64.2% (64.3%). On an
HSS with three (four) devices, Harmonia outperforms the best-performing prior
work by 37.0% (42.0%). Harmonia's performance benefits come with low latency
(240ns for inference) and storage overheads (206 KiB for both RL agents
together). We plan to open-source Harmonia's implementation to aid future
research on HSS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Blockchain-Enabled Framework for Storage and Retrieval of Social Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Parab, Prakhar Pradhan, Yogesh Simmhan, Arnab K. Paul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing availability of data from diverse sources, including trusted
entities such as governments, as well as untrusted crowd-sourced contributors,
demands a secure and trustworthy environment for storage and retrieval.
Blockchain, as a distributed and immutable ledger, offers a promising solution
to address these challenges. This short paper studies the feasibility of a
blockchain-based framework for secure data storage and retrieval across trusted
and untrusted sources, focusing on provenance, storage mechanisms, and smart
contract security. Through initial experiments using Hyper Ledger Fabric (HLF),
we evaluate the storage efficiency, scalability, and feasibility of the
proposed approach. This study serves as a motivation for future research to
develop a comprehensive blockchain-based storage and retrieval framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoNimbus: A serverless framework to build earth observation and
  environmental services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dante D. Sánchez-Gallegos, Diana Carrizales-Espinoza, Alejandro Zequeira, Catherine Torres-Charles, J. L. Gonzalez-Compean, Jesus Carretero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud computing has become a popular solution for organizations implementing
Earth Observation Systems (EOS). However, this produces a dependency on
provider resources. Moreover, managing and executing tasks and data in these
environments are challenges that commonly arise when building an EOS. This
paper presents GeoNimbus, a serverless framework for composing and deploying
spatio-temporal EOS on multiple infrastructures, e.g., on-premise resources and
public or private clouds. This framework organizes EOS tasks as functions and
automatically manages their deployment, invocation, scalability, and monitoring
in the cloud. GeoNimbus framework enables organizations to reuse and share
available functions to compose multiple EOS. We use this framework to implement
EOS as a service for conducting a case study focused on measuring water
resource changes in a lake in the south of Mexico. The experimental evaluation
revealed the feasibility and efficiency of using GeoNimbus to build different
earth observation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 images. Presented at the 1st workshop about
  High-Performance e-Science in the EuroPar2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L4: Diagnosing Large-scale LLM Training Failures via Automated Log
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Jiang, Junjie Huang, Zhuangbin Chen, Yichen Li, Guangba Yu, Cong Feng, Yongqiang Yang, Zengyin Yang, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) show their capabilities across various
applications, training customized LLMs has become essential for modern
enterprises. However, due to the complexity of LLM training, which requires
massive computational resources and extensive training time, failures are
inevitable during the training process. These failures result in considerable
waste of resource and time, highlighting the critical need for effective and
efficient failure diagnosis to reduce the cost of LLM training.
  In this paper, we present the first empirical study on the failure reports of
428 LLM training failures in our production Platform-X between May 2023 and
April 2024. Our study reveals that hardware and user faults are the predominant
root causes, and current diagnosis processes rely heavily on training logs.
Unfortunately, existing log-based diagnostic methods fall short in handling LLM
training logs. Considering the unique features of LLM training, we identify
three distinct patterns of LLM training logs: cross-job, spatial, and temporal
patterns. We then introduce our Log-based Large-scale LLM training failure
diagnosis framework, L4, which can automatically extract failure-indicating
information (i.e., log events, nodes, stages, and iterations) from extensive
training logs, thereby reducing manual effort and facilitating failure
recovery. Experimental results on real-world datasets show that L4 outperforms
existing approaches in identifying failure-indicating logs and localizing
faulty nodes. Furthermore, L4 has been applied in Platform-X and demonstrated
its effectiveness in enabling accurate and efficient failure diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in companion proceedings of the 33rd ACM International
  Conference on the Foundations of Software Engineering (FSE'25). 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maya: Optimizing Deep Learning Training Workloads using Emulated Virtual
  Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srihas Yarlagadda, Amey Agrawal, Elton Pinto, Hakesh Darapaneni, Mitali Meratwal, Shivam Mittal, Pranavi Bajjuri, Srinivas Sridharan, Alexey Tumanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large foundation models costs hundreds of millions of dollars,
making deployment optimization critical. Current approaches require machine
learning engineers to manually craft training recipes through error-prone
trial-and-error on expensive compute clusters. To enable efficient exploration
of training configurations, researchers have developed performance modeling
systems. However, these systems force users to translate their workloads into
custom specification languages, introducing a fundamental semantic gap between
the actual workload and its representation. This gap creates an inherent
tradeoff: systems must either support a narrow set of workloads to maintain
usability, require complex specifications that limit practical adoption, or
compromise prediction accuracy with simplified models.
  We present Maya, a performance modeling system that eliminates these
tradeoffs through transparent device emulation. By operating at the narrow
interface between training frameworks and accelerator devices, Maya can capture
complete workload behavior without requiring code modifications or
translations. Maya intercepts device API calls from unmodified training code to
directly observe low-level operations, enabling accurate performance prediction
while maintaining both ease of use and generality. Our evaluation shows Maya
achieves less than 5% prediction error across diverse models and optimization
strategies, identifying configurations that reduce training costs by up to 56%
compared to existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIGC-assisted Federated Learning for Edge Intelligence: Architecture
  Design, Research Challenges and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianke Qiang, Zheng Chang, Ying-Chang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) can fully leverage large-scale terminal data while
ensuring privacy and security, and is considered as a distributed alternative
for the centralized machine learning. However, the issue of data heterogeneity
poses limitations on FL's performance. To address this challenge, artificial
intelligence-generated content (AIGC) which is an innovative data synthesis
technique emerges as one potential solution. In this article, we first provide
an overview of the system architecture, performance metrics, and challenges
associated with AIGC-assistant FL system design. We then propose the Generative
federated learning (GenFL) architecture and present its workflow, including the
design of aggregation and weight policy. Finally, using the CIFAR10 and
CIFAR100 datasets, we employ diffusion models to generate dataset and improve
FL performance. Experiments conducted under various non-independent and
identically distributed (non-IID) data distributions demonstrate the
effectiveness of GenFL on overcoming the bottlenecks in FL caused by data
heterogeneity. Open research directions in the research of AIGC-assisted FL are
also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Distributed Fog Load Balancing with Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maad Ebrahim, Abdelhakim Hafid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time Internet of Things (IoT) applications require real-time support to
handle the ever-growing demand for computing resources to process IoT
workloads. Fog Computing provides high availability of such resources in a
distributed manner. However, these resources must be efficiently managed to
distribute unpredictable traffic demands among heterogeneous Fog resources.
This paper proposes a fully distributed load-balancing solution with
Multi-Agent Reinforcement Learning (MARL) that intelligently distributes IoT
workloads to optimize the waiting time while providing fair resource
utilization in the Fog network. These agents use transfer learning for
life-long self-adaptation to dynamic changes in the environment. By leveraging
distributed decision-making, MARL agents effectively minimize the waiting time
compared to a single centralized agent solution and other baselines, enhancing
end-to-end execution delay. Besides performance gain, a fully distributed
solution allows for a global-scale implementation where agents can work
independently in small collaboration regions, leveraging nearby local
resources. Furthermore, we analyze the impact of a realistic frequency to
observe the state of the environment, unlike the unrealistic common assumption
in the literature of having observations readily available in real-time for
every required action. The findings highlight the trade-off between realism and
performance using an interval-based Gossip-based multi-casting protocol against
assuming real-time observation availability for every generated workload.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TNSM with 14 pages, 11 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN
  Inference on NVIDIA GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13996v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13996v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Zhang, Haoxuan Yu, Chenxia Han, Cheng Wang, Baotong Lu, Yunzhe Li, Zhifeng Jiang, Yang Li, Xiaowen Chu, Huaicheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud service providers heavily colocate high-priority, latency-sensitive
(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU
to improve resource utilization in data centers. Among the critical shared GPU
resources, there has been very limited analysis on the dynamic allocation of
compute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU
resource management solutions are either hardware-specific, or unable to
dynamically allocate resources to different tenants, or both; (2) NVIDIA
doesn't expose interfaces for VRAM bandwidth allocation, and the software stack
and VRAM channel architectures are black-box, both of which limit the
software-level resource management. These drive prior work to design either
conservative sharing policies detrimental to throughput, or static resource
partitioning only applicable to a few GPU models.
  To bridge this gap, this paper proposes SGDRC, a fully software-defined
dynamic VRAM bandwidth and compute unit management solution for concurrent DNN
inference services. SGDRC aims at guaranteeing service quality, maximizing the
overall throughput, and providing general applicability to NVIDIA GPUs. SGDRC
first reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs
through comprehensive reverse engineering and eliminates VRAM channel conflicts
using software-level cache coloring. SGDRC applies bimodal tensors and tidal SM
masking to dynamically allocate VRAM bandwidth and compute units, and guides
the allocation of resources based on offline profiling. We evaluate 11
mainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show
that compared with the state-of-the-art GPU sharing solutions, SGDRC achieves
the highest SLO attainment rates (99.0% on average), and improves overall
throughput by up to 1.47x and BE job throughput by up to 2.36x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via
  Speculative Inference Filling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02550v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02550v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunchi Lv, Xiao Shi, Dong Liang, Wenting Tan, Xiaofang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL), especially with Large Language Models (LLMs), brings
benefits to various areas. However, DL training systems usually yield prominent
idling GPU resources due to many factors, such as resource allocation and
collective communication. To improve GPU utilization, we present SpecInF, which
adopts a Speculative Inference Filling method to exploit idle GPU resources. It
collocates each primary training instance with additional inference instances
on the same GPU, detects the training bubbles and adaptively fills with online
or offline inference workloads. Our results show that SpecInF can effectively
enhance GPU utilization under mainstream parallel training modes, delivering
additional up to 14$\times$ offline inference throughputs than TGS and 67\%
reduction in online inference p95 latency than MPS, while guaranteeing
collocated training throughput.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consensus Capacity of Noisy Broadcast Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06073v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06073v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Sangwan, Varun Narayanan, Vinod M. Prabhakaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study communication with consensus over a broadcast channel - the
receivers reliably decode the sender's message when the sender is honest, and
their decoder outputs agree even if the sender acts maliciously. We
characterize the broadcast channels which permit this byzantine consensus and
determine their capacity. We show that communication with consensus is possible
only when the broadcast channel has embedded in it a natural ''common channel''
whose output both receivers can unambiguously determine from their own channel
outputs. Interestingly, in general, the consensus capacity may be larger than
the point-to-point capacity of the common channel, i.e., while decoding, the
receivers may make use of parts of their output signals on which they may not
have consensus provided there are some parts (namely, the common channel
output) on which they can agree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Accelerated Distributed Stochastic Gradient Method with Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09714v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09714v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Huang, Shi Pu, Angelia Nedić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an accelerated distributed stochastic gradient
method with momentum for solving the distributed optimization problem, where a
group of $n$ agents collaboratively minimize the average of the local objective
functions over a connected network. The method, termed ``Distributed Stochastic
Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the
momentum tracking technique as well as the Loopless Chebyshev Acceleration
(LCA) method. We show that DSMT can asymptotically achieve comparable
convergence rates as centralized stochastic gradient descent (SGD) method under
a general variance condition regarding the stochastic gradients. Moreover, the
number of iterations (transient times) required for DSMT to achieve such rates
behaves as $\mathcal{O}(n^{5/3}/(1-\lambda))$ for minimizing general smooth
objective functions, and $\mathcal{O}(\sqrt{n/(1-\lambda)})$ under the
Polyak-{\L}ojasiewicz (PL) condition. Here, the term $1-\lambda$ denotes the
spectral gap of the mixing matrix related to the underlying network topology.
Notably, the obtained results do not rely on multiple inter-node communications
or stochastic gradient accumulation per iteration, and the transient times are
the shortest under the setting to the best of our knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medha: Efficiently Serving Multi-Million Context Length LLM Inference
  Requests Without Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie Zhang, Alexey Tumanov, Esha Choukse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) handle increasingly longer contexts, serving
inference requests for context lengths in the range of millions of tokens
presents unique challenges. While existing techniques are effective for
training, they fail to address the unique challenges of inference, such as
varying prefill and decode phases and their associated latency constraints --
like Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,
no long-context inference solutions address head-of-line blocking today.
  We present Medha, a system for efficient long-context LLM inference that
introduces three key innovations: adaptive chunking with slack-aware scheduling
to prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce
TTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into
a novel 3D parallelism serving engine, Medha achieves unprecedented scale --
supporting contexts up to 10M tokens with production-grade latency. Our
evaluation shows Medha reduces median latency by up to 30x compared to
state-of-the-art systems when serving a mix of short and long requests, while
improving throughput by upwards of 5x. This enables, for the first time,
efficient long-context LLM inference at scale without compromising on shorter
request latencies or system efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Some new methods <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Art of Tool Interface Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunnan Wu, Paul Chen, Deshank Baranwal, Jinlong Zhou, Jian Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an agentic framework, Thinker, which achieves state of art
performance in challenging reasoning tasks for realistic customer service
scenarios that involve complex business logic and human interactions via long
horizons. On the $\tau$-bench retail dataset, Thinker achieves 82.6\% success
rate with GPT-4o (version 2024-06-01) (baseline: 68.3\%), and 81.9\% success
rate with Llama-3.1 405B (baseline: 49.6\%), without any fine-tuning. Thinker
effectively closes the gap in reasoning capabilities between the base models by
introducing proper structure.
  The key features of the Thinker framework are: (1) State-Machine Augmented
Generation (SMAG), which represents business logic as state machines and the
LLM uses state machines as tools. (2) Delegation of tasks from the main
reasoning loop to LLM-powered tools. (3) Adaptive context management.
  Our prompting-only solution achieves signficant gains, while still
maintaining a standard agentic architecture with a ReAct style reasoning loop.
The key is to innovate on the tool interface design, as exemplified by SMAG and
the LLM-powered tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do We Need to Verify Step by Step? Rethinking Process Supervision from a
  Theoretical Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Jia, Alexander Rakhlin, Tengyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models have evolved, it has become crucial to distinguish
between process supervision and outcome supervision -- two key reinforcement
learning approaches to complex reasoning tasks. While process supervision
offers intuitive advantages for long-term credit assignment, the precise
relationship between these paradigms has remained an open question.
Conventional wisdom suggests that outcome supervision is fundamentally more
challenging due to the trajectory-level coverage problem, leading to
significant investment in collecting fine-grained process supervision data.
  In this paper, we take steps towards resolving this debate. Our main theorem
shows that, under standard data coverage assumptions, reinforcement learning
through outcome supervision is no more statistically difficult than through
process supervision, up to polynomial factors in horizon. At the core of this
result lies the novel Change of Trajectory Measure Lemma -- a technical tool
that bridges return-based trajectory measure and step-level distribution shift.
Furthermore, for settings with access to a verifier or a rollout capability, we
prove that any policy's advantage function can serve as an optimal process
reward model, providing a direct connection between outcome and process
supervision. These findings suggest that the empirically observed performance
gap -- if any -- between outcome and process supervision likely stems from
algorithmic limitations rather than inherent statistical difficulties,
potentially transforming how we approach data collection and algorithm design
for reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-25T00:00:00Z">2025-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Interpretation to Correction: A Decentralized Optimization
  Framework for Exact Convergence in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bicheng Ying, Zhe Li, Haibo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a novel decentralized framework to interpret federated
learning (FL) and, consequently, correct the biases introduced by arbitrary
client participation and data heterogeneity, which are two typical traits in
practical FL. Specifically, we first reformulate the core processes of FedAvg -
client participation, local updating, and model aggregation - as stochastic
matrix multiplications. This reformulation allows us to interpret FedAvg as a
decentralized algorithm. Leveraging the decentralized optimization framework,
we are able to provide a concise analysis to quantify the impact of arbitrary
client participation and data heterogeneity on FedAvg's convergence point. This
insight motivates the development of Federated Optimization with Exact
Convergence via Push-pull Strategy (FOCUS), a novel algorithm inspired by the
decentralized algorithm that eliminates these biases and achieves exact
convergence without requiring the bounded heterogeneity assumption.
Furthermore, we theoretically prove that FOCUS exhibits linear convergence
(exponential decay) for both strongly convex and non-convex functions
satisfying the Polyak-Lojasiewicz condition, regardless of the arbitrary nature
of client participation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARGO-SLSA: Software Supply Chain Security in Argo Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohomed Thariq, Indrajith Ekanayake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed systems widely adopt microservice architecture to handle growing
complexity and scale. This approach breaks applications into independent,
loosely coupled services. Kubernetes has become the de facto standard for
managing microservices, and automating complex, multi-step workflows is a
common requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine
for managing these workflows in an automated fashion. These workflows generate
artifacts such as executables, logs, container images, and packages, which
often require proper management through software supply chain security.
However, Argo Workflows does not include built-in functionality for frameworks
like Supply-chain Levels for Software Artifacts (SLSA), which is essential for
ensuring artifact integrity, traceability, and security. This gap compels
practitioners to rely on external tools to meet software supply chain security
standards. In response, this paper proposes a Kubernetes-native controller
built on top of existing open-source Argo Workflows to enhance artifact
security. By generating cryptographic signing and provenance attestations, the
controller enables Argo Workflows to comply with SLSA standards. We demonstrate
that implementations can provide such cryptographic signing and provenance
attestations for artifacts produced by the controller, allowing software
artifacts built with Argo Workflows to adhere to SLSA requirements. The
proposed validation model evaluates the proof of concept of the controller,
including its ability to reconcile workflows, detect pods associated with
workflow nodes, operate without disrupting existing operations, enforce
integrity, and monitor software artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmoneam Ali, Ahmed Arafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of cluster identity estimation in a personalized
federated learning (PFL) setting in which users aim to learn different personal
models. The backbone of effective learning in such a setting is to cluster
users into groups whose objectives are similar. A typical approach in the
literature is to achieve this by training users' data on different proposed
personal models and assign them to groups based on which model achieves the
lowest value of the users' loss functions. This process is to be done
iteratively until group identities converge. A key challenge in such a setting
arises when users have noisy labeled data, which may produce misleading values
of their loss functions, and hence lead to ineffective clustering. To overcome
this challenge, we propose a label-agnostic data similarity-based clustering
algorithm, coined RCC-PFL, with three main advantages: the cluster identity
estimation procedure is independent from the training labels; it is a one-shot
clustering algorithm performed prior to the training; and it requires fewer
communication rounds and less computation compared to iterative-based
clustering methods. We validate our proposed algorithm using various models and
datasets and show that it outperforms multiple baselines in terms of average
accuracy and variance reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in the 2025 IEEE International Conference on Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing the Run-time Behavior of Modern PDES Engines on Alternative
  Hardware Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romolo Marotta, Francesco Quaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current trend of technology has brought parallel machines equipped with
multiple processors and multiple memory sockets to be available off-the-shelf
-- or via renting through Iaas Clouds -- at reasonable costs. This has opened
the possibility of natively supporting HPC in diffused realities, like industry
or academic labs. At the same time, the Parallel Discrete Event Simulation
(PDES) area has given rise to attractive simulation engines, designed with
orientation to high performance and scalability, also targeting differentiated
exploitation of the specific support offered by the underlying hardware. In
this article, we present an experimental study where we deploy two
last-generation open-source PDES platforms -- one optimistic (USE) and one
conservative (PARSIR) -- on top of two significantly different hardware
chipsets based on either {\sf x86} CISC or {\sf powerPC} RISC technology, both
offering multiple Non-Uniform-Memory-Access (NUMA) nodes and multiple tens of
cores and hardware-threads (logical CPUs). Also, we consider real-world
simulation models configured in a variety of different manners in order to
investigate the actual execution profile of the PDES engines on the two
distinct hardware platforms. Our objective is the one of providing insights on
current performance trends, which can support decisions in terms of both
strategies -- for software platforms to adopt -- and investments -- in terms of
hardware platforms -- in the area of discrete event simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIGC-assisted Federated Learning for Vehicular Edge Intelligence:
  Vehicle Selection, Resource Allocation and Model Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianke Qiang, Zheng Chang, Geyong Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To leverage the vast amounts of onboard data while ensuring privacy and
security, federated learning (FL) is emerging as a promising technology for
supporting a wide range of vehicular applications. Although FL has great
potential to improve the architecture of intelligent vehicular networks,
challenges arise due to vehicle mobility, wireless channel instability, and
data heterogeneity. To mitigate the issue of heterogeneous data across
vehicles, artificial intelligence-generated content (AIGC) can be employed as
an innovative data synthesis technique to enhance FL model performance. In this
paper, we propose AIGC-assisted Federated Learning for Vehicular Edge
Intelligence (GenFV). We further propose a weighted policy using the Earth
Mover's Distance (EMD) to quantify data distribution heterogeneity and
introduce a convergence analysis for GenFV. Subsequently, we analyze system
delay and formulate a mixed-integer nonlinear programming (MINLP) problem to
minimize system delay. To solve this MINLP NP-hard problem, we propose a
two-scale algorithm. At large communication scale, we implement label sharing
and vehicle selection based on velocity and data heterogeneity. At the small
computation scale, we optimally allocate bandwidth, transmission power and
amount of generated data. Extensive experiments show that GenFV significantly
improves the performance and robustness of FL in dynamic, resource-constrained
environments, outperforming other schemes and confirming the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tight Meta-theorem for LOCAL Certification of MSO$_2$ Properties
  within Bounded Treewidth Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linda Cook, Eun Jung Kim, Tomáš Masařík
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed networks are prone to errors so verifying their output is
critical. Hence, we develop LOCAL certification protocols for graph properties
in which nodes are given certificates that allow them to check whether their
network as a whole satisfies some fixed property while only communicating with
their local network. Most known LOCAL certification protocols are specifically
tailored to the problem they work on and cannot be translated more generally.
Thus we target general protocols that can certify any property expressible
within a certain logical framework. We consider Monadic Second Order Logic
(MSO$_2$), a powerful framework that can express properties such as
non-$k$-colorability, Hamiltonicity, and $H$-minor-freeness. Unfortunately, in
general, there are MSO$_2$-expressible properties that cannot be certified
without huge certificates. For instance, non-3-colorability requires
certificates of size $\Omega(n^2/\log n)$ on general $n$-vertex graphs
(G\"o\"os, Suomela 2016). Hence, we impose additional structural restrictions
on the graph.
  We provide a LOCAL certification protocol for certifying any
MSO$_2$-expressible property on graphs of bounded treewidth and, consequently,
a LOCAL certification protocol for certifying bounded treewidth. That is for
each integer $k$ and each MSO$_2$-expressible property $\Pi$ we give a LOCAL
Certification protocol to certify that a graph satisfies $\Pi$ and has
treewidth at most $k$ using certificates of size $\mathcal{O}(\log n)$ (which
is asymptotically optimal). Our LOCAL certification protocol requires only one
round of distributed communication, hence it is also proof-labeling scheme.
  Our result improves upon work by Fraigniaud, Montealegre, Rapaport, and
Todinca (Algorithmica 2024), Bousquet, Feuilloley, Pierron (PODC 2022), and the
very recent work of Baterisna and Chang.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness in Proof of Team Sprint (PoTS): Evaluating Reward Distribution
  Across Performance Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Yonezawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain consensus mechanisms must balance security, decentralization, and
efficiency while ensuring fair participation. Proof of Team Sprint (PoTS) is a
cooperative consensus mechanism designed to address the energy inefficiencies
and centralization tendencies of traditional Proof of Work (PoW). Unlike PoW,
where rewards disproportionately favor high-performance nodes, PoTS encourages
collaboration by forming teams and distributing rewards more equitably among
participants. In this study, we evaluate the fairness properties of PoTS by
analyzing reward distribution under varying computational power distributions.
Through extensive simulations, we compare equal-share allocation and
proportional reward allocation, highlighting their impact on decentralization
and participation. Our results demonstrate that PoTS significantly reduces
reward disparity between high-performance and low-performance nodes, fostering
a more inclusive ecosystem. Additionally, we observe that as team sizes
increase, the influence of individual computational power is mitigated,
allowing lower-performance nodes to contribute meaningfully. Moreover, our
findings reveal that the marginal benefit of investing in extremely
high-performance hardware diminishes, which discourages centralization and
aligns incentives toward sustainable participation. We also discuss the
economic implications of PoTS, particularly its potential to reshape blockchain
mining strategies by balancing fairness with computational efficiency. These
insights contribute to the broader discussion on blockchain fairness and
provide a foundation for further research into cooperative consensus
mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness of Proof of Team Sprint (PoTS) Against Attacks: A
  Simulation-Based Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Yonezawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the robustness of Proof of Team Sprint (PoTS) against
adversarial attacks through simulations, focusing on the attacker win rate and
computational efficiency under varying team sizes (\( N \)) and attacker ratios
(\( \alpha \)). Our results demonstrate that PoTS effectively reduces an
attacker's ability to dominate the consensus process. For instance, when \(
\alpha = 0.5 \), the attacker win rate decreases from 50.7\% at \( N = 1 \) to
below 0.4\% at \( N = 8 \), effectively neutralizing adversarial influence.
Similarly, at \( \alpha = 0.8 \), the attacker win rate drops from 80.47\% at
\( N = 1 \) to only 2.79\% at \( N = 16 \). In addition to its strong security
properties, PoTS maintains high computational efficiency. We introduce the
concept of Normalized Computation Efficiency (NCE) to quantify this efficiency
gain, showing that PoTS significantly improves resource utilization as team
size increases. The results indicate that as \( N \) grows, PoTS not only
enhances security but also achieves better computational efficiency due to the
averaging effects of execution time variations. These findings highlight PoTS
as a promising alternative to traditional consensus mechanisms, offering both
robust security and efficient resource utilization. By leveraging team-based
block generation and randomized participant reassignment, PoTS provides a
scalable and resilient approach to decentralized consensus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Evaluation and Scalability Analysis of Proof of Team Sprint
  (PoTS): Reward Fairness, Energy Efficiency, and System Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Yonezawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an empirical evaluation of the Proof of Team Sprint
(PoTS) consensus algorithm, focusing on reward fairness, energy efficiency,
system stability, and scalability. We conducted large-scale simulations
comparing PoTS with conventional Proof of Work (PoW) across various team sizes
and computational conditions. In PoW, the highest-performance node ranked first
in all 100 trials, demonstrating extreme centralization. In contrast, PoTS
reduced this dominance: the same node ranked first only 54 times, indicating
fairer reward distribution. Statistical analysis showed that as team size
increased, skewness and kurtosis of reward distributions decreased, confirming
improved equity among participants. PoTS also demonstrated significant energy
savings. The total active computation time followed a near $1/N$ scaling trend,
reducing energy use by up to 64 times when team size was 64, while preserving
consensus integrity. Repeated simulations showed stable reward distributions
and system performance, affirming PoTS's robustness. Furthermore, the
correlation between performance and reward peaked at 0.90 for team size 16,
reflecting an optimal balance between fairness and meritocracy. Overall, PoTS
offers a cooperative, energy-efficient alternative to PoW, mitigating
centralization risks and promoting equitable participation. These findings
validate PoTS as a sustainable and fair consensus mechanism suited for future
blockchain systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOCO: Rethinking Objects for Network Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Hodgkins, Mark Madler, Joseph Izraelevitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore an object-based programming model for filling the
space between shared memory and distributed systems programming. We argue that
the natural representation for resources distributed across a memory network
(e.g. RDMA or CXL) is the traditional shared memory object. This concurrent
object (which we call a "channel" object) exports traditional methods, but,
especially in an incoherent or uncacheable memory network, stores its state in
a distributed fashion across all participating nodes. In a sense, the channel
object's state is stored "across the network".
  Based on this philosophy, we introduce the Library of Channel Objects (LOCO),
a library for building multi-node objects on RDMA. Channel objects are
composable and designed for both the strong locality effects and the weak
consistency of RDMA. Unlike prior work, channel objects do not hide memory
complexity, instead relying on the programmer to use NUMA-like techniques to
explicitly manage each object. As a consequence, our channel objects have
performance similar to custom RDMA systems (e.g. distributed maps), but with a
far simpler programming model. Our distributed map channel has better read and
comparable write performance to a state-of-the-art custom RDMA solution, using
well-encapsulated and reusable primitives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedECA: A Federated External Control Arm Method for Causal Inference
  with Time-To-Event Data in Distributed Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16984v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16984v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Taïeb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, Jérome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, Félix Balazard, Mathieu Andreux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  External control arms (ECA) can inform the early clinical development of
experimental drugs and provide efficacy evidence for regulatory approval.
However, the main challenge in implementing ECA lies in accessing real-world or
historical clinical trials data. Indeed, regulations protecting patients'
rights by strictly controlling data processing make pooling data from multiple
sources in a central server often difficult. To address these limitations, we
develop a new method, 'FedECA' that leverages federated learning (FL) to enable
inverse probability of treatment weighting (IPTW) for time-to-event outcomes on
separate cohorts without needing to pool data. To showcase the potential of
FedECA, we apply it in different settings of increasing complexity culminating
with a real-world use-case in which FedECA provides evidence for a differential
effect between two drugs that would have otherwise gone unnoticed. By sharing
our code, we hope FedECA will foster the creation of federated research
networks and thus accelerate drug development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at: https://github.com/owkin/fedeca, bug in SMD
  computation present in v1 and v2 fixed, many experiments on real data added +
  fix in YODA experiments using imputed data instead of raw data (v3->v4) +
  affiliations fix + more precise wording for acknowledgments, real-world
  experiment results fixed by excluding data with bias + text polished (v5->v6)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs <span class="chip">EuroSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05215v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05215v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhe Yao, Qinghao Hu, Ana Klimovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) greatly improves model quality for
downstream tasks. However, serving many fine-tuned LLMs concurrently is
challenging due to the sporadic, bursty, and varying request patterns of
different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system
that efficiently serves multiple full-parameter fine-tuned models concurrently
by aggressively compressing model deltas by up to 10x while maintaining high
model quality. The key insight behind this design is that fine-tuning results
in small-magnitude changes to the pre-trained model. By co-designing the
serving system with the compression algorithm, DeltaZip achieves 2x to 12x
improvement in throughput compared to the state-of-the-art systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EuroSys 2025'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeBS-Flow: Benchmarking Serverless Cloud Function Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03480v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03480v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa Schmid, Marcin Copik, Alexandru Calotoiu, Laurin Brandner, Anne Koziolek, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing has emerged as a prominent paradigm, with a significant
adoption rate among cloud customers. While this model offers advantages such as
abstraction from the deployment and resource scheduling, it also poses
limitations in handling complex use cases due to the restricted nature of
individual functions. Serverless workflows address this limitation by
orchestrating multiple functions into a cohesive application. However, existing
serverless workflow platforms exhibit significant differences in their
programming models and infrastructure, making fair and consistent performance
evaluations difficult in practice. To address this gap, we propose the first
serverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic
workflow model that enables consistent benchmarking across various platforms.
SeBS-Flow includes six real-world application benchmarks and four
microbenchmarks representing different computational patterns. We conduct
comprehensive evaluations on three major cloud platforms, assessing
performance, cost, scalability, and runtime deviations. We make our benchmark
suite open-source, enabling rigorous and comparable evaluations of serverless
workflows over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Blockchain Throughput: Parallel EVM Execution with Asynchronous
  Storage for Reddio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Qi, Xinran Chen,  Asiy, Neil Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing adoption of blockchain technology has led to a growing demand
for higher transaction throughput. Traditional blockchain platforms, such as
Ethereum, execute transactions sequentially within each block, limiting
scalability. Parallel execution has been proposed to enhance performance, but
existing approaches either impose strict dependency annotations, rely on
conservative static analysis, or suffer from high contention due to inefficient
state management. Moreover, even when transaction execution is parallelized at
the upper layer, storage operations remain a bottleneck due to sequential state
access and I/O amplification. In this paper, we propose Reddio, a batch-based
parallel transaction execution framework with asynchronous storage. Reddio
processes transactions in parallel while addressing the storage bottleneck
through three key techniques: (i) direct state reading, which enables efficient
state access without traversing the Merkle Patricia Trie (MPT); (ii)
asynchronous parallel node loading, which preloads trie nodes concurrently with
execution to reduce I/O overhead; and (iii) pipelined workflow, which decouples
execution, state reading, and storage updates into overlapping phases to
maximize hardware utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monte Cimone v2: Down the Road of RISC-V High-Performance Computers <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Venieri, Simone Manoni, Giacomo Madella, Federico Ficarelli, Daniele Gregori, Daniele Cesarini, Luca Benini, Andrea Bartolini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many RISC-V platforms and SoCs have been announced in recent years targeting
the HPC sector, but only a few of them are commercially available and
engineered to fit the HPC requirements. The Monte Cimone project targeted
assessing their capabilities and maturity, aiming to make RISC-V a competitive
choice when building a datacenter. Nowadays, RV SoCs with vector extension,
form factor and memory capacity suitable for HPC applications are available in
the market, but it is unclear how compilers and open-source libraries can take
advantage of its performance. In this paper, we describe the performance
assessment of the upgrade of the Monte Cimone (MCv2) cluster with the Sophgo
SG2042 processor's HPC operations. The upgrade increases the attained node's
performance by 127x on HPL DP FLOP/s and 69x on Stream Memory Bandwidth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RISC-V Summit Europe 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on
  Edge <span class="chip">EuroSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Large Language Models (LLMs) on edge devices is
increasingly important to enhance on-device intelligence. Weight quantization
is crucial for reducing the memory footprint of LLMs on devices. However,
low-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low
precision weights and high precision activations during inference. Existing
systems, lacking native support for mpGEMM, resort to dequantize weights for
high precision computation. Such an indirect way can lead to a significant
inference overhead.
  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based
method designed for efficient low-bit LLM (i.e., weight-quantized LLM)
inference on CPUs. T-MAC directly supports mpGEMM without dequantization, while
simultaneously eliminating multiplications and reducing additions required.
Specifically, T-MAC transforms the traditional data-type-centric multiplication
to bit-wise table lookup, and enables a unified and scalable mpGEMM solution.
  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on
low-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in
throughput and 70% reduction in energy consumption compared to llama.cpp. For
BitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s
with a single core and 71 tokens/s with eight cores on M2-Ultra, and 11
tokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds
the adult average reading speed. T-MAC with LUT-based computing paradigm, paves
the way for the practical deployment of low-bit LLMs on resource-constrained
edge devices without compromising computational efficiency. The system is
open-sourced at https://github.com/microsoft/T-MAC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EuroSys 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Distributed Algorithms for Shape Reduction via Reconfigurable
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nada Almalki, Siddharth Gupta, Othon Michail, Andreas Padalkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of efficiently reducing geometric shapes
into other such shapes in a distributed setting through size-changing
operations. We develop distributed algorithms using the reconfigurable circuit
model to enable fast node-to-node communication. Our study considers two graph
update models: the connectivity model and the adjacency model. Let $n$ denote
the number of nodes and $k$ the number of turning points in the initial shape.
In the connectivity model, we show that the system of nodes can reduce itself
from any tree to a single node using only shrinking operations in $O(k \log n)$
rounds w.h.p. and any tree to its minimal (incompressible) form in $O(\log n)$
rounds with additional knowledge or $O(k \log n)$ without, w.h.p. We also give
an algorithm to transform any tree to any topologically equivalent tree in $O(k
\log n+\log^2 n)$ rounds w.h.p. if both shrinking and growth operations are
available to the nodes. On the negative side, we show that one cannot hope for
$o(\log^2 n)$-round transformations for all shapes of $O(\log n)$ turning
points: for all reasonable values of $k$, there exists a pair of geometrically
equivalent paths of $k$ turning points each, such that $\Omega(k\log n)$ rounds
are required to reduce one to the other. In the adjacency model, we show that
the system can reduce itself from any connected shape to a single node using
only shrinking in $O(\log n)$ rounds w.h.p.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Datacenter Environmental Sustainability Using Carbon
  Depreciation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixin Ji, Zhuoping Yang, Alex K. Jones, Peipei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing need for increasingly capable computing resources to be
available on-demand has led to the prosperity of data centers. These data
centers have led to several challenges and opportunities to address the
environmental impacts from this computing resource. Conventional thinking has
been concerned with minimizing energy usage of data centers to address
sustainability. However, due to energy efficiency trends and renewable energy
integration, recent evidence has demonstrated that embodied carbon is
increasingly important and calls for improvements in data center provisioning
strategies. In this paper we propose to adopt carbon depreciation models to
better encourage the longer lifetime of hardware in the data center. Carbon
depreciation models apply a higher proportion of embodied carbon to newly
provisioned servers. This promotes provisioning fewer new servers to service
jobs only with strict quality-of-service (QoS) constraints and extending
lifetime of existing servers whose embodied carbon has already been mostly
recovered. Along with carbon depreciation, we make the case that both embodied
and operational carbon from server idle time must also be recovered during
active jobs. This promotes provisioning strategies that maintain high rates of
utilization. We show that prior carbon accounting strategies are
counterproductive for sustainability with a greedy job scheduler that attempts
to minimize carbon under QoS constraints as they price jobs as 25% cheaper on
new versus old hardware. Our approach uses a greedy scheduler that prefers
older hardware due to non-linear carbon depreciation promoting sustainable
provisioning. Our approach reduces carbon by between 28--57% depending on
assumptions for server lifetimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-24T00:00:00Z">2025-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COoL-TEE: Client-TEE Collaboration for Resilient Distributed Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Bettinger, Etienne Rivière, Sonia Ben Mokhtar, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current marketplaces rely on search mechanisms with distributed systems but
centralized governance, making them vulnerable to attacks, failures, censorship
and biases. While search mechanisms with more decentralized governance (e.g.,
DeSearch) have been recently proposed, these are still exposed to information
head-start attacks (IHS) despite the use of Trusted Execution Environments
(TEEs). These attacks allow malicious users to gain a head-start over other
users for the discovery of new assets in the market, which give them an unfair
advantage in asset acquisition. We propose COoL-TEE, a TEE-based provider
selection mechanism for distributed search, running in single- or
multi-datacenter environments, that is resilient to information head-start
attacks. COoL-TEE relies on a Client-TEE collaboration, which enables clients
to distinguish between slow providers and malicious ones. Performance
evaluations in single- and multi-datacenter environments show that, using
COoL-TEE, malicious users respectively gain only up to 2% and 7% of assets more
than without IHS, while they can claim 20% or more on top of their fair share
in the same conditions with DeSearch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliability is Blind: Collective Incentives for Decentralized Computing
  Marketplaces without Individual Behavior Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Mont, Matthieu Bettinger, Sonia Ben Mokhtar, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decentralized cloud computing marketplaces, ensuring fair and efficient
interactions among asset providers and end-users is crucial. A key concern is
meeting agreed-upon service-level objectives like the service's reliability. In
this decentralized context, traditional mechanisms often fail to address the
complexity of task failures, due to limited available and trustworthy insights
into these independent actors' individual behavior. This paper proposes a
collective incentive mechanism that blindly punishes all involved parties when
a task fails. Based on ruin theory, we show that Collective Incentives improve
behavior in the marketplace by creating a disincentive for faults and
misbehavior even when the parties at fault are unknown, in turn leading to a
more robust marketplace. Simulations for small and large pools of marketplace
assets show that Collective Incentives enable to meet or exceed a reliability
target, i.e., the success-rate of tasks run using marketplace assets, by
eventually discarding failure-prone assets while preserving reliable ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mist: Efficient Distributed Training of Large Language Models via
  Memory-Parallelism Co-Optimization <span class="chip">EuroSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanda Zhu, Christina Giannoula, Muralidhar Andoorveedu, Qidong Su, Karttikeya Mangalam, Bojian Zheng, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various parallelism, such as data, tensor, and pipeline parallelism, along
with memory optimizations like activation checkpointing, redundancy
elimination, and offloading, have been proposed to accelerate distributed
training for Large Language Models. To find the best combination of these
techniques, automatic distributed training systems are proposed. However,
existing systems only tune a subset of optimizations, due to the lack of
overlap awareness, inability to navigate the vast search space, and ignoring
the inter-microbatch imbalance, leading to sub-optimal performance. To address
these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware
automatic distributed training system that comprehensively co-optimizes all
memory footprint reduction techniques alongside parallelism. Mist is based on
three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating
optimizations in an overlapped manner, (2) symbolic-based performance analysis
that predicts runtime and memory usage using symbolic expressions for fast
tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process
into an inter-stage imbalance and overlap aware Mixed Integer Linear
Programming problem and an intra-stage Dual-Objective Constrained Optimization
problem, and connecting them through Pareto frontier sampling. Our evaluation
results show that Mist achieves an average of 1.28$\times$ (up to 1.73$\times$)
and 1.27$\times$ (up to 2.04$\times$) speedup compared to state-of-the-art
manual system Megatron-LM and state-of-the-art automatic system Aceso,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EuroSys 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Õptimal Fault-Tolerant Labeling for Reachability and Approximate
  Distances in Directed Planar Graphs <span class="chip">STOC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Boneh, Shiri Chechik, Shay Golan, Shay Mozes, Oren Weimann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a labeling scheme that assigns labels of size $\tilde O(1)$ to the
vertices of a directed weighted planar graph $G$, such that for any fixed
$\varepsilon>0$ from the labels of any three vertices $s$, $t$ and $f$ one can
determine in $\tilde O(1)$ time a $(1+\varepsilon)$-approximation of the
$s$-to-$t$ distance in the graph $G\setminus\{f\}$. For approximate distance
queries, prior to our work, no efficient solution existed, not even in the
centralized oracle setting. Even for the easier case of reachability, $\tilde
O(1)$ queries were known only with a centralized oracle of size $\tilde O(n)$
[SODA 21].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in STOC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AES-SpMM: Balancing Accuracy and Speed by Adaptive Edge Sampling
  Strategy to Accelerate SpMM in GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingchen Song, Yaobin Wang, Yi Luo, Huan Wu, Pingping Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinating the design of sampling and sparse-dense matrix multiplication
(SpMM) is crucial for accelerating graph neural networks (GNNs). However, due
to irrational sampling strategies, existing methods face a trade-off between
accuracy and speed. Moreover, as computational optimizations progress, data
loading has gradually become the primary bottleneck in GNN inference. To
address these issues, we propose AES-SpMM, an adaptive edge sampling SpMM
kernel. It considers the relationship between the number of non-zero elements
in each matrix row and the shared memory width. The edge sampling scheme is
adaptively selected according to the different situations of each row. AES-SpMM
reduces the graph size through adaptive edge sampling to fit the GPU's shared
memory, lowering the computational cost and enhancing data locality, thus
balancing the accuracy and speed of GNN inference. Additionally, we introduce a
quantization-based AES-SpMM, which applies quantization and dequantization to
feature data in GNNs. This approach significantly reduces data loading time
while keeping accuracy loss negligible. We evaluated AES-SpMM with common GNN
models and datasets. The results show that AES-SpMM outperforms both the
cuSPARSE SpMM kernel and GE-SpMM by up to 25.87 times and 23.01 times,
respectively, with less than 1% accuracy loss. Compared to ES-SpMM, it reduces
accuracy loss by 3.4% on average , achieving a 1.31 times speedup. Compared to
AES-SpMM, quantization-based AES-SpMM has a maximum accuracy loss of 0.3% and
feature data loading time overhead is reduced by 50.91%-70.51%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ED-DAO: Energy Donation Algorithms based on Decentralized Autonomous
  Organization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulrezzak Zekiye, Ouns Bouachir, Öznur Özkasap, Moayad Aloqaily
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy is a fundamental component of modern life, driving nearly all aspects
of daily activities. As such, the inability to access energy when needed is a
significant issue that requires innovative solutions. In this paper, we propose
ED-DAO, a novel fully transparent and community-driven decentralized autonomous
organization (DAO) designed to facilitate energy donations. We analyze the
energy donation process by exploring various approaches and categorizing them
based on both the source of donated energy and funding origins. We propose a
novel Hybrid Energy Donation (HED) algorithm, which enables contributions from
both external and internal donors. External donations are payments sourced from
entities such as charities and organizations, where energy is sourced from the
utility grid and prosumers. Internal donations, on the other hand, come from
peer contributors with surplus energy. HED prioritizes donations in the
following sequence: peer-sourced energy (P2D), utilitygrid-sourced energy
(UG2D), and direct energy donations by peers (P2PD). By merging these donation
approaches, the HED algorithm increases the volume of donated energy, providing
a more effective means to address energy poverty. Experiments were conducted on
a dataset to evaluate the effectiveness of the proposed method. The results
showed that HED increased the total donated energy by at least 0.43% (64
megawatts) compared to the other algorithms (UG2D, P2D, and P2PD).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, and 4 tables. Accepted for publication in IEEE
  International Conference on Communications (IEEE ICC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jenga: Effective Memory Management for Serving LLM with Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Kuntai Du, Shu Liu, Woosuk Kwon, Xiangxi Mo, Yufeng Wang, Xiaoxuan Liu, Kaichao You, Zhuohan Li, Mingsheng Long, Jidong Zhai, Joseph Gonzalez, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used but expensive to run, especially
as inference workloads grow. To lower costs, maximizing the request batch size
by managing GPU memory efficiently is crucial. While PagedAttention has
recently been proposed to improve the efficiency of memory management, we find
that the growing heterogeneity in the embeddings dimensions, attention, and
access patterns of modern LLM architectures introduces new challenges for
memory allocation.
  In this paper, we present Jenga, a novel memory allocation framework for
heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)
minimizing memory fragmentation when managing embeddings of different sizes,
and (2) enabling flexible caching and eviction policies tailored to the
specific token-dependency patterns of various layers. Jenga employs a two-level
memory allocator, leveraging the least common multiple (LCM) of embedding sizes
to optimize memory usage and providing APIs to express layer-specific caching
logic to enhance memory reuse.
  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and
evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations
show that Jenga improves GPU memory utilization by up to 79.6%, and increases
serving throughput by up to 4.92x (1.80x on average).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk Management for Distributed Arbitrage Systems: Integrating
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akaash Vishal Hazarika, Mahak Shah, Swapnil Patil, Pradyumna Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective risk management solutions become absolutely crucial when financial
markets embrace distributed technology and decentralized financing (DeFi). This
study offers a thorough survey and comparative analysis of the integration of
artificial intelligence (AI) in risk management for distributed arbitrage
systems. We examine several modern caching techniques namely in memory caching,
distributed caching, and proxy caching and their functions in enhancing
performance in decentralized settings. Through literature review we examine the
utilization of AI techniques for alleviating risks related to market
volatility, liquidity challenges, operational failures, regulatory compliance,
and security threats. This comparison research evaluates various case studies
from prominent DeFi technologies, emphasizing critical performance metrics like
latency reduction, load balancing, and system resilience. Additionally, we
examine the problems and trade offs associated with these technologies,
emphasizing their effects on consistency, scalability, and fault tolerance. By
meticulously analyzing real world applications, specifically centering on the
Aave platform as our principal case study, we illustrate how the purposeful
amalgamation of AI with contemporary caching methodologies has revolutionized
risk management in distributed arbitrage systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on AI and Financial Innovation AIFI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Emotions and Architecture: Sentiment Analysis in Modern
  Distributed Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahak Shah, Akaash Vishal Hazarika, Meetu Malhotra, Sachin C. Patil, Joshit Mohanty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is a field within NLP that has gained importance because
it is applied in various areas such as; social media surveillance, customer
feedback evaluation and market research. At the same time, distributed systems
allow for effective processing of large amounts of data. Therefore, this paper
examines how sentiment analysis converges with distributed systems by
concentrating on different approaches, challenges and future investigations.
Furthermore, we do an extensive experiment where we train sentiment analysis
models using both single node configuration and distributed architecture to
bring out the benefits and shortcomings of each method in terms of performance
and accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE 3rd International Conference on Advancements in Smart, Secure
  and Intelligent Computing (ASSIC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in
  LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiying Shen, Tanmoy Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to grow, reducing costs and
alleviating GPU demands has become increasingly critical. However, existing
schedulers primarily target either GPU compute or Key-Value Cache (KVC)
utilization, failing to fully optimize both GPU compute and KVC usage during
each iteration or guarantee timely KVC allocations when needed. To address
these challenges, we conducted a trace-based experimental analysis and made
insightful observations, leading to the design of a system called EconoServe.
EconoServe maximizes multi-resource utilization while ensuring service-level
objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch
to maximize GPU utilization in each iteration, EconoServe maintains separate
waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It
batches GTs with the same predicted response lengths (RL) to save scheduling
time and allocates KVC space for the predicted RL to avoid KVC allocation
failures. It further has a novel KVC pipelining method, allowing sharing
allocated but unused KVC space to enhance KVC utilization. In addition, it
prioritizes queued requests that occupy more KVC to release KVC earlier and
satisfy request service-level-objective (SLO). Experimental results demonstrate
that EconoServe increases throughput by up to 4$\times$ with the same level of
latency, generates up to 91\% lower job completion time and up to 91\% higher
SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs
used in DistServe by up to 78\% while maintaining the same level of goodput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ILVES: Accurate and efficient bond length and angle constraints in
  molecular dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorién López-Villellas, Carl Christian Kjelgaard Mikkelsen, Juan José Galano-Frutos, Santiago Marco-Sola, Jesús Alastruey-Benedé, Pablo Ibáñez, Miquel Moretó, Maria Cristina De Rosa, Pablo García-Risueño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Force field-based molecular dynamics simulations are customarily carried out
by constraining internal degrees of freedom. The de facto state-of-the-art
algorithms for this purpose, SHAKE, LINCS and P-LINCS, converge slowly,
impeding high-accuracy calculations and limiting the realism of simulations.
Furthermore, LINCS and P-LINCS cannot handle general angular constraints, which
restricts increasing the time step.
  In this paper, we introduce ILVES, a set of parallel algorithms that converge
so rapidly that it is now practical to solve bond length and associated angular
constraint equations as accurately as the hardware will allow. We have
integrated our work into Gromacs and our analysis demonstrates that, in most
cases, our software is superior to the state-of-the-art. We anticipate that
ILVES will allow for an increase in the time step, thus accelerating
contemporary calculations by a factor of at least 2. This will allow the
scientific community to increase the range of phenomena that can therefore be
simulated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel $k$-Core Decomposition: Theory and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youzhe Liu, Xiaojun Dong, Yan Gu, Yihan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes efficient solutions for $k$-core decomposition with high
parallelism. The problem of $k$-core decomposition is fundamental in graph
analysis and has applications across various domains. However, existing
algorithms face significant challenges in achieving work-efficiency in theory
and/or high parallelism in practice, and suffer from various performance
bottlenecks.
  We present a simple, work-efficient parallel framework for $k$-core
decomposition that is easy to implement and adaptable to various strategies for
improving work-efficiency. We introduce two techniques to enhance parallelism:
a sampling scheme to reduce contention on high-degree vertices, and vertical
granularity control (VGC) to mitigate scheduling overhead for low-degree
vertices. Furthermore, we design a hierarchical bucket structure to optimize
performance for graphs with high coreness values.
  We evaluate our algorithm on a diverse set of real-world and synthetic
graphs. Compared to state-of-the-art parallel algorithms, including ParK, PKC,
and Julienne, our approach demonstrates superior performance on 23 out of 25
graphs when tested on a 96-core machine. Our algorithm shows speedups of up to
315$\times$ over ParK, 33.4$\times$ over PKC, and 52.5$\times$ over Julienne.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterizing GPU Resilience and Impact on AI/HPC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkun Cui, Archit Patke, Ziheng Chen, Aditya Ranjan, Hung Nguyen, Phuong Cao, Saurabh Jha, Brett Bode, Gregory Bauer, Chandra Narayanaswami, Daby Sow, Catello Di Martino, Zbigniew T. Kalbarczyk, Ravishankar K. Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we characterize GPU failures in Delta, the current large-scale
AI system with over 600 petaflops of peak compute throughput. The system
comprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA
A40, A100, and H100 GPUs. The study uses two and a half years of data on GPU
errors. We evaluate the resilience of GPU hardware components to determine the
vulnerability of different GPU components to failure and their impact on the
GPU and node availability. We measure the key propagation paths in GPU
hardware, GPU interconnect (NVLink), and GPU memory. Finally, we evaluate the
impact of the observed GPU errors on user jobs. Our key findings are: (i)
Contrary to common beliefs, GPU memory is over 30x more reliable than GPU
hardware in terms of MTBE (mean time between errors). (ii) The newly introduced
GSP (GPU System Processor) is the most vulnerable GPU hardware component. (iii)
NVLink errors did not always lead to user job failure, and we attribute it to
the underlying error detection and retry mechanisms employed. (iv) We show
multiple examples of hardware errors originating from one of the key GPU
hardware components, leading to application failure. (v) We project the impact
of GPU node availability on larger scales with emulation and find that
significant overprovisioning between 5-20% would be necessary to handle GPU
failures. If GPU availability were improved to 99.9%, the overprovisioning
would be reduced by 4x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emulating Full Participation: An Effective and Fair Client Selection
  Strategy for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, Juzheng Miao, Puning Zhao, Li Zhou, H. Vicky Zhao, Shouling Ji, Bowen Zhou, Furui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning, client selection is a critical problem that
significantly impacts both model performance and fairness. Prior studies
typically treat these two objectives separately, or balance them using simple
weighting schemes. However, we observe that commonly used metrics for model
performance and fairness often conflict with each other, and a straightforward
weighted combination is insufficient to capture their complex interactions. To
address this, we first propose two guiding principles that directly tackle the
inherent conflict between the two metrics while reinforcing each other. Based
on these principles, we formulate the client selection problem as a long-term
optimization task, leveraging the Lyapunov function and the submodular nature
of the problem to solve it effectively. Experiments show that the proposed
method improves both model performance and fairness, guiding the system to
converge comparably to full client participation. This improvement can be
attributed to the fact that both model performance and fairness benefit from
the diversity of the selected clients' data distributions. Our approach
adaptively enhances this diversity by selecting clients based on their data
distributions, thereby improving both model performance and fairness.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-23T00:00:00Z">2025-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Performance of Photonic SRAM-based In-Memory Computing for
  Tensor Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasindu Wijeratne, Sugeet Sunder, Md Abdullah-Al Kaiser, Akhilesh Jaiswal, Clynn Mathew, Ajey P. Jacob, Viktor Prasanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonics-based in-memory computing systems have demonstrated a significant
speedup over traditional transistor-based systems because of their ultra-fast
operating frequencies and high data bandwidths. Photonic static random access
memory (pSRAM) is a crucial component for achieving the objective of ultra-fast
photonic in-memory computing systems. In this work, we model and evaluate the
performance of a novel photonic SRAM array architecture in development.
Additionally, we examine hyperspectral operation through wavelength division
multiplexing (WDM) to enhance the throughput of the pSRAM array. We map
Matricized Tensor Times Khatri-Rao Product (MTTKRP), a computational kernel
commonly used in tensor decomposition, to the proposed pSRAM array
architecture. We also develop a predictive performance model to estimate the
sustained performance of different configurations of the pSRAM array. Using the
predictive performance model, we demonstrate that the pSRAM array achieves 17
PetaOps while performing MTTKRP in a practical hardware configuration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Sparse MTTKRP for Small Tensor Decomposition on GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasindu Wijeratne, Rajgopal Kannan, Viktor Prasanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the
bottleneck kernel of sparse tensor decomposition. In tensor decomposition,
spMTTKRP is performed iteratively along all the modes of an input tensor. In
this work, we propose a mode-specific tensor layout on GPU that uses multiple
tensor copies, where each copy is optimized for a specific mode. The proposed
tensor layout increases the data locality of external memory accesses and
eliminates the intermediate values communicated between the GPU thread blocks
and the GPU global memory. We also propose a tensor partitioning scheme to
optimally distribute the total computations among GPU streaming multiprocessors
based on the sparsity and the dimensions of the input tensor. Our approach
achieves a geometric mean speedup of 2.4x, 7.9x, and 8.9x in total execution
time compared with the state-of-the-art GPU baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INDIGO: Page Migration for Hardware Memory Disaggregation Across a
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Patke, Christian Pinto, Saurabh Jha, Haoran Qiu, Zbigniew Kalbarczyk, Ravishankar Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hardware memory disaggregation (HMD) is an emerging technology that enables
access to remote memory, thereby creating expansive memory pools and reducing
memory underutilization in datacenters. However, a significant challenge arises
when accessing remote memory over a network: increased contention that can lead
to severe application performance degradation. To reduce the performance
penalty of using remote memory, the operating system uses page migration to
promote frequently accessed pages closer to the processor. However, previously
proposed page migration mechanisms do not achieve the best performance in HMD
systems because of obliviousness to variable page transfer costs that occur due
to network contention. To address these limitations, we present INDIGO: a
network-aware page migration framework that uses novel page telemetry and a
learning-based approach for network adaptation. We implemented INDIGO in the
Linux kernel and evaluated it with common cloud and HPC applications on a real
disaggregated memory system prototype. Our evaluation shows that INDIGO offers
up to 50-70% improvement in application performance compared to other
state-of-the-art page migration policies and reduces network traffic up to 2x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Replication Protocols on SmartNICs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. R. Siavash Katebzadeh, Antonios Katsarakis, Boris Grot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's datacenter applications rely on datastores that are required to
provide high availability, consistency, and performance. To achieve high
availability, these datastores replicate data across several nodes. Such
replication is managed through a reliable protocol designed to keep the
replicas consistent using a consistency model, even in the presence of faults.
For several applications, strong consistency models are favored over weaker
consistency models, as the former guarantee a more intuitive behavior for
clients. Furthermore, to meet the demands of high online traffic, datastores
must offer high throughput and low latency.
  However, delivering both strong consistency and high performance
simultaneously can be challenging. Reliable replication protocols typically
require multiple rounds of communication over the network stack, which
introduces latency and increases the load on network resources. Moreover, these
protocols consume considerable CPU resources, which impacts the overall
performance of applications, especially in high-throughput environments.
  In this work, we aim to design a hardware-accelerated system for replication
protocols to address these challenges. We approach offloading the replication
protocol onto SmartNICs, which are specialized network interface cards that can
be programmed to implement custom logic directly on the NIC. By doing so, we
aim to enhance performance while preserving strong consistency, all while
saving valuable CPU cycles that can be used for applications' logic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, Yuchen Hao, Yufei Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for
large language model training. We first thoroughly analyze the workload
imbalance issue in LLM training and identify two primary sources of imbalance
at the pipeline parallelism and context parallelism levels. Then, to address
the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a
workload-aware variable-length document packing method to balance the
computation and communication workload across micro-batches. Additionally, at
the context parallelism level, WLB-LLM introduces a novel fine-grained
per-document sharding strategy, ensuring each worker within a context
parallelism group has an identical workload. Comprehensive experiments under
different model scales demonstrate that WLB-LLM significantly mitigates the
workload imbalance during 4D parallelism LLM training and achieves an average
speedup of 1.23x when applying WLB-LLM in our internal LLM training framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Computation in Congested Anonymous Dynamic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07849v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07849v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe A. Di Luna, Giovanni Viglietta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An anonymous dynamic network is a network of indistinguishable processes
whose communication links may appear or disappear unpredictably over time.
Previous research has shown that deterministically computing an arbitrary
function of a multiset of input values given to these processes takes only a
linear number of communication rounds (Di Luna-Viglietta, FOCS 2022).
  However, fast algorithms for anonymous dynamic networks rely on the
construction and transmission of large data structures called "history trees",
whose size is polynomial in the number of processes. This approach is
unfeasible if the network is congested, and only messages of logarithmic size
can be sent through its links. Observe that sending a large message piece by
piece over several rounds is not in itself a solution, due to the anonymity of
the processes combined with the dynamic nature of the network. Moreover, it is
known that certain basic tasks such as all-to-all token dissemination (by means
of single-token forwarding) require $\Omega(n^2/\log n)$ rounds in congested
networks (Dutta et al., SODA 2013).
  In this work, we develop a series of practical and efficient techniques that
make it possible to use history trees in congested anonymous dynamic networks.
Among other applications, we show how to compute arbitrary functions in such
networks in $O(n^3)$ communication rounds, greatly improving upon previous
state-of-the-art algorithms for congested networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-22T00:00:00Z">2025-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRDT-Based Game State Synchronization in Peer-to-Peer VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Dantas, Carlos Baquero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual presence demands ultra-low latency, a factor that centralized
architectures, by their nature, cannot minimize. Local peer-to-peer
architectures offer a compelling alternative, but also pose unique challenges
in terms of network infrastructure. This paper introduces a prototype
leveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time
collaboration in a shared virtual environment. Using this prototype, we
investigate latency, synchronization, and the challenges of decentralized
coordination in dynamic non-Byzantine contexts. We aim to question prevailing
assumptions about decentralized architectures and explore the practical
potential of P2P in advancing virtual presence. This work challenges the
constraints of mediated networks and highlights the potential of decentralized
architectures to redefine collaboration and interaction in digital spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Total PDF pages: 11 Figures: 11</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neutron particle transport 3D method of characteristic Multi GPU
  platform Parallel Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faguo Zhou, Shunde Li, Rong Xue, Lingkun Bu, Ningming Nie, Peng Shi, Jue Wang, Yun Hu, Zongguo Wang, Yangang Wang, Qinmeng Yang, Miao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional neutron transport calculations using the Method of
Characteristics (MOC) are highly regarded for their exceptional computational
efficiency, precision, and stability. Nevertheless, when dealing with
extensive-scale computations, the computational demands are substantial,
leading to prolonged computation times. To address this challenge while
considering GPU memory limitations, this study transplants the real-time
generation and characteristic line computation techniques onto the GPU
platform. Empirical evidence emphasizes that the GPU-optimized approach
maintains a heightened level of precision in computation results and produces a
significant acceleration effect. Furthermore, to fully harness the
computational capabilities of GPUs, a dual approach involving characteristic
line preloading and load balancing mechanisms is adopted, further enhancing
computational efficiency. The resulting increase in computational efficiency,
compared to traditional methods, reaches an impressive 300 to 400-fold
improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures. Submitted to a peer-reviewed journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PipeBoost: Resilient Pipelined Architecture for Fast Serverless LLM
  Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongpeng Liu, Xiaojian Liao, Hancheng Liu, Limin Xiao, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents PipeBoost, a low-latency LLM serving system for multi-GPU
(serverless) clusters, which can rapidly launch inference services in response
to bursty requests without preemptively over-provisioning GPUs. Many LLM
inference tasks rely on the same base model (e.g., LoRA). To leverage this,
PipeBoost introduces fault-tolerant pipeline parallelism across both model
loading and inference stages. This approach maximizes aggregate PCIe bandwidth
and parallel computation across GPUs, enabling faster generation of the first
token. PipeBoost also introduces recovery techniques that enable uninterrupted
inference services by utilizing the shared advantages of multiple GPUs.
Experimental results show that, compared to state-of-the-art low-latency LLM
serving systems, PipeBoost reduces inference latency by 31% to 49.8%. For
certain models (e.g., OPT-1.3B), PipeBoost achieves cold-start latencies in the
range of a few hundred microseconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Ma, Senkang Hu, Zhengru Fang, Yun Ji, Yiqin Deng, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accommodate constantly changing road conditions, real-time model training
is essential for autonomous driving (AD). Federated learning (FL) serves as a
promising paradigm to enable autonomous vehicles to train models
collaboratively with their onboard computing resources. However, existing
vehicle selection schemes for FL all assume predetermined and
location-independent vehicles' datasets, neglecting the fact that vehicles
collect training data along their routes, thereby resulting in suboptimal
vehicle selection. To improve the perception quality in AD for a region, we
propose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring
trajectory-dependent vehicular training data collection. To this end, we first
derive the convergence bound of FL by considering the impact of both vehicles'
uncertain trajectories and uploading probabilities, from which we discover that
minimizing the training loss is equivalent to minimizing a weighted sum of
local and global earth mover's distance (EMD) between vehicles' collected data
distribution and global data distribution. Based on this observation, we
formulate the trajectory-dependent vehicle selection and data collection
problem for FL in AD. Given that the problem is NP-hard, we develop an
efficient algorithm to find the solution with an approximation guarantee.
Extensive simulation results have demonstrated the effectiveness of our
approach in improving object detection performance compared with existing
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using a Market Economy to Provision Compute Resources Across Planet-wide
  Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murray Stokely, Jim Winget, Ed Keyes, Carrie Grimes, Benjamin Yolken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a practical, market-based solution to the resource provisioning
problem in a set of heterogeneous resource clusters. We focus on provisioning
rather than immediate scheduling decisions to allow users to change long-term
job specifications based on market feedback. Users enter bids to purchase
quotas, or bundles of resources for long-term use. These requests are mapped
into a simulated clock auction which determines uniform, fair resource prices
that balance supply and demand. The reserve prices for resources sold by the
operator in this auction are set based on current utilization, thus guiding the
users as they set their bids towards under-utilized resources. By running these
auctions at regular time intervals, prices fluctuate like those in a real-world
economy and provide motivation for users to engineer systems that can best take
advantage of available resources.
  These ideas were implemented in an experimental resource market at Google.
Our preliminary results demonstrate an efficient transition of users from more
congested resource pools to less congested resources. The disparate engineering
costs for users to reconfigure their jobs to run on less expensive resource
pools was evidenced by the large price premiums some users were willing to pay
for more expensive resources. The final resource allocations illustrated how
this framework can lead to significant, beneficial changes in user behavior,
reducing the excessive shortages and surpluses of more traditional allocation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2009 IEEE International Symposium on Parallel &
  Distributed Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time- and Space-Optimal Silent Self-Stabilizing Exact Majority in
  Population Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haruki Kanaya, Ryota Eguchi, Taisho Sasada, Fukuhito Ooshita, Michiko Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the self-stabilizing exact majority problem in the population
protocol model, introduced by Angluin, Aspnes, Diamadi, Fischer, and Peralta
(2004). In this model, there are $n$ state machines, called agents, which form
a network. At each time step, only two agents interact with each other, and
update their states. In the self-stabilizing exact majority problem, each agent
has a fixed opinion, $\mathtt{A}$ or $\mathtt{B}$, and stabilizes to a safe
configuration in which all agents output the majority opinion from any initial
configuration.
  In this paper, we show the impossibility of solving the self-stabilizing
exact majority problem without knowledge of $n$ in any protocol. We propose a
silent self-stabilizing exact majority protocol, which stabilizes within $O(n)$
parallel time in expectation and within $O(n \log n)$ parallel time with high
probability, using $O(n)$ states, with knowledge of $n$. Here, a silent
protocol means that, after stabilization, the state of each agent does not
change. We establish lower bounds, proving that any silent protocol requires
$\Omega(n)$ states, $\Omega(n)$ parallel time in expectation, and $\Omega(n
\log n)$ parallel time with high probability to reach a safe configuration.
Thus, the proposed protocol is time- and space-optimal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative Caching System for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Caching has the potential to be of significant benefit for accessing large
language models (LLMs) due to their high latencies which typically range from a
small number of seconds to well over a minute. Furthermore, many LLMs charge
money for queries; caching thus has a clear monetary benefit. This paper
presents a new caching system for improving user experiences with LLMs. In
addition to reducing both latencies and monetary costs for accessing LLMs, our
system also provides important features that go beyond the performance benefits
typically associated with caches. A key feature we provide is generative
caching, wherein multiple cached responses can be synthesized to provide
answers to queries which have never been seen before. Our generative caches
function as repositories of valuable information which can be mined and
analyzed. We also improve upon past semantic caching techniques by tailoring
the caching algorithms to optimally balance cost and latency reduction with the
quality of responses provided. Performance tests indicate that our caches are
considerably faster than GPTcache.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network
  Acceleration on GPUs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Xie, Yuebo Luo, Hongwu Peng, Caiwen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Top-k selection algorithms are fundamental in a wide range of applications,
including high-performance computing, information retrieval, big data
processing, and neural network model training. In this paper, we present
RTop-K, a highly efficient parallel row-wise top-k selection algorithm
specifically designed for GPUs. RTop-K leverages a binary search-based approach
to optimize row-wise top-k selection, providing a scalable and accelerated
solution. We conduct a detailed analysis of early stopping in our algorithm,
showing that it effectively maintains the testing accuracy of neural network
models while substantially improving performance. Our GPU implementation of
RTop-K demonstrates superior performance over state-of-the-art row-wise top-k
GPU implementations, achieving an average speed-up of up to 11.49$\times$ with
early stopping and 7.29$\times$ without early stopping. Moreover, RTop-K
accelerates the overall training workflow of MaxK-GNNs, delivering speed-ups
ranging from 11.97% to 33.29% across different models and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Seamless Hierarchical Federated Learning under Intermittent
  Client Participation: A Stagewise Decision-Making Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a pioneering distributed learning paradigm
that enables devices/clients to build a shared global model. This global model
is obtained through frequent model transmissions between clients and a central
server, which may cause high latency, energy consumption, and congestion over
backhaul links. To overcome these drawbacks, Hierarchical Federated Learning
(HFL) has emerged, which organizes clients into multiple clusters and utilizes
edge nodes (e.g., edge servers) for intermediate model aggregations between
clients and the central server. Current research on HFL mainly focus on
enhancing model accuracy, latency, and energy consumption in scenarios with a
stable/fixed set of clients. However, addressing the dynamic availability of
clients -- a critical aspect of real-world scenarios -- remains underexplored.
This study delves into optimizing client selection and client-to-edge
associations in HFL under intermittent client participation so as to minimize
overall system costs (i.e., delay and energy), while achieving fast model
convergence. We unveil that achieving this goal involves solving a complex
NP-hard problem. To tackle this, we propose a stagewise methodology that splits
the solution into two stages, referred to as Plan A and Plan B. Plan A focuses
on identifying long-term clients with high chance of participation in
subsequent model training rounds. Plan B serves as a backup, selecting
alternative clients when long-term clients are unavailable during model
training rounds. This stagewise methodology offers a fresh perspective on
client selection that can enhance both HFL and conventional FL via enabling
low-overhead decision-making processes. Through evaluations on MNIST and
CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks
in terms of model accuracy and system costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures,5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debunking the CUDA Myth Towards GPU-based AI Systems <span class="chip">ISCA-52</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjae Lee, Juntaek Lim, Jehyeon Bang, Eunyeong Cho, Huijong Jeong, Taesu Kim, Hyungjun Kim, Joonhyung Lee, Jinseop Im, Ranggi Hwang, Se Jung Kwon, Dongsoo Lee, Minsoo Rhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive evaluation of Intel Gaudi NPUs as an
alternative to NVIDIA GPUs, which is currently the de facto standard in AI
system design. First, we create a suite of microbenchmarks to compare Intel
Gaudi-2 with NVIDIA A100, showing that Gaudi-2 achieves competitive performance
not only in primitive AI compute, memory, and communication operations but also
in executing several important AI workloads end-to-end. We then assess Gaudi
NPU's programmability by discussing several software-level optimization
strategies to employ for implementing critical FBGEMM operators and vLLM,
evaluating their efficiency against GPU-optimized counterparts. Results
indicate that Gaudi-2 achieves energy efficiency comparable to A100, though
there are notable areas for improvement in terms of software maturity. Overall,
we conclude that, with effective integration into high-level AI frameworks,
Gaudi NPUs could challenge NVIDIA GPU's dominance in the AI server market,
though further improvements are necessary to fully compete with NVIDIA's robust
software ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 52nd IEEE/ACM International Symposium
  on Computer Architecture (ISCA-52), 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-21T00:00:00Z">2025-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Container scheduling <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serinv: A Scalable Library for the Selected Inversion of
  Block-Tridiagonal with Arrowhead Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Maillou, Lisa Gaedke-Merzhaeuser, Alexandros Nikolaos Ziogas, Olaf Schenk, Mathieu Luisier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inversion of structured sparse matrices is a key but computationally and
memory-intensive operation in many scientific applications. There are cases,
however, where only particular entries of the full inverse are required. This
has motivated the development of so-called selected-inversion algorithms,
capable of computing only specific elements of the full inverse. Currently,
most of them are either shared-memory codes or limited to CPU implementations.
Here, we introduce Serinv, a scalable library providing distributed, GPU-based
algorithms for the selected inversion and Cholesky decomposition of
positive-definite, block-tridiagonal arrowhead matrices. This matrix class is
highly relevant in statistical climate modeling and materials science
applications. The performance of Serinv is demonstrated on synthetic and real
datasets from statistical air temperature prediction models. In our numerical
tests, Serinv achieves 32.3% strong and 47.2% weak scaling efficiency and up to
two orders of magnitude speedup over the sparse direct solvers PARDISO and
MUMPS on 16 GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Efficiency trends in HPC: what high-energy and astrophysicists
  need to know 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Estela Suarez, Jorge Amaya, Martin Frank, Oliver Freyermuth, Maria Girone, Bartosz Kostrzewa, Susanne Pfalzner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing energy demands of HPC systems have made energy efficiency a
critical concern for system developers and operators. However, HPC users are
generally less aware of how these energy concerns influence the design,
deployment, and operation of supercomputers even though they experience the
consequences. This paper examines the implications of HPC's energy consumption,
providing an overview of current trends aimed at improving energy efficiency.
We describe how hardware innovations such as energy-efficient processors, novel
system architectures, power management techniques, and advanced scheduling
policies do have a direct impact on how applications need to be programmed and
executed on HPC systems. For application developers, understanding how these
new systems work and how to analyse and report the performances of their own
software is critical in the dialog with HPC system designers and
administrators. The paper aims to raise awareness about energy efficiency among
users, particularly in the high energy physics and astrophysics domains,
offering practical advice on how to analyse and optimise applications to reduce
their energy consumption without compromising on performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoGoFair: Post-Processing for Local and Global Fairness in Federated
  Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhang, Chaochao Chen, Zhongxuan Han, Qiyong Zhong, Xiaolin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has garnered considerable interest for its capability
to learn from decentralized data sources. Given the increasing application of
FL in decision-making scenarios, addressing fairness issues across different
sensitive groups (e.g., female, male) in FL is crucial. Current research often
focuses on facilitating fairness at each client's data (local fairness) or
within the entire dataset across all clients (global fairness). However,
existing approaches that focus exclusively on either local or global fairness
fail to address two key challenges: (\textbf{CH1}) Under statistical
heterogeneity, global fairness does not imply local fairness, and vice versa.
(\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the
aforementioned challenges, this paper proposes a novel post-processing
framework for achieving both Local and Global Fairness in the FL context,
namely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal
classifier under local and global fairness constraints, which strikes the
optimal accuracy-fairness balance in the probabilistic sense. To address CH2,
LoGoFair employs a model-agnostic federated post-processing procedure that
enables clients to collaboratively optimize global fairness while ensuring
local fairness, thereby achieving the optimal fair classifier within FL.
Experimental results on three real-world datasets further illustrate the
effectiveness of the proposed LoGoFair framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness of deep learning classification to adversarial input on GPUs:
  asynchronous parallel accumulation is a source of vulnerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, Ada Sedova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of machine learning (ML) classification models to resist small,
targeted input perturbations - known as adversarial attacks - is a key measure
of their safety and reliability. We show that floating-point non associativity
(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to
result in misclassification, without any perturbation to the input.
Additionally, we show this misclassification is particularly significant for
inputs close to the decision boundary and that standard adversarial robustness
results may be overestimated up to 4.6% when not considering machine-level
details. We first study a linear classifier, before focusing on standard Graph
Neural Network (GNN) architectures and datasets. We present a novel black-box
attack using Bayesian optimization to determine external workloads that bias
the output of reductions on GPUs and reliably lead to misclassification.
Motivated by these results, we present a new learnable permutation (LP)
gradient-based approach, to learn floating point operation orderings that lead
to misclassifications, making the assumption that any reduction or permutation
ordering is possible. This LP approach provides a worst-case estimate in a
computationally efficient manner, avoiding the need to run identical
experiments tens of thousands of times over a potentially large set of possible
GPU states or architectures. Finally, we investigate parallel reduction
ordering across different GPU architectures for a reduction under three
conditions: (1) executing external background workloads, (2) utilizing
multi-GPU virtualization, and (3) applying power capping. Our results
demonstrate that parallel reduction ordering varies significantly across
architectures under the first two conditions. The results and methods developed
here can help to include machine-level considerations into adversarial
robustness assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at EuroPar 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the End-to-End Efficiency of Offline Inference for Multi-LLM
  Applications Based on Sampling and Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingzhi Fang, Yanyan Shen, Yue Wang, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) have shown great success in many tasks, they
are used in various applications. While a lot of works have focused on the
efficiency of single-LLM application (e.g., offloading, request scheduling,
parallelism strategy selection), multi-LLM applications receive less attention,
particularly in offline inference scenarios. In this work, we aim to improve
the offline end-to-end inference efficiency of multi-LLM applications in the
single-node multi-GPU environment. The problem involves two key decisions: (1)
determining which LLMs to run concurrently each time (we may not run all the
models at the same time), and (2) selecting a parallelism strategy to use for
each LLM. This problem is NP-hard. Naive solutions may not work well because
the running time for a model to complete a set of requests depends on the
request workload and the selected parallelism strategy, and they lack an
accurate model of the running time. As the LLM output lengths are unknown
before running, to estimate the model running time, we propose a
sampling-then-simulation method which first estimates the output lengths by
sampling from an empirical cumulative function we obtained from a large dataset
in advance, and then simulates the LLM inference process accordingly. Based on
the simulation, we estimate the per-iteration latencys to get the total
latency. A greedy method is proposed to optimize the scheduling of the LLMs in
the application across the GPUs. We then propose a framework SamuLLM which
contains two phases: planning, which calls the greedy method for an application
and running, which runs the application and dynamically adjust the model
scheduling based on the runtime information. Experiments on 3 applications and
a mixed application show that SamuLLM can achieve 1.0-2.4$\times$ end-to-end
speedups compared to the competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Cross-Domain Click-Through Rate Prediction With Large Language
  Model Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangcheng Qin, Xueyuan Zhang, Baisong Liu, Jiangbo Qian, Yangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting click-through rates (CTR) under stringent privacy
constraints poses profound challenges, particularly when user-item interactions
are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)
methods frequently assume homogeneous feature spaces and rely on centralized
data sharing, neglecting complex inter-domain discrepancies and the subtle
trade-offs imposed by privacy-preserving protocols. Here, we present Federated
Cross-Domain CTR Prediction with Large Language Model Augmentation
(FedCCTR-LM), a federated framework engineered to address these limitations by
synchronizing data augmentation, representation disentanglement, and adaptive
privacy protection. Our approach integrates three core innovations. First, the
Privacy-Preserving Augmentation Network (PrivAugNet) employs large language
models to enrich user and item representations and expand interaction
sequences, mitigating data sparsity and feature incompleteness. Second, the
Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL)
module disentangles domain-specific and shared user preferences, employing
intra-domain representation alignment (IDRA) and crossdomain representation
disentanglement (CDRD) to refine the learned embeddings and enhance knowledge
transfer across domains. Finally, the Adaptive Local Differential Privacy
(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal
balance between rigorous privacy guarantees and predictive accuracy. Empirical
evaluations on four real-world datasets demonstrate that FedCCTR-LM
substantially outperforms existing baselines, offering robust,
privacy-preserving, and generalizable cross-domain CTR prediction in
heterogeneous, federated environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeFT: Mitigating Data Dependencies for Flexible Communication Scheduling
  in Distributed Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Meng, Yuzhong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication scheduling aims to reduce communication bottlenecks in data
parallel training (DP) by maximizing the overlap between computation and
communication. However, existing schemes fall short due to three main issues:
(1) hard data dependencies break some overlapping between communication and
computation; (2) high coverage rates impair further improvement on performance;
(3) imbalanced communication/computation times of tensors caused by
partitioning/fusion strategies cause more bubbles. To address these drawbacks,
we propose a new communication scheduling scheme DeFT, whose key insight is to
mitigate data dependencies and support flexible scheduling in distributed
training. DeFT uncovers new overlapping chances in training by transforming the
scheduling problem into multiple knapsack problems. Specifically, DeFT
eliminates hard dependencies with delayed updates, reducing the coverage rate
by adjusting update frequency and utilizing heterogeneous communication links,
merging the computation times of backward or forward as the knapsack capacity
to avoid the negative impact of unbalanced tensors. Additionally, DeFT
preserves training accuracy by adjusting its scheduling strategy via
convergence loss quantification. Extensive experiments with 16 A100 GPUs showed
that DeFT achieved speedups of 29% to 115% on three representative benchmarks
compared to US-Byte and Bytescheduler with no loss of accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Ratio based Real-time Job Offloading and Resource Allocation in
  Mobile Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanchao Gao, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile Edge Computing (MEC) has emerged as a promising paradigm enabling
vehicles to handle computation-intensive and time-sensitive applications for
intelligent transportation. Due to the limited resources in MEC, effective
resource management is crucial for improving system performance. While existing
studies mostly focus on the job offloading problem and assume that job resource
demands are fixed and given apriori, the joint consideration of job offloading
(selecting the edge server for each job) and resource allocation (determining
the bandwidth and computation resources for offloading and processing) remains
underexplored. This paper addresses the joint problem for deadline-constrained
jobs in MEC with both communication and computation resource constraints,
aiming to maximize the total utility gained from jobs. To tackle this problem,
we propose an approximation algorithm, $\mathtt{IDAssign}$, with an
approximation bound of $\frac{1}{6}$, and experimentally evaluate the
performance of $\mathtt{IDAssign}$ by comparing it to state-of-the-art
heuristics using a real-world taxi trace and object detection applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by The 4th Real-time And intelliGent Edge computing
  workshop, hold on May 6th, 2025 in Irvine, CA, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based
  Proof-of-Stake Blockchains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeta Avarikioti, Eleftherios Kokoris Kogias, Ray Neiheiser, Christos Stefo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a formal analysis of quorum-based State Machine Replication (SMR)
protocols in Proof-of-Stake (PoS) systems under a hybrid threat model
comprising honest, Byzantine, and rational validators. Our analysis of
traditional quorum-based protocols establishes two fundamental impossibility
results: (1) in partially synchronous networks, no quorum-based protocol can
achieve SMR when rational and Byzantine validators comprise more than $1/3$ of
participants, and (2) in synchronous networks, SMR remains impossible when
rational and Byzantine validators comprise $2/3$ or more of participants.
  To overcome these limitations, we propose two complementary solutions in our
hybrid model. First, we introduce a protocol that enforces a bound on the
volume of the total transacted amount that is finalized within any time window
$\Delta$ and prove that this bound is necessary for secure SMR protocols in our
model. Second, we present the \emph{strongest chain rule}, which enables
efficient finalization of transactions when the majority of honest participants
provably support the SMR execution. Through empirical analysis of Ethereum and
Cosmos networks, we demonstrate that validator participation consistently
exceeds the required ${5}/{6}$ threshold, establishing the practical
feasibility of our solution in production PoS systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGO: Systematic Performance Optimization for Retrieval-Augmented
  Generation Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG), which combines large language models
(LLMs) with retrievals from external knowledge databases, is emerging as a
popular approach for reliable LLM serving. However, efficient RAG serving
remains an open challenge due to the rapid emergence of many RAG variants and
the substantial differences in workload characteristics across them. In this
paper, we make three fundamental contributions to advancing RAG serving. First,
we introduce RAGSchema, a structured abstraction that captures the wide range
of RAG algorithms, serving as a foundation for performance optimization.
Second, we analyze several representative RAG workloads with distinct
RAGSchema, revealing significant performance variability across these
workloads. Third, to address this variability and meet diverse performance
requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a
system optimization framework for efficient RAG serving. Our evaluation shows
that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in
time-to-first-token latency compared to RAG systems built on LLM-system
extensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 19 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Quantum Approximate Optimization Algorithm on a
  Quantum-Centric Supercomputing Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongmin Kim, Vincent R. Pascuzzi, Zhihao Xu, Tengfei Luo, Eungkyu Lee, In-Saeng Suh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum approximate optimization algorithm (QAOA) has shown promise in
solving combinatorial optimization problems by providing quantum speedup on
near-term gate-based quantum computing systems. However, QAOA faces challenges
for high-dimensional problems due to the large number of qubits required and
the complexity of deep circuits, limiting its scalability for real-world
applications. In this study, we present a distributed QAOA (DQAOA), which
leverages distributed computing strategies to decompose a large computational
workload into smaller tasks that require fewer qubits and shallower circuits
than necessitated to solve the original problem. These sub-problems are
processed using a combination of high-performance and quantum computing
resources. The global solution is iteratively updated by aggregating
sub-solutions, allowing convergence toward the optimal solution. We demonstrate
that DQAOA can handle considerably large-scale optimization problems (e.g.,
1,000-bit problem) achieving a high approximation ratio ($\sim$99%) and short
time-to-solution ($\sim$276 s), outperforming existing strategies. Furthermore,
we realize DQAOA on a quantum-centric supercomputing architecture, paving the
way for practical applications of gate-based quantum computers in real-world
optimization tasks. To extend DQAOA's applicability to materials science, we
further develop an active learning algorithm integrated with our DQAOA
(AL-DQAOA), which involves machine learning, DQAOA, and active data production
in an iterative loop. We successfully optimize photonic structures using
AL-DQAOA, indicating that solving real-world optimization problems using
gate-based quantum computing is feasible. We expect the proposed DQAOA to be
applicable to a wide range of optimization problems and AL-DQAOA to find
broader applications in material design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, Íñigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, Rodrigo Fonseca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) demonstrate impressive capabilities in
understanding images, videos, and audio beyond text. However, efficiently
serving LMMs in production environments poses significant challenges due to
their complex architectures and heterogeneous characteristics across their
multi-stage inference pipelines. We present the first comprehensive systems
analysis of two prominent LMM architectures, decoder-only and cross-attention,
across six representative open-source models, revealing key systems design
implications. We also present an in-depth analysis of production LMM inference
traces, uncovering unique workload characteristics, including variable,
heavy-tailed request distributions and bursty traffic patterns. Based on these
insights, we propose ModServe, a modular LMM serving system that decouples
stages for independent optimization and adaptive scaling. ModServe dynamically
reconfigures stages and handles bursty traffic with modality-aware scheduling
and autoscaling to meet tail latency SLOs while minimizing costs. ModServe
achieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while
meeting SLOs on a 128-GPU cluster with production traces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashikant Ilager, Lukas Florian Briem, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are becoming integral to daily life, showcasing
their vast potential across various Natural Language Processing (NLP) tasks.
Beyond NLP, LLMs are increasingly used in software development tasks, such as
code completion, modification, bug fixing, and code translation. Software
engineers widely use tools like GitHub Copilot and Amazon Q, streamlining
workflows and automating tasks with high accuracy. While the resource and
energy intensity of LLM training is often highlighted, inference can be even
more resource-intensive over time, as it's a continuous process with a high
number of invocations. Therefore, developing resource-efficient alternatives
for LLM inference is crucial for sustainability. This work proposes GREEN-CODE,
a framework for energy-aware code generation in LLMs. GREEN-CODE performs
dynamic early exit during LLM inference. We train a Reinforcement Learning (RL)
agent that learns to balance the trade-offs between accuracy, latency, and
energy consumption. Our approach is evaluated on two open-source LLMs, Llama
3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that
our method reduces the energy consumption between 23-50 % on average for code
generation tasks without significantly affecting accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission in ACM/IEEE conference, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offload Rethinking by Cloud Assistance for Efficient Environmental Sound
  Recognition on LPWANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15285v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15285v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhang, Quanling Zhao, Run Wang, Shirley Bian, Onat Gungor, Flavio Ponzina, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based environmental sound recognition has emerged as a crucial
method for ultra-low-power environmental monitoring in biological research and
city-scale sensing systems. These systems usually operate under limited
resources and are often powered by harvested energy in remote areas. Recent
efforts in on-device sound recognition suffer from low accuracy due to resource
constraints, whereas cloud offloading strategies are hindered by high
communication costs. In this work, we introduce ORCA, a novel
resource-efficient cloud-assisted environmental sound recognition system on
batteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),
targeting wide-area audio sensing applications. We propose a cloud assistance
strategy that remedies the low accuracy of on-device inference while minimizing
the communication costs for cloud offloading. By leveraging a
self-attention-based cloud sub-spectral feature selection method to facilitate
efficient on-device inference, ORCA resolves three key challenges for
resource-constrained cloud offloading over LPWANs: 1) high communication costs
and low data rates, 2) dynamic wireless channel conditions, and 3) unreliable
offloading. We implement ORCA on an energy-harvesting batteryless
microcontroller and evaluate it in a real world urban sound testbed. Our
results show that ORCA outperforms state-of-the-art methods by up to $80
\times$ in energy savings and $220 \times$ in latency reduction while
maintaining comparable accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 23rd ACM Conference on Embedded Networked Sensor
  Systems (SenSys '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Controllable and Realistic Framework for Evaluating Microservice
  Scheduling in Cloud-Edge Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Chen, Muhammed Tawfiqul Islam, Maria Rodriguez Read, Rajkumar Buyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transition from traditional architectures to containerized microservices
within the cloud-edge computing continuum introduces significant challenges,
particularly in the efficient scheduling of microservices under dynamic
conditions. Complex and fluctuating call-graph dependencies, varying cross-node
communication latencies, and unpredictable bandwidth conditions substantially
impact the performance and reliability of deployed microservices. Consequently,
accurately evaluating scheduling policies in such dynamic environments remains
essential yet challenging due to the lack of realistic and controllable
evaluation frameworks.
  In this paper, we propose iDynamics, a novel evaluation framework designed
explicitly to address these challenges. iDynamics provides comprehensive and
controllable evaluation capabilities by emulating realistic dynamics, including
configurable call-graph topologies, cross-node communication delays, and
bandwidth variability. The framework is composed of modular components, such as
the Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy
Extender, enabling fine-grained environmental control and facilitating
systematic comparisons of different scheduling strategies. Extensive
experiments on a real cloud-edge testbed demonstrate that iDynamics effectively
captures diverse dynamic scenarios encountered in microservice deployments,
offering a robust solution for evaluating and optimizing policy performance
under realistic and controllable conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-03-29T05:27:55.222189172Z">
            2025-03-29 05:27:55 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
